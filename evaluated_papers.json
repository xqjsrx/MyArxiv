[
  {
    "id": "http://arxiv.org/abs/2508.21063v1",
    "updated": "2025-08-28T17:59:05Z",
    "published": "2025-08-28T17:59:05Z",
    "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation",
    "authors": [
      "Ruixuan Liu",
      "Philip Huang",
      "Ava Pun",
      "Kangle Deng",
      "Shobhit Aggarwal",
      "Kevin Tang",
      "Michelle Liu",
      "Deva Ramanan",
      "Jun-Yan Zhu",
      "Jiaoyang Li",
      "Changliu Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21063v1.pdf",
    "comment": "12 pages, 10 figures, 2 tables",
    "category": "Artificial Intelligence",
    "abstract": "Creating assembly products demands significant manual effort and expert knowledge in 1) designing the assembly and 2) constructing the product. This paper introduces Prompt-to-Product, an automated pipeline that generates real-world assembly products from natural language prompts. Specifically, we leverage LEGO bricks as the assembly platform and automate the process of creating brick assembly structures. Given the user design requirements, Prompt-to-Product generates physically buildable brick designs, and then leverages a bimanual robotic system to construct the real assembly products, bringing user imaginations into the real world. We conduct a comprehensive user study, and the results demonstrate that Prompt-to-Product significantly lowers the barrier and reduces manual effort in creating assembly products from imaginative ideas.",
    "score": 3,
    "reason": "该论文聚焦于基于自然语言提示生成乐高积木装配结构并由双臂机器人实现物理构建，属于具身智能与生成式机器人控制领域。虽然涉及自然语言理解与视觉-动作生成的跨模态交互，但其核心目标是物理产品装配，而非文档图像理解（DIU）。尽管使用了多模态输入（文本+视觉），但应用场景、技术路径与DIU无直接关联，无法直接迁移至文档信息抽取、实体识别或关系建模任务。因此不值得在DIU研究中重点关注。",
    "summary": "Prompt-to-Product提出一个端到端系统，将自然语言描述转化为可物理构建的乐高积木装配结构，并通过双臂机器人完成实物搭建。该系统整合了文本理解、结构生成与机器人执行，旨在降低创意产品制造的门槛。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21061v1",
    "updated": "2025-08-28T17:58:29Z",
    "published": "2025-08-28T17:58:29Z",
    "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models",
    "authors": [
      "Adam Coscia",
      "Shunan Guo",
      "Eunyee Koh",
      "Alex Endert"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21061v1.pdf",
    "comment": "Accepted to UIST 2025. 18 pages, 9 figures, 2 tables. For a demo video, see https://youtu.be/uobhmxo6EIE",
    "category": "Artificial Intelligence",
    "abstract": "As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.",
    "score": 6,
    "reason": "该论文聚焦于多轮对话中目标追踪与可视化，虽涉及LLM在交互中的目标管理，但其核心是人机交互界面设计与用户行为研究，而非直接提升DIU任务的识别、理解或推理能力。虽然提及LLM辅助评估和解释，但未涉及视觉信息抽取、文档结构理解或VLM/LLM推理增强技术（如CoT、ToT等）在DIU中的应用。其贡献更偏向UI/UX层面，对DIU领域技术突破的直接迁移价值有限。尽管被UIST 2025接收（CCF B类会议），且有演示视频，但与DIU任务无紧密技术关联，故仅部分相关。",
    "summary": "OnGoal是一个面向多轮LLM对话的交互界面，通过实时追踪和可视化用户对话目标，提供目标对齐评估、解释说明及进展概览，帮助用户更高效地完成写作等任务。基于20名用户的实验表明，使用OnGoal可减少达成目标所需的时间与精力，并提升对提示策略的探索意愿。研究提出了改进LLM聊天界面的设计启示，强调目标通信与认知负荷管理。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21058v1",
    "updated": "2025-08-28T17:57:55Z",
    "published": "2025-08-28T17:57:55Z",
    "title": "Mixture of Contexts for Long Video Generation",
    "authors": [
      "Shengqu Cai",
      "Ceyuan Yang",
      "Lvmin Zhang",
      "Yuwei Guo",
      "Junfei Xiao",
      "Ziyan Yang",
      "Yinghao Xu",
      "Zhenheng Yang",
      "Alan Yuille",
      "Leonidas Guibas",
      "Maneesh Agrawala",
      "Lu Jiang",
      "Gordon Wetzstein"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21058v1.pdf",
    "comment": "Project page: https://primecai.github.io/moc/",
    "category": "Artificial Intelligence",
    "abstract": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.",
    "score": 2,
    "reason": "该论文聚焦于长视频生成中的上下文记忆问题，提出Mixture of Contexts（MoC）机制以优化长序列建模。虽然其核心思想——通过稀疏注意力路由实现高效长程记忆——在理论上可能对DIU中处理长文档布局或跨段落语义关联有启发意义，但研究任务是视频生成而非文档理解，且未涉及文本、实体或关系抽取等DIU核心模块。尽管模型具备一定的长程信息保留能力，但缺乏与文档结构、OCR输出、视觉-语言对齐等DIU关键要素的直接结合。因此，虽具一定潜在参考价值，但无法直接迁移应用，相关性较低。",
    "summary": "本文提出Mixture of Contexts（MoC）机制，用于解决长视频生成中因自注意力二次复杂度导致的计算与记忆瓶颈。MoC通过动态查询选择关键历史片段和强制锚点（如局部窗口、标题），实现因果路由下的稀疏注意力，从而支持分钟级视频的一致性生成。该方法在训练和推理中均表现出近线性扩展性，并自然涌现出长期记忆能力。项目主页已公开。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21052v1",
    "updated": "2025-08-28T17:55:14Z",
    "published": "2025-08-28T17:55:14Z",
    "title": "FakeParts: a New Family of AI-Generated DeepFakes",
    "authors": [
      "Gaetan Brison",
      "Soobash Daiboo",
      "Samy Aimeur",
      "Awais Hussain Sani",
      "Xi Wang",
      "Gianni Franchi",
      "Vicky Kalogeiton"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21052v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations.",
    "score": 2,
    "reason": "该论文聚焦于局部深度伪造（FakeParts）视频的生成与检测，属于多媒体安全领域，与文档图像理解（DIU）、多模态模型（VLM）、大语言模型推理增强（inference scaling）或智能体（agent）技术无直接关联。尽管其提出的数据集具有价值，但应用场景为视频篡改检测，无法直接迁移至文档信息抽取任务。因此不值得在DIU研究中优先阅读。",
    "summary": "本文提出了一类新型局部深度伪造（FakeParts），通过在真实视频中对特定空间区域或时间片段进行细微修改（如表情、物体替换、背景变更），实现高度隐蔽的伪造。为此构建了FakePartsBench数据集，包含超过2.5万段带像素级和帧级标注的视频，用于评估检测方法。实验表明，此类伪造显著降低人类与现有检测模型的识别准确率，揭示了当前检测技术的重大漏洞。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21051v1",
    "updated": "2025-08-28T17:55:07Z",
    "published": "2025-08-28T17:55:07Z",
    "title": "Enabling Equitable Access to Trustworthy Financial Reasoning",
    "authors": [
      "William Jurayj",
      "Nils Holzenberger",
      "Benjamin Van Durme"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21051v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "According to the United States Internal Revenue Service, ''the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.",
    "score": 7,
    "reason": "该论文提出将LLM与符号求解器结合的神经符号架构，用于税务推理任务，涉及复杂规则应用和数值计算，与DIU中的财务文档理解（如税单）高度相关。其核心思想——通过形式化规则转换+检索增强提升准确性和可审计性——可直接迁移至DIU中对规则密集型文档（如合同、申报表）的理解与验证。虽然未明确提及图像输入或布局分析，但其在逻辑推理与可信推理方面的技术路径对DIU中基于VLM的多步推理系统具有启发意义。不过，论文未涉及视觉信息抽取或文档图像处理，且未开源，因此不达顶尖水平。",
    "summary": "本文提出一种神经符号方法，将大语言模型与符号求解器结合，用于高精度、可审计的税务推理。通过将法律文本转化为形式化逻辑程序，并利用检索到的示例进行推理，显著提升在SARA数据集上的性能，同时降低部署成本。该方法为构建可信的自动化财务辅助系统提供了可行路径，对DIU中规则驱动型文档理解具有借鉴价值。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21048v1",
    "updated": "2025-08-28T17:53:05Z",
    "published": "2025-08-28T17:53:05Z",
    "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
    "authors": [
      "Hao Tan",
      "Jun Lan",
      "Zichang Tan",
      "Ajian Liu",
      "Chuanbiao Song",
      "Senyuan Shi",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jun Wan",
      "Zhen Lei"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21048v1.pdf",
    "comment": "Project: https://github.com/EricTan7/Veritas",
    "category": "Artificial Intelligence",
    "abstract": "Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as \"planning\" and \"self-reflection\" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs.",
    "score": 4,
    "reason": "该论文聚焦于深度伪造检测，虽使用了多模态大语言模型（MLLM）和模式感知推理（pattern-aware reasoning），但其核心任务与文档图像理解（DIU）无直接关联。尽管引入了类似chain-of-thought的推理机制，且具备可解释性输出，但应用场景为视频/图像伪造检测，而非文档结构化理解。虽然其两阶段训练策略和推理设计具有一定启发性，但无法直接迁移至DIU中的实体识别或关系抽取任务。此外，论文未涉及文档布局、OCR、信息抽取等关键DIU组件。因此相关性较低。",
    "summary": "Veritas提出一种基于多模态大语言模型的通用深度伪造检测方法，通过引入'规划'和'自我反思'等批判性推理模式，模拟人类法医分析过程。依托新构建的HydraFake数据集（涵盖跨模型、新型伪造技术与真实场景数据），采用两阶段训练将推理能力内化到MLLM中，在OOD场景下显著优于现有方法。项目已开源，但应用领域为深度伪造检测，与文档图像理解（DIU）无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21036v1",
    "updated": "2025-08-28T17:40:42Z",
    "published": "2025-08-28T17:40:42Z",
    "title": "Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop",
    "authors": [
      "Lev Tankelevitch",
      "Elena L. Glassman",
      "Jessica He",
      "Aniket Kittur",
      "Mina Lee",
      "Srishti Palani",
      "Advait Sarkar",
      "Gonzalo Ramos",
      "Yvonne Rogers",
      "Hari Subramonyam"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21036v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Generative AI (GenAI) radically expands the scope and capability of automation for work, education, and everyday tasks, a transformation posing both risks and opportunities for human cognition. How will human cognition change, and what opportunities are there for GenAI to augment it? Which theories, metrics, and other tools are needed to address these questions? The CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of how the use of GenAI affects human thought, from metacognition to critical thinking, memory, and creativity, with an emerging design practice for building GenAI tools that both protect and augment human thought. Fifty-six researchers, designers, and thinkers from across disciplines as well as industry and academia, along with 34 papers and portfolios, seeded a day of discussion, ideation, and community-building. We synthesize this material here to begin mapping the space of research and design opportunities and to catalyze a multidisciplinary community around this pressing area of research.",
    "score": 3,
    "reason": "该论文聚焦于生成式AI对人类认知的影响，属于人机交互与认知科学范畴，虽涉及GenAI工具的设计，但未直接关联文档图像理解（DIU）、视觉多模态模型（VLM）、推理增强技术（如CoT/TOT）或智能体系统。其内容偏重理论探讨与跨学科对话，缺乏可直接迁移至DIU任务的技术创新或方法论贡献，因此与目标领域关联较弱。",
    "summary": "本文综述了CHI 2025年‘Thought Tools’研讨会的成果，探讨生成式AI如何影响人类认知（如元认知、批判性思维、记忆与创造力），并试图构建一个融合研究与设计的跨学科社区，以应对GenAI带来的认知变革挑战。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19200v2",
    "updated": "2025-08-28T17:29:36Z",
    "published": "2025-08-26T17:03:43Z",
    "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
    "authors": [
      "Xinran Zhao",
      "Boyuan Zheng",
      "Chenglei Si",
      "Haofei Yu",
      "Ken Liu",
      "Runlong Zhou",
      "Ruochen Li",
      "Tong Chen",
      "Xiang Li",
      "Yiming Zhang",
      "Tongshuang Wu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19200v2.pdf",
    "comment": "21 pages, 3 figures",
    "category": "Artificial Intelligence",
    "abstract": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.",
    "score": 3,
    "reason": "该论文探讨的是基于拉蒙·柳利的组合逻辑构建AI驱动的研究创意生成系统，虽涉及LLM与创造性思维，但核心是科研选题生成，与文档图像理解（DIU）无直接关联。其方法论聚焦于主题、领域、方法三轴组合，属于抽象层面的创新，无法直接迁移至DIU任务中的OCR、实体识别或关系抽取。尽管使用了LLM，但未涉及inference scaling、reasoning技术（如CoT、ToT）或agent架构，也未触及多模态模型（VLM）在视觉信息抽取中的应用。因此，相关性较弱，仅对研究范式有启发意义，不值得优先阅读。",
    "summary": "本文受中世纪哲学家拉蒙·柳利的组合艺术启发，提出一种用于自动化研究创意生成的‘柳利思维机器’。通过定义主题、领域和方法三个维度的组合空间，利用LLM生成具有多样性与文献依据的研究想法，旨在增强科学创造力。该方法强调可解释性和轻量化设计，适用于科研选题探索，但与文档图像理解、多模态推理或智能体系统无直接联系。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21016v1",
    "updated": "2025-08-28T17:18:31Z",
    "published": "2025-08-28T17:18:31Z",
    "title": "Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance",
    "authors": [
      "Luozhijie Jin",
      "Zijie Qiu",
      "Jie Liu",
      "Zijie Diao",
      "Lifeng Qiao",
      "Ning Ding",
      "Alex Lamb",
      "Xipeng Qiu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21016v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Denoising-based generative models, particularly diffusion and flow matching algorithms, have achieved remarkable success. However, aligning their output distributions with complex downstream objectives, such as human preferences, compositional accuracy, or data compressibility, remains challenging. While reinforcement learning (RL) fine-tuning methods, inspired by advances in RL from human feedback (RLHF) for large language models, have been adapted to these generative frameworks, current RL approaches are suboptimal for diffusion models and offer limited flexibility in controlling alignment strength after fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models through the lens of stochastic differential equations and implicit reward conditioning. We introduce Reinforcement Learning Guidance (RLG), an inference-time method that adapts Classifier-Free Guidance (CFG) by combining the outputs of the base and RL fine-tuned models via a geometric average. Our theoretical analysis shows that RLG's guidance scale is mathematically equivalent to adjusting the KL-regularization coefficient in standard RL objectives, enabling dynamic control over the alignment-quality trade-off without further training. Extensive experiments demonstrate that RLG consistently improves the performance of RL fine-tuned models across various architectures, RL algorithms, and downstream tasks, including human preferences, compositional control, compressibility, and text rendering. Furthermore, RLG supports both interpolation and extrapolation, thereby offering unprecedented flexibility in controlling generative alignment. Our approach provides a practical and theoretically sound solution for enhancing and controlling diffusion model alignment at inference. The source code for RLG is publicly available at the Github: https://github.com/jinluo12345/Reinforcement-learning-guidance.",
    "score": 4,
    "reason": "该论文提出一种基于强化学习的推理阶段对齐控制方法（RLG），用于扩散模型，虽在技术上具有创新性且支持动态调整生成质量与对齐度之间的权衡，但其核心应用场景聚焦于图像生成任务（如文本到图像、压缩性控制等），与文档图像理解（DIU）无直接关联。尽管其提出的‘推理阶段控制’思想可能间接启发DIU中多步推理或生成优化策略，但缺乏与视觉信息抽取、布局理解、实体关系建模等DIU关键环节的直接结合。此外，未提及开源代码以外的信息也影响加分。因此，不推荐作为DIU领域重点阅读论文。",
    "summary": "本文提出Reinforcement Learning Guidance（RLG），一种基于随机微分方程视角的推理阶段对齐控制方法，通过几何平均融合基础模型与RL微调模型输出，实现无需再训练即可动态调节扩散模型生成结果与复杂目标（如人类偏好、组合准确性）之间的平衡。理论分析表明其等价于调整KL正则化系数，具备插值与外推能力，适用于多种下游任务。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2301.06375v2",
    "updated": "2025-08-28T17:12:29Z",
    "published": "2023-01-16T11:40:50Z",
    "title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset",
    "authors": [
      "Jeongkyun Park",
      "Jung-Wook Hwang",
      "Kwanghee Choi",
      "Seung-Hyun Lee",
      "Jun Hwan Ahn",
      "Rae-Hong Park",
      "Hyung-Min Park"
    ],
    "pdf_url": "https://arxiv.org/pdf/2301.06375v2.pdf",
    "comment": "Accepted to ICASSP 2024",
    "category": "Artificial Intelligence",
    "abstract": "Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the OLKAVS dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis.",
    "score": 2,
    "reason": "该论文聚焦于韩语音频-视觉语音数据集的构建，属于多模态语音识别领域，与文档图像理解（DIU）无直接关联。尽管涉及多模态，但其核心是音频-视觉语音识别，而非文本与布局结合的文档理解。此外，未提及视觉语言模型（VLM）、推理增强技术（如CoT/TOT）或智能体（Agent）相关应用，也未开源或在CCF A类会议发表（ICASSP虽为重要会议，但非A类）。因此不值得阅读。",
    "summary": "OLKAVS是一个大规模韩语音频-视觉语音数据集，包含1,150小时来自1,107名说话者的多视角视频和转录音频，用于支持多模态语音识别、唇读等任务。论文提供了预训练基线模型，并验证了多视角训练的有效性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21010v1",
    "updated": "2025-08-28T17:10:53Z",
    "published": "2025-08-28T17:10:53Z",
    "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Basura Fernando"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21010v1.pdf",
    "comment": "Project page: https://paritoshparmar.github.io/chainreaction/",
    "category": "Artificial Intelligence",
    "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/",
    "score": 4,
    "reason": "该论文聚焦于因果视频问答中的可解释推理，提出使用自然语言因果链作为中间表示，虽引入了结构化推理机制，但研究对象为视频理解而非文档图像理解。尽管其inference scaling与reasoning思想（如模块化推理链）对DIU有潜在启发意义，但应用场景不直接相关，且未涉及文档、布局、表格或OCR等DIU核心要素。虽然提出的方法具有可迁移性潜力，但缺乏与DIU任务的直接结合设计，因此仅具间接参考价值。",
    "summary": "本文提出一种基于因果链的模块化框架用于因果视频问答，通过分离因果推理与答案生成阶段，利用大模型生成因果链作为中间表示以提升可解释性。该方法在多个基准上优于现有模型，并引入新评估指标CauCo。项目页面已公开。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21001v1",
    "updated": "2025-08-28T17:04:00Z",
    "published": "2025-08-28T17:04:00Z",
    "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
    "authors": [
      "Yaniv Hassidof",
      "Tom Jurgenson",
      "Kiril Solovey"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21001v1.pdf",
    "comment": "Accepted to CoRL 2025. Project page: https://sites.google.com/view/ditree",
    "category": "Artificial Intelligence",
    "abstract": "Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a \\emph{provably-generalizable} framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs to yield \\emph{provably-safe} solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a \\emph{single environment}. In comprehensive evaluations on OOD scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than classical SBPs), while improving the average success rate over DP and SBPs. DiTree is on average 3x faster than classical SBPs, and outperforms all other approaches by achieving roughly 30\\% higher success rate. Project webpage: https://sites.google.com/view/ditree.",
    "score": 2,
    "reason": "该论文研究的是基于扩散模型的运动规划问题，属于机器人学与控制领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型（LLM）推理增强或智能体（Agent）系统无直接关联。尽管其提出的方法在采样效率和可推广性方面有创新，但其应用场景、技术路径与DIU核心任务（文本识别、实体抽取、关系建模、结构理解）不具迁移价值，无法直接应用于文档理解或AI Agent中的信息抽取与推理环节。",
    "summary": "本文提出Diffusion Tree（DiTree），一种结合扩散策略（DP）与采样基规划器（SBP）的可证明泛化运动规划框架。通过使用在单一环境中训练的扩散策略作为有向采样器，显著提升RRT等SBP算法的探索效率，在OOD场景下实现比传统SBP快3倍且成功率高出约30%的表现，同时保证安全性和完备性。该方法在复杂动力系统中表现出色，已接受于CoRL 2025。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20996v1",
    "updated": "2025-08-28T16:57:33Z",
    "published": "2025-08-28T16:57:33Z",
    "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery",
    "authors": [
      "Junda Wang",
      "Zonghai Yao",
      "Zhichao Yang",
      "Lingxi Li",
      "Junhui Qian",
      "Hong Yu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20996v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet few receive effective care due to stigma, motivational barriers, and limited personalized support. Although large language models (LLMs) show promise for mental-health assistance, most systems lack tight integration with clinically validated strategies, reducing effectiveness in addiction recovery. We present ChatThero, a multi-agent conversational framework that couples dynamic patient modeling with context-sensitive therapeutic dialogue and adaptive persuasive strategies grounded in cognitive behavioral therapy (CBT) and motivational interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy, Medium, and Hard resistance levels, and train ChatThero with a two-stage pipeline comprising supervised fine-tuning (SFT) followed by direct preference optimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in patient motivation, a 0.49\\% increase in treatment confidence, and resolves hard cases with 26\\% fewer turns than GPT-4o, and both automated and human clinical assessments rate it higher in empathy, responsiveness, and behavioral realism. The framework supports rigorous, privacy-preserving study of therapeutic conversation and provides a robust, replicable basis for research and clinical translation.",
    "score": 3,
    "reason": "该论文聚焦于基于LLM的聊天机器人在戒瘾康复中的心理治疗支持，虽涉及LLM与多智能体系统（multi-agent）设计，但应用场景为心理健康干预，与文档图像理解（DIU）无直接关联。尽管其采用动态患者建模和上下文感知对话，但这些技术未针对视觉信息抽取、布局理解或文本-视觉联合推理进行设计，无法直接迁移至DIU任务。此外，论文未涉及OCR、实体识别、关系抽取或VLM等DIU核心模块，也未使用视觉输入或处理文档图像。因此，虽然具备一定的agent架构思想，但应用领域和任务本质差异较大，不具直接借鉴价值。",
    "summary": "ChatThero是一个用于物质使用障碍康复的多智能体聊天机器人框架，结合认知行为疗法（CBT）和动机性访谈（MI）策略，通过两阶段微调（SFT + DPO）提升治疗对话的疗效。在合成基准上表现优于GPT-4o，显著提升患者动机与治疗信心，但主要面向纯文本对话场景，不涉及文档图像理解或视觉信息处理。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20991v1",
    "updated": "2025-08-28T16:53:03Z",
    "published": "2025-08-28T16:53:03Z",
    "title": "ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts",
    "authors": [
      "Patryk Będkowski",
      "Jan Dubiński",
      "Filip Szatkowski",
      "Kamil Deja",
      "Przemysław Rokita",
      "Tomasz Trzciński"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20991v1.pdf",
    "comment": "Accepted at ECAI 2025 28th European Conference on Artificial Intelligence",
    "category": "Artificial Intelligence",
    "abstract": "Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.",
    "score": 2,
    "reason": "该论文聚焦于粒子物理中的探测器仿真，使用混合生成专家模型加速蒙特卡洛模拟，属于生成建模在科学计算领域的应用。尽管其技术架构（Mixture-of-Generative-Experts）具有一定创新性，但与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。其应用场景、数据模态（高能物理事件）和任务目标均与DIU领域无关，无法直接迁移至文档理解任务中。因此不值得阅读。",
    "summary": "ExpertSim提出一种基于混合生成专家架构的深度学习方法，用于高效模拟ALICE实验中零度量能器的响应。通过为不同数据子集分配专用专家，提升仿真精度与速度，显著优于传统蒙特卡洛方法。代码已开源，论文被ECAI 2025接收。"
  },
  {
    "id": "http://arxiv.org/abs/2407.15161v3",
    "updated": "2025-08-28T16:44:25Z",
    "published": "2024-07-21T13:33:08Z",
    "title": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference",
    "authors": [
      "Qian Feng",
      "Jianxiang Feng",
      "Zhaopeng Chen",
      "Rudolph Triebel",
      "Alois Knoll"
    ],
    "pdf_url": "https://arxiv.org/pdf/2407.15161v3.pdf",
    "comment": "First two authors contributed equally, whose ordering decided via coin-tossing. Accepted for CoRL 2025",
    "category": "Artificial Intelligence",
    "abstract": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions (Project Page: https://sites.google.com/view/ffhflow/home/).",
    "score": 2,
    "reason": "该论文聚焦于多指机器人抓取生成，涉及不确定性建模和流变分推断，属于机器人学与强化学习交叉领域。虽然其方法论中包含不确定性感知和多样性生成的思想，但与文档图像理解（DIU）、视觉语言模型（VLM）、大语言模型推理增强（inference scaling）或智能体（agent）系统无直接关联。其技术栈（如正常化流、点云处理）无法直接迁移至DIU任务，且不涉及文本、布局、实体或关系抽取等核心DIU要素。因此，不值得阅读。",
    "summary": "FFHFlow提出一种基于流的变分推断框架，用于从部分观测点云中生成多样且对形状不确定性敏感的多指抓取策略。通过构建分层抓取流形并利用流的可逆性和精确似然性，显式建模感知不确定性，并结合判别式评估器实现风险感知的抓取排序。在仿真与真实场景中均优于现有基线，尤其在杂乱受限环境中表现出色。"
  },
  {
    "id": "http://arxiv.org/abs/2507.22931v2",
    "updated": "2025-08-28T16:42:39Z",
    "published": "2025-07-24T13:46:51Z",
    "title": "Dynamic Context Compression for Efficient RAG",
    "authors": [
      "Shuyu Guo",
      "Zhaochun Ren"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.22931v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.",
    "score": 8,
    "reason": "该论文提出动态上下文压缩方法ACC-RAG，针对RAG中因检索上下文过长导致的推理开销问题，通过根据输入复杂度自适应调整压缩率，在保持准确率的同时显著提升推理效率。此技术与DIU密切相关，尤其适用于DIU系统中基于外部知识的问答模块（如文档语义理解、字段关系推理等），可有效降低多轮交互或复杂文档解析时的延迟。虽未直接涉及DIU、VLM或agent，但其在LLM推理阶段的高效优化（inference scaling）属于关键支撑技术，可直接迁移至DIU中的LLM后处理环节。若未来结合视觉-语言对齐的检索机制，将更具潜力。",
    "summary": "本文提出ACC-RAG，一种基于输入复杂度动态调整压缩率的上下文压缩框架，通过层次化压缩器与上下文选择器实现高效信息保留，显著提升RAG推理速度（提速超4倍）且不牺牲准确性，适用于需要高效知识增强的下游任务，如文档理解中的问答与推理。"
  },
  {
    "id": "http://arxiv.org/abs/2505.03818v2",
    "updated": "2025-08-28T16:38:13Z",
    "published": "2025-05-02T20:03:35Z",
    "title": "Program Semantic Inequivalence Game with Large Language Models",
    "authors": [
      "Antonio Valerio Miceli-Barone",
      "Vaishak Belle",
      "Ali Payani"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.03818v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Large Language Models (LLMs) can achieve strong performance on everyday coding tasks, but they can fail on complex tasks that require non-trivial reasoning about program semantics. Finding training examples to teach LLMs to solve these tasks can be challenging. In this work, we explore a method to synthetically generate code reasoning training data based on a semantic inequivalence game SInQ: a generator agent creates program variants that are semantically distinct, derived from a dataset of real-world programming tasks, while an evaluator agent has to identify input examples that cause the original programs and the generated variants to diverge in their behaviour, with the agents training each other semi-adversarially. We prove that this setup enables theoretically unlimited improvement through self-play in the limit of infinite computational resources. We evaluated our approach on multiple code generation and understanding benchmarks, including cross-language vulnerability detection (Lu et al., 2021), where our method improves vulnerability detection in C/C++ code despite being trained exclusively on Python code, and the challenging Python builtin identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas modern LLMs still struggle with this benchmark, our approach yields substantial improvements. We release the code needed to replicate the experiments, as well as the generated synthetic data, which can be used to fine-tune LLMs.",
    "score": 8,
    "reason": "该论文提出了一种基于语义不等价博弈的自动生成代码推理训练数据的方法，通过生成器与评估器的半对抗式自洽训练，显著提升LLM在复杂程序语义理解任务上的表现。其核心思想——利用自生成高质量推理样本进行模型训练——与DIU中需要增强视觉-文本语义推理能力的需求高度契合。尤其在文档理解中涉及字段语义判断、逻辑关系推断等场景时，该方法可迁移至构建更鲁棒的文档语义推理模块。虽然论文本身聚焦于编程领域，但其inference scaling和reasoning增强机制（如自洽训练、对抗性数据生成）对DIU中的关系抽取和实体语义理解具有直接启发意义。且代码开源，便于复现与集成，值得深入阅读。",
    "summary": "本文提出SInQ框架，通过生成器与评估器在程序语义不等价性上的对抗训练，自动生成用于强化LLM代码理解能力的合成训练数据。实验表明该方法在跨语言漏洞检测和内置标识符替换等复杂任务上显著优于基线模型，且无需依赖目标语言的真实标注数据。研究为提升LLM在复杂推理任务中的泛化能力提供了有效路径，对DIU中增强语义推理能力具有直接借鉴价值。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20978v1",
    "updated": "2025-08-28T16:33:27Z",
    "published": "2025-08-28T16:33:27Z",
    "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective",
    "authors": [
      "Marianne Defresne",
      "Romain Gambardella",
      "Sophie Barbe",
      "Thomas Schiex"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20978v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with. Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems. Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy. Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.",
    "score": 7,
    "reason": "该论文提出一种可微分的神经符号架构，用于从自然输入中学习求解NP-hard推理问题，尤其在约束和目标函数联合学习方面具有创新性。其方法通过将组合求解器移出训练循环，实现高效训练与精确推理，适用于需要结构化逻辑推理的任务。虽然未直接涉及DIU、VLM或LLM推理增强，但其在神经符号结合与复杂约束优化方面的技术对DIU中的关系抽取与布局理解等任务有潜在启发价值，尤其在构建可解释、高精度的文档推理系统方面可能应用。不过缺乏与视觉信息、文档理解或多模态模型的直接关联，因此不属核心相关，但具备一定迁移潜力。",
    "summary": "本文提出一种可微分神经符号架构，能够从自然输入中联合学习约束与目标函数，以解决NP-hard推理问题。通过设计概率损失函数并移除训练中的组合求解器，实现高效训练与精确推理。在Sudoku（符号、视觉、多解）和视觉Min-Cut/Max-Cut任务上表现优异，并成功应用于蛋白质设计这一真实世界优化问题。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20976v1",
    "updated": "2025-08-28T16:29:46Z",
    "published": "2025-08-28T16:29:46Z",
    "title": "WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations",
    "authors": [
      "Jaeyeon Kim",
      "Heeseung Yun",
      "Sang Hoon Woo",
      "Chao-Han Huck Yang",
      "Gunhee Kim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20976v1.pdf",
    "comment": "Preprint. Project page: https://jaeyeonkim99.github.io/wow_bench/",
    "category": "Artificial Intelligence",
    "abstract": "Large audio language models (LALMs) extend language understanding into the auditory domain, yet their ability to perform low-level listening, such as pitch and duration detection, remains underexplored. However, low-level listening is critical for real-world, out-of-distribution tasks where models must reason about unfamiliar sounds based on fine-grained acoustic cues. To address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to evaluate low-level auditory perception and cognition using marine mammal vocalizations. WoW-bench is composed of a Perception benchmark for categorizing novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess the abilities to remember, understand, apply, and analyze sound events. For the Cognition benchmark, we additionally introduce distractor questions to evaluate whether models are truly solving problems through listening rather than relying on other heuristics. Experiments with state-of-the-art LALMs show performance far below human levels, indicating a need for stronger auditory grounding in LALMs.",
    "score": 2,
    "reason": "该论文聚焦于音频语言模型在海洋哺乳动物叫声上的细粒度听觉感知评估，属于音频-语言模型领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）技术无直接关联。其核心任务为音频感知与认知评测，无法直接迁移至文本-视觉联合理解的DIU场景，且未涉及文档、布局、表格或视觉信息抽取等关键方向，因此不值得在DIU研究中优先阅读。",
    "summary": "本文提出WoW-Bench基准，用于评估大音频语言模型（LALMs）在海洋哺乳动物 vocalizations 上的低层次听觉感知与认知能力。包含感知和认知两个子任务，基于Bloom分类法设计，引入干扰项以验证模型是否真正依赖听觉线索进行推理。实验表明当前LALMs性能远低于人类水平，揭示了音频建模中听觉基础能力的不足。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20973v1",
    "updated": "2025-08-28T16:26:44Z",
    "published": "2025-08-28T16:26:44Z",
    "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents",
    "authors": [
      "Tianjian Liu",
      "Fanqi Wan",
      "Jiajian Guo",
      "Xiaojun Quan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20973v1.pdf",
    "comment": "21 pages, 6 Figures",
    "category": "Artificial Intelligence",
    "abstract": "Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.",
    "score": 6,
    "reason": "该论文聚焦于主动对话代理的评估框架，虽涉及LLM推理与规划能力，但其核心是对话系统评估，而非文档图像理解。尽管提到的推理能力与DIU中的agent设计有关联，但论文内容未直接涉及视觉信息抽取、布局理解或文档结构建模，且无与VLM或DIU任务的直接结合。因此，相关性较弱，仅对DIU agent中‘规划’模块有一定启发，但不足以构成直接应用价值。",
    "summary": "本文提出ProactiveEval，一个统一的评估框架，用于衡量大语言模型在主动对话中的目标规划与对话引导能力。通过分解主动对话为两个子任务，并构建328个跨6个领域的评估环境，实现自动化挑战数据生成。实验表明DeepSeek-R1和Claude-3.7-Sonnet在不同任务上表现优异，并探讨了推理能力对主动行为的影响。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20953v1",
    "updated": "2025-08-28T16:16:10Z",
    "published": "2025-08-28T16:16:10Z",
    "title": "A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling",
    "authors": [
      "Vipul Patel",
      "Anirudh Deodhar",
      "Dagnachew Birru"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20953v1.pdf",
    "comment": "8 pages, 7 figures, Accepted at the Multi-Objective Decision Making Workshop (MODeM2025) at ECAI 2025",
    "category": "Artificial Intelligence",
    "abstract": "Workforce scheduling in the healthcare sector is a significant operational challenge, characterized by fluctuating patient loads, diverse clinical skills, and the critical need to control labor costs while upholding high standards of patient care. This problem is inherently multi-objective, demanding a delicate balance between competing goals: minimizing payroll, ensuring adequate staffing for patient needs, and accommodating staff preferences to mitigate burnout. We propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital unit workforce scheduling problem as a multi-objective optimization task. Our model incorporates real-world complexities, including hourly appointment-driven demand and the use of modular shifts for a multi-skilled workforce. By defining objective functions for cost, patient care coverage, and staff satisfaction, the GA navigates the vast search space to identify a set of high-quality, non-dominated solutions. Demonstrated on datasets representing a typical hospital unit, the results show that our MOO-GA generates robust and balanced schedules. On average, the schedules produced by our algorithm showed a 66\\% performance improvement over a baseline that simulates a conventional, manual scheduling process. This approach effectively manages trade-offs between critical operational and staff-centric objectives, providing a practical decision support tool for nurse managers and hospital administrators.",
    "score": 2,
    "reason": "该论文研究的是医疗人员调度的多目标遗传算法，属于运筹优化领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型（LLM）推理增强（inference scaling/reasoning）或智能体（agent）技术无直接关联。其应用场景、方法论和技术栈均不适用于DIU任务，无法直接迁移应用。尽管被ECAI Workshop接收，但该会议非CCF A类，且内容无关，故不予推荐。",
    "summary": "本文提出一种多目标遗传算法（MOO-GA）用于医院科室的医护人员排班问题，旨在平衡成本、患者护理覆盖率和员工满意度。通过建模小时级需求和模块化轮班，算法在真实数据集上相比传统人工排班提升了66%的性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.11017v2",
    "updated": "2025-08-28T15:51:55Z",
    "published": "2025-08-14T18:44:13Z",
    "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
    "authors": [
      "Carter Blum",
      "Katja Filippova",
      "Ann Yuan",
      "Asma Ghandeharioun",
      "Julian Zimmert",
      "Fred Zhang",
      "Jessica Hoffmann",
      "Tal Linzen",
      "Martin Wattenberg",
      "Lucas Dixon",
      "Mor Geva"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11017v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.",
    "score": 6,
    "reason": "该论文研究了多语言模型中跨语言知识迁移的统一表示机制，对理解LLM在多语言场景下的泛化动态有理论价值。虽然涉及LLM的推理与跨模态/跨语言能力，但其核心是小规模Transformer在合成数据上的训练，与DIU任务中的文档视觉-文本联合理解、布局感知、实体关系建模等实际需求关联较弱。尽管研究了语言间表征统一性，这可能间接启发VLM在多语言文档处理中的设计，但缺乏直接应用到DIU的路径。未开源，非CCF A类会议，因此加分有限。",
    "summary": "本文通过在合成多语言数据上从头训练小型Transformer模型，研究了跨语言知识转移中的表征统一性问题。发现模型在学习过程中会形成独立或统一的语言间表征，而统一表征是实现有效跨语言迁移的关键。研究揭示了语言信息可提取性与事实-语言互信息对统一程度的影响，并提出调控方法与可视化工具。为改进LLM跨语言能力提供了理论洞见。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20912v1",
    "updated": "2025-08-28T15:41:49Z",
    "published": "2025-08-28T15:41:49Z",
    "title": "Research Challenges in Relational Database Management Systems for LLM Queries",
    "authors": [
      "Kerem Akillioglu",
      "Anurag Chakraborty",
      "Sairaj Voruganti",
      "M. Tamer Özsu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20912v1.pdf",
    "comment": "This paper will appear in the 6th International Workshop on Applied AI for Database Systems and Applications, AIDB Workshop at VLDB 2025",
    "category": "Artificial Intelligence",
    "abstract": "Large language models (LLMs) have become essential for applications such as text summarization, sentiment analysis, and automated question-answering. Recently, LLMs have also been integrated into relational database management systems to enhance querying and support advanced data processing. Companies such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly within SQL, denoted as LLM queries, to boost data insights. However, open-source solutions currently have limited functionality and poor performance. In this work, we present an early exploration of two open-source systems and one enterprise platform, using five representative queries to expose functional, performance, and scalability limits in today's SQL-invoked LLM integrations. We identify three main issues: enforcing structured outputs, optimizing resource utilization, and improving query planning. We implemented initial solutions and observed improvements in accommodating LLM powered SQL queries. These early gains demonstrate that tighter integration of LLM+DBMS is the key to scalable and efficient processing of LLM queries.",
    "score": 3,
    "reason": "该论文聚焦于LLM与关系型数据库管理系统（DBMS）的集成，讨论的是LLM用于SQL查询的系统性挑战。虽然涉及LLM的应用场景，但核心是数据库查询优化与系统架构问题，与文档图像理解（DIU）、视觉多模态模型（VLM）或推理增强技术（如CoT、ToT）无直接关联。其内容不适用于DIU任务中的OCR、实体识别、关系抽取或agent构建等环节，也未提出可迁移至DIU的新型推理范式或多模态处理方法。尽管发表于VLDB Workshop，但领域相关性较弱，仅在LLM应用层面有间接参考价值。",
    "summary": "本文探讨了将大语言模型（LLM）集成到关系型数据库管理系统中以支持LLM驱动的SQL查询所面临的挑战，包括结构化输出控制、资源利用率和查询规划等问题。通过评估开源与企业级平台，作者识别出关键瓶颈并提出初步解决方案，强调LLM与DBMS更紧密集成的重要性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20907v1",
    "updated": "2025-08-28T15:37:40Z",
    "published": "2025-08-28T15:37:40Z",
    "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant",
    "authors": [
      "Nicolas Dupuis",
      "Adarsh Tiwari",
      "Youssef Mroueh",
      "David Kremer",
      "Ismael Faro",
      "Juan Cruz-Benito"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20907v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Qiskit is an open-source quantum computing framework that allows users to design, simulate, and run quantum circuits on real quantum hardware. We explore post-training techniques for LLMs to assist in writing Qiskit code. We introduce quantum verification as an effective method for ensuring code quality and executability on quantum hardware. To support this, we developed a synthetic data pipeline that generates quantum problem-unit test pairs and used it to create preference data for aligning LLMs with DPO. Additionally, we trained models using GRPO, leveraging quantum-verifiable rewards provided by the quantum hardware. Our best-performing model, combining DPO and GRPO, surpasses the strongest open-source baselines on the challenging Qiskit-HumanEval-hard benchmark.",
    "score": 3,
    "reason": "该论文聚焦于量子计算代码生成任务，虽涉及LLM后训练与奖励机制，但其应用领域（Qiskit代码辅助）与文档图像理解（DIU）无直接关联。尽管使用了DPO和GRPO等先进训练技术，但这些技术在推理阶段的scaling或reasoning增强并未体现，且未涉及视觉、布局、表格或多模态信息抽取等DIU核心要素。此外，论文未开源，也未发表于CCF A类会议，因此对DIU研究参考价值较低。",
    "summary": "本文提出一种基于量子可验证奖励的后训练方法，用于提升LLM在生成Qiskit量子电路代码方面的能力。通过构建合成数据集并利用真实量子硬件进行执行验证，结合DPO和GRPO优化模型，在Qiskit-HumanEval-hard基准上超越现有开源模型。"
  },
  {
    "id": "http://arxiv.org/abs/2503.17513v2",
    "updated": "2025-08-28T15:33:02Z",
    "published": "2025-03-21T19:56:59Z",
    "title": "Improving Quantization with Post-Training Model Expansion",
    "authors": [
      "Giuseppe Franco",
      "Pablo Monteagudo-Lago",
      "Ian Colbert",
      "Nicholas Fraser",
      "Michaela Blott"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.17513v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "The size of a model has been a strong predictor of its quality, as well as its cost. As such, the trade-off between model cost and quality has been well-studied. Post-training optimizations like quantization and pruning have typically focused on reducing the overall volume of pre-trained models to reduce inference costs while maintaining model quality. However, recent advancements have introduced optimization techniques that, interestingly, expand models post-training, increasing model size to improve quality when reducing volume. For instance, to enable 4-bit weight and activation quantization, incoherence processing often necessitates inserting online Hadamard rotations in the compute graph, and preserving highly sensitive weights often calls for additional higher precision computations. However, if application requirements cannot be met, the prevailing solution is to relax quantization constraints. In contrast, we demonstrate post-training model expansion is a viable strategy to improve model quality within a quantization co-design space, and provide theoretical justification. We show it is possible to progressively and selectively expand the size of a pre-trained large language model (LLM) to improve model quality without end-to-end retraining. In particular, when quantizing the weights and activations to 4 bits for Llama3 1B, we reduce the gap to full-precision perplexity by an average of 9% relative to both QuaRot and SpinQuant with only 5% more parameters, which is still a 3.8% reduction in volume relative to a BF16 reference model.",
    "score": 3,
    "reason": "该论文研究的是后训练模型扩展以提升量化性能，属于模型压缩与优化方向。虽然涉及大语言模型（LLM）的推理效率问题，但其核心是量化过程中的计算图重构与参数扩展，与DIU任务无直接关联。尽管LLM是DIU的上游技术，但本文方法无法直接应用于文档图像理解中的视觉-文本对齐、布局建模或实体关系抽取等关键环节。此外，未提及开源或顶会发表，且未体现对多模态、推理链、Agent架构等前沿范式的支持。因此不值得在DIU研究中优先阅读。",
    "summary": "本文提出一种后训练模型扩展策略，在保持低比特量化的同时通过选择性增加模型参数来提升LLM性能。实验表明，该方法在4-bit量化下可显著缩小与全精度模型的困惑度差距，同时仍保持较低的总体体积。"
  },
  {
    "id": "http://arxiv.org/abs/2507.17232v2",
    "updated": "2025-08-28T15:15:18Z",
    "published": "2025-07-23T05:56:20Z",
    "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task",
    "authors": [
      "Mashiro Toyooka",
      "Kiyoharu Aizawa",
      "Yoko Yamakata"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.17232v2.pdf",
    "comment": "Accepted to ACM Multimedia 2025. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
    "category": "Artificial Intelligence",
    "abstract": "Large Language Models (LLMs) are trained on a vast amount of procedural texts, but they do not directly observe real-world phenomena. In the context of cooking recipes, this poses a challenge, as intermediate states of ingredients are often omitted, making it difficult for models to track ingredient states and understand recipes accurately. In this paper, we apply state probing, a method for evaluating a language model's understanding of the world, to the domain of cooking. We propose a new task and dataset for evaluating how well LLMs can recognize intermediate ingredient states during cooking procedures. We first construct a new Japanese recipe dataset with clear and accurate annotations of ingredient state changes, collected from well-structured and controlled recipe texts. Using this dataset, we design three novel tasks to evaluate whether LLMs can track ingredient state transitions and identify ingredients present at intermediate steps. Our experiments with widely used LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state knowledge improves their understanding of cooking processes, achieving performance comparable to commercial LLMs. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
    "score": 6,
    "reason": "该论文提出一个用于状态探测的高质量日语食谱数据集，专注于评估LLM对食材状态变化的理解能力。虽然其任务与文档理解中的实体状态追踪有一定关联，但核心聚焦于烹饪过程的时序理解，属于特定场景下的语言模型评估任务。尽管数据集开源且被ACM MM接收（CCF A类），但其应用场景（烹饪）与DIU中常见的账单、简历、表格等结构化文档差异较大，直接迁移至DIU的潜力有限。此外，未涉及视觉信息、OCR或多模态内容，也未触及VLM或推理增强技术（如CoT/TOT），因此在DIU领域应用价值较弱。",
    "summary": "本文构建了一个高质量的日语食谱数据集，标注了烹饪过程中食材的状态变化，用于评估大语言模型对中间状态的理解能力。设计了三项新任务以测试模型是否能追踪食材状态转变，并在Llama3.1-70B和Qwen2.5-72B上进行了实验，结果表明学习状态知识可提升模型对烹饪流程的理解。数据集已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20866v1",
    "updated": "2025-08-28T14:59:39Z",
    "published": "2025-08-28T14:59:39Z",
    "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning",
    "authors": [
      "Amine Lbath",
      "Massih-Reza Amini",
      "Aurelien Delaitre",
      "Vadim Okun"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20866v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the critical need for effective automated vulnerability detection and repair systems. Traditional methods, such as static program analysis, face significant challenges related to scalability, adaptability, and high false-positive and false-negative rates. AI-driven approaches, particularly those using machine learning and deep learning models, show promise but are heavily reliant on the quality and quantity of training data. This paper introduces a novel framework designed to automatically introduce realistic, category-specific vulnerabilities into secure C/C++ codebases to generate datasets. The proposed approach coordinates multiple AI agents that simulate expert reasoning, along with function agents and traditional code analysis tools. It leverages Retrieval-Augmented Generation for contextual grounding and employs Low-Rank approximation of weights for efficient model fine-tuning. Our experimental study on 116 code samples from three different benchmarks suggests that our approach outperforms other techniques with regard to dataset accuracy, achieving between 89\\% and 95\\% success rates in injecting vulnerabilities at function level.",
    "score": 3,
    "reason": "该论文聚焦于AI驱动的漏洞注入与修复，虽涉及智能体（AI agents）和推理机制，但其应用场景为软件安全领域，与文档图像理解（DIU）无直接关联。尽管提到了多智能体协作和检索增强生成，但其核心目标是生成带漏洞的代码数据集，而非处理文档结构、文本理解或视觉-语言对齐任务。此外，论文未提及任何与VLM、LLM推理增强（如CoT、ToT）在DIU中应用的探索，也未涉及文档布局、表格、实体识别等关键DIU组件。因此，虽有‘agent’和‘reasoning’关键词，但技术路径和应用场景均不适用于DIU领域，不具备直接迁移价值。",
    "summary": "本文提出一种基于多智能体框架的AI漏洞注入系统，通过协调具有专家推理能力的AI代理与传统代码分析工具，在C/C++代码库中生成类别特定的漏洞数据集。利用检索增强生成进行上下文建模，并采用低秩权重微调提升效率。实验表明该方法在函数级漏洞注入上成功率达89%-95%，优于现有技术。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20848v1",
    "updated": "2025-08-28T14:40:27Z",
    "published": "2025-08-28T14:40:27Z",
    "title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring",
    "authors": [
      "Junjie Chu",
      "Mingjie Li",
      "Ziqing Yang",
      "Ye Leng",
      "Chenhao Lin",
      "Chao Shen",
      "Michael Backes",
      "Yun Shen",
      "Yang Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20848v1.pdf",
    "comment": "17 pages, 5 figures. For the code and data supporting this work, see https://trustairlab.github.io/jades.github.io/",
    "category": "Artificial Intelligence",
    "abstract": "Accurately determining whether a jailbreak attempt has succeeded is a fundamental yet unresolved challenge. Existing evaluation methods rely on misaligned proxy indicators or naive holistic judgments. They frequently misinterpret model responses, leading to inconsistent and subjective assessments that misalign with human perception. To address this gap, we introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal jailbreak evaluation framework. Its key mechanism is to automatically decompose an input harmful question into a set of weighted sub-questions, score each sub-answer, and weight-aggregate the sub-scores into a final decision. JADES also incorporates an optional fact-checking module to strengthen the detection of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a newly introduced benchmark proposed in this work, consisting of 400 pairs of jailbreak prompts and responses, each meticulously annotated by humans. In a binary setting (success/failure), JADES achieves 98.5% agreement with human evaluators, outperforming strong baselines by over 9%. Re-evaluating five popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show that JADES could deliver accurate, consistent, and interpretable evaluations, providing a reliable basis for measuring future jailbreak attacks.",
    "score": 2,
    "reason": "该论文聚焦于模型越狱攻击的评估框架JADES，核心是通过分解问题和加权评分来判断越狱是否成功。虽然涉及大语言模型（LLM）的安全性评估，但其研究目标与文档图像理解（DIU）、视觉多模态模型（VLM）或推理增强技术（如CoT、ToT）无直接关联。其方法论主要用于安全评测而非信息抽取、结构理解或智能体决策，无法直接迁移至DIU任务。尽管在顶会/顶刊中发表的可能性存在且代码开源，但内容与DIU领域无关，因此不具阅读价值。",
    "summary": "JADES提出一种基于分解评分的越狱攻击评估框架，通过将有害问题拆解为子问题并加权聚合得分，实现更准确、一致且可解释的越狱成功率判定。在新构建的JailbreakQR基准上达到98.5%的人类一致性，显著优于现有方法。同时引入事实核查模块以检测幻觉，有效纠正了多个主流攻击的过估计问题。"
  },
  {
    "id": "http://arxiv.org/abs/2508.14926v2",
    "updated": "2025-08-28T14:35:03Z",
    "published": "2025-08-19T14:24:02Z",
    "title": "Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving",
    "authors": [
      "Dianzhao Li",
      "Ostap Okhrin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.14926v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding robust ethical reasoning into routine and emergency maneuvers, particularly to protect vulnerable road users (VRUs) such as pedestrians and cyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that explicitly integrates moral considerations with standard driving objectives. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on rich, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing ethical risk and maintaining driving performance. To our knowledge, this is the first study of ethical decision-making for autonomous vehicles via Safe RL evaluated on real-world, human-mixed traffic scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy that explicitly protects those most at risk in urban traffic environments.",
    "score": 2,
    "reason": "该论文聚焦于自动驾驶中的伦理决策问题，虽涉及多模态感知与强化学习，但其核心是道德推理在车辆控制中的应用，与文档图像理解（DIU）、视觉语言模型（VLM）的推理增强、或智能体在文档处理中的应用无直接关联。尽管使用了安全强化学习和复杂决策机制，但其任务场景、数据类型和目标均与DIU领域无关，无法直接迁移至文档信息抽取、实体识别或关系建模等任务。因此不值得阅读。",
    "summary": "本文提出一种分层安全强化学习框架，用于在自动驾驶中融合道德考量，通过复合伦理风险成本优化驾驶决策，并结合动态经验回放和路径规划实现高安全性轨迹生成。在真实交通数据上验证了其在保护弱势道路使用者方面的有效性，但研究主题与文档图像理解无关。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20840v1",
    "updated": "2025-08-28T14:31:48Z",
    "published": "2025-08-28T14:31:48Z",
    "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic Learning",
    "authors": [
      "Qiao Sun",
      "Liujia Yang",
      "Wei Tang",
      "Wei Huang",
      "Kaixin Xu",
      "Yongchao Chen",
      "Mingyu Liu",
      "Jiange Yang",
      "Haoyi Zhu",
      "Yating Wang",
      "Tong He",
      "Yilun Chen",
      "Xili Dai",
      "Nanyang Ye",
      "Qinying Gu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20840v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \"GPT moment\" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.",
    "score": 3,
    "reason": "该论文聚焦于具身世界模型（Embodied World Models）的构建，核心是通过学习基础动作（primitive motions）来提升机器人学习的可扩展性。虽然涉及视觉-语言模型（VLM）和长时序生成，但其应用场景为机器人控制与物理交互，而非文档图像理解（DIU）。尽管VLM被用于规划，但其任务目标、数据模态（真实物理交互视频）与DIU领域差异显著，且未提及文档结构、布局分析、信息抽取或实体关系建模等关键环节。虽有部分技术如VLM规划可能间接启发DIU agent中的规划模块，但直接迁移应用价值低，不满足‘极其紧密相关’的要求。",
    "summary": "本文提出Primitive Embodied World Models (PEWM)，通过限制视频生成在短时间窗口内，利用基础动作建模实现更细粒度的语言-动作对齐，提升数据效率与推理速度，并结合VLM规划器与起止热图引导机制支持复杂任务的组合泛化。适用于机器人具身智能，但与文档图像理解无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21072v1",
    "updated": "2025-08-28T17:59:59Z",
    "published": "2025-08-28T17:59:59Z",
    "title": "First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge",
    "authors": [
      "Fahad Shamshad",
      "Tameem Bakr",
      "Yahia Shaaban",
      "Noor Hussein",
      "Karthik Nandakumar",
      "Nils Lukas"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21072v1.pdf",
    "comment": "Winning solution to the NeurIPS 2024 Erasing the Invisible challenge",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.",
    "score": 3,
    "reason": "该论文聚焦于对抗性去除不可见水印，属于图像安全与鲁棒性领域，虽涉及视觉模型（VAE、扩散模型）和多模态线索（ChatGPT生成描述），但核心目标是攻击水印而非文档理解。其方法与DIU任务无直接关联：未处理文档布局、文本语义识别或信息抽取，且未面向真实文档场景（如账单、简历）。尽管使用了扩散模型和LLM生成提示，但仅作为辅助手段用于图像修复，非用于结构化信息提取。因此，对DIU、VLM推理增强或agent系统构建无直接迁移价值。",
    "summary": "本文提出NeurIPS 2024隐形水印移除挑战赛的优胜方案，针对黑盒与灰盒攻击场景设计自适应VAE攻击与基于扩散模型的图像修复方法。通过测试时优化和CIELAB空间色彩恢复，在保留图像质量的前提下实现95.7%的水印移除率。研究重点在于评估并攻破现有水印鲁棒性，而非文档理解或信息抽取。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21070v1",
    "updated": "2025-08-28T17:59:55Z",
    "published": "2025-08-28T17:59:55Z",
    "title": "Dress&Dance: Dress up and Dance as You Like It - Technical Preview",
    "authors": [
      "Jun-Kun Chen",
      "Aayush Bansal",
      "Minh Phuoc Vo",
      "Yu-Xiong Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21070v1.pdf",
    "comment": "Project Page: https://immortalco.github.io/DressAndDance/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We present Dress&Dance, a video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a user wearing desired garments while moving in accordance with a given reference video. Our approach requires a single user image and supports a range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in a single pass. Key to our framework is CondNet, a novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and a larger, more readily available image dataset, in a multistage progressive manner. Dress&Dance outperforms existing open source and commercial solutions and enables a high quality and flexible try-on experience.",
    "score": 2,
    "reason": "该论文聚焦于虚拟试衣视频生成，属于视频生成与动作合成领域，尽管涉及多模态输入（图像、文本、视频）和条件控制，但其核心任务与文档图像理解（DIU）无直接关联。虽然CondNet使用注意力机制融合多模态信息，但应用场景为时尚试穿，不适用于文档结构解析、实体识别或关系抽取。此外，未提及任何与VLM推理增强、inference scaling、agent架构或视觉信息抽取相关的技术。因此，虽在技术上有一定创新性，但无法直接迁移至DIU领域，相关性极低。",
    "summary": "Dress&Dance提出一种基于扩散模型的视频生成框架，可生成高质量的5秒虚拟试衣视频，支持单图输入下多种衣物的同步试穿，并通过CondNet统一处理文本、图像和视频多模态输入以提升服装对齐与动作保真度。该方法在有限视频数据上结合大规模图像数据进行渐进式训练，实现高灵活性和视觉质量。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21066v1",
    "updated": "2025-08-28T17:59:46Z",
    "published": "2025-08-28T17:59:46Z",
    "title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning",
    "authors": [
      "Yuan Gong",
      "Xionghui Wang",
      "Jie Wu",
      "Shiyin Wang",
      "Yitong Wang",
      "Xinglong Wu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21066v1.pdf",
    "comment": "project url: https://one-reward.github.io",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \\textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io",
    "score": 4,
    "reason": "该论文聚焦于多任务掩码引导图像生成，使用统一的奖励模型（OneReward）进行强化学习训练，虽涉及视觉语言模型（VLM），但核心任务是图像编辑生成而非文档理解。其技术路径与DIU中OCR、实体识别、关系抽取等关键环节无直接关联，且未涉及文档布局、文本语义理解或信息抽取。尽管开源且为CVPR类会议（推测为顶会），但应用领域不匹配，无法直接迁移至DIU任务。因此不值得优先阅读。",
    "summary": "OneReward提出一种基于单一VLM作为奖励模型的多任务图像生成框架，用于掩码引导的图像编辑任务（如填充、扩展、移除对象和文本渲染）。通过多任务强化学习避免任务特定微调，显著提升生成质量。代码与模型已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21060v1",
    "updated": "2025-08-28T17:58:20Z",
    "published": "2025-08-28T17:58:20Z",
    "title": "Multi-View 3D Point Tracking",
    "authors": [
      "Frano Rajič",
      "Haofei Xu",
      "Marko Mihajlovic",
      "Siyuan Li",
      "Irem Demir",
      "Emircan Gündoğdu",
      "Lei Ke",
      "Sergey Prokudin",
      "Marc Pollefeys",
      "Siyu Tang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21060v1.pdf",
    "comment": "ICCV 2025, Oral. Project page: https://ethz-vlg.github.io/mvtracker",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page available at https://ethz-vlg.github.io/mvtracker.",
    "score": 2,
    "reason": "该论文聚焦于多视角3D点跟踪，属于计算机视觉中的动态场景重建与跟踪任务，与文档图像理解（DIU）、视觉语言模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。其技术栈（如多相机深度融合、Transformer更新机制）虽具创新性，但应用场景为动作捕捉与机器人感知，无法直接迁移至文档结构解析、文本语义理解或基于LLM的推理链构建。因此不值得在DIU研究中优先阅读。",
    "summary": "本文提出首个数据驱动的多视角3D点跟踪方法，利用4个摄像头实现在线、鲁棒的长距离3D对应关系估计，通过融合多视图特征与Transformer更新机制，在真实世界数据集上达到厘米级精度。该方法适用于动态场景下的物体运动追踪，但与文档图像理解、视觉语言模型或智能体系统无直接应用关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21058v1",
    "updated": "2025-08-28T17:57:55Z",
    "published": "2025-08-28T17:57:55Z",
    "title": "Mixture of Contexts for Long Video Generation",
    "authors": [
      "Shengqu Cai",
      "Ceyuan Yang",
      "Lvmin Zhang",
      "Yuwei Guo",
      "Junfei Xiao",
      "Ziyan Yang",
      "Yinghao Xu",
      "Zhenheng Yang",
      "Alan Yuille",
      "Leonidas Guibas",
      "Maneesh Agrawala",
      "Lu Jiang",
      "Gordon Wetzstein"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21058v1.pdf",
    "comment": "Project page: https://primecai.github.io/moc/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.",
    "score": 3,
    "reason": "该论文聚焦于长视频生成中的上下文记忆问题，提出Mixture of Contexts（MoC）机制以解决自注意力机制的二次计算开销。虽然其在长序列建模和高效记忆检索方面具有创新性，但研究对象为视频生成，与文档图像理解（DIU）、视觉多模态模型（VLM）或推理增强技术（如CoT、ToT）无直接关联。尽管其稀疏注意力机制可能间接启发DIU中布局信息的长期依赖建模，但缺乏明确的应用路径，且未涉及文本理解、实体识别或关系抽取等DIU核心任务。因此，相关性较低，仅因顶会潜力（CVPR领域）略加分。",
    "summary": "本文提出Mixture of Contexts（MoC）模块，用于解决长视频生成中自注意力机制带来的二次计算复杂度问题。通过动态选择关键历史片段与强制锚点进行稀疏注意力路由，实现近线性扩展的长程记忆能力，从而在分钟级视频生成中保持语义一致性与身份稳定性。项目页面已公开。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21052v1",
    "updated": "2025-08-28T17:55:14Z",
    "published": "2025-08-28T17:55:14Z",
    "title": "FakeParts: a New Family of AI-Generated DeepFakes",
    "authors": [
      "Gaetan Brison",
      "Soobash Daiboo",
      "Samy Aimeur",
      "Awais Hussain Sani",
      "Xi Wang",
      "Gianni Franchi",
      "Vicky Kalogeiton"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21052v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations.",
    "score": 2,
    "reason": "该论文聚焦于局部深度伪造（FakeParts）视频的生成与检测，属于计算机视觉中的多媒体安全方向。尽管其数据集构建和检测挑战具有价值，但核心内容与文档图像理解（DIU）、多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。其技术路径不适用于文档信息抽取、实体关系建模或基于LLM的推理架构优化，无法直接迁移至DIU领域。因此，相关性极低。",
    "summary": "本文提出FakeParts，一种通过局部区域（如面部表情、物体替换、背景修改）进行细微篡改的新型深度伪造视频，具有高度欺骗性。为应对检测难题，作者构建了FakePartsBench，一个包含超过2.5万段带像素级和帧级标注的视频数据集。用户研究表明，FakeParts使人类和现有检测模型的准确率下降超30%，揭示了当前检测方法在局部篡改场景下的严重漏洞。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21048v1",
    "updated": "2025-08-28T17:53:05Z",
    "published": "2025-08-28T17:53:05Z",
    "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
    "authors": [
      "Hao Tan",
      "Jun Lan",
      "Zichang Tan",
      "Ajian Liu",
      "Chuanbiao Song",
      "Senyuan Shi",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jun Wan",
      "Zhen Lei"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21048v1.pdf",
    "comment": "Project: https://github.com/EricTan7/Veritas",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as \"planning\" and \"self-reflection\" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs.",
    "score": 4,
    "reason": "该论文聚焦于深度伪造检测，虽使用了多模态大语言模型（MLLM）并引入了模式感知推理（pattern-aware reasoning），与DIU领域在技术路径上有一定交集（如推理机制、多模态建模），但其核心任务是伪造内容识别而非文档信息理解。尽管提出的方法涉及chain-of-thought类推理，但应用场景和目标函数与DIU无关，无法直接迁移用于实体识别或关系抽取等核心DIU任务。此外，虽然项目已开源且发表于CVPR（CCF A类会议），但其研究方向与DIU的核心需求关联较弱，故不推荐作为DIU研究的重点阅读。",
    "summary": "Veritas提出一种基于多模态大语言模型的深度伪造检测方法，通过引入'规划'和'自我反思'等推理模式模拟人类法医分析过程，并在新构建的HydraFake数据集上实现对未知伪造技术与数据域的强泛化能力。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21046v1",
    "updated": "2025-08-28T17:50:58Z",
    "published": "2025-08-28T17:50:58Z",
    "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
    "authors": [
      "Wei Li",
      "Renshan Zhang",
      "Rui Shao",
      "Jie He",
      "Liqiang Nie"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21046v1.pdf",
    "comment": "23 pages, 8 figures, Project Page: https://jiutian-vl.github.io/CogVLA-page",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.",
    "score": 9,
    "reason": "该论文提出CogVLA，一个认知对齐的视觉-语言-动作模型，通过指令驱动的路由与稀疏化机制显著提升效率与性能。其核心思想——在视觉编码和语言模型中引入指令感知的动态路由与剪枝策略——可直接迁移至DIU领域，用于构建更高效、低延迟的文档理解系统。尤其LFP-Routing实现语言模型中视觉相关token的稀疏化，有助于减少OCR后实体识别与关系抽取中的冗余计算。此外，该工作开源且发表于CVPR（CCF A类会议），具备高实用价值。虽然主要面向机器人任务，但其框架设计对DIU Agent中‘感知-决策-行动’链路具有启发意义。",
    "summary": "CogVLA提出一种认知对齐的视觉-语言-动作框架，通过三阶段渐进式架构：EFA-Routing在视觉编码器中注入指令信息以压缩多流视觉token；LFP-Routing在语言模型中基于动作意图剪枝无关视觉token，实现token级稀疏性；CAtten则通过耦合注意力机制保障动作生成的准确性。在LIBERO和真实机器人任务中表现卓越，训练成本降低2.5倍，推理延迟下降2.8倍，已开源，适用于DIU系统中高效多模态理解与决策模块的设计。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21044v1",
    "updated": "2025-08-28T17:50:03Z",
    "published": "2025-08-28T17:50:03Z",
    "title": "MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs",
    "authors": [
      "Junpeng Ma",
      "Qizhe Zhang",
      "Ming Lu",
      "Zhibin Wang",
      "Qiang Zhou",
      "Jun Song",
      "Shanghang Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21044v1.pdf",
    "comment": "10 pages, 3 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Video Large Language Models (VLLMs) excel in video understanding, but their excessive visual tokens pose a significant computational challenge for real-world applications. Current methods aim to enhance inference efficiency by visual token pruning. However, they do not consider the dynamic characteristics and temporal dependencies of video frames, as they perceive video understanding as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel training-free visual token pruning framework that removes redundancy by Maximizing Marginal Gains at both segment-level and token-level. Specifically, we first divide the video into segments based on frame similarity, and then dynamically allocate the token budget for each segment to maximize the marginal gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm that jointly models inter-frame uniqueness and intra-frame diversity, thereby maximizing the marginal gain of each token. By combining both stages, MMG-Vid can maximize the utilization of the limited token budget, significantly improving efficiency while maintaining strong performance. Extensive experiments demonstrate that MMG-Vid can maintain over 99.5% of the original performance, while effectively reducing 75% visual tokens and accelerating the prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.",
    "score": 6,
    "reason": "该论文聚焦于视频大模型的视觉令牌压缩，提出一种在段级和词元级最大化边际收益的训练无关剪枝框架MMG-Vid。虽然其核心思想——通过动态分配token预算提升效率——对DIU中处理高分辨率文档图像的视觉编码阶段具有潜在启发意义（如减少布局分析中的冗余信息），但研究对象是视频而非静态文档图像，且未涉及文本理解、实体识别或关系抽取等DIU关键任务。尽管方法可能间接适用于文档图像的视觉表示优化，但直接迁移应用性较弱，且缺乏与OCR、结构理解或多模态推理的结合。因此仅具有限参考价值。",
    "summary": "MMG-Vid提出一种无需训练的视觉令牌剪枝框架，通过基于帧相似性的分段与时间引导的DPC算法，在保持99.5%性能的同时减少75%视觉令牌并加速预填充阶段，适用于高效视频LLM推理。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21041v1",
    "updated": "2025-08-28T17:45:22Z",
    "published": "2025-08-28T17:45:22Z",
    "title": "Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025",
    "authors": [
      "Guillaume Balezo",
      "Raphaël Bourgade",
      "Thomas Walter"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21041v1.pdf",
    "comment": "3 pages. Challenge report for MIDOG 2025 (Task 2: Atypical Mitotic Figure Classification)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Atypical mitotic figures (AMFs) are markers of abnormal cell division associated with poor prognosis, yet their detection remains difficult due to low prevalence, subtle morphology, and inter-observer variability. The MIDOG 2025 challenge introduces a benchmark for AMF classification across multiple domains. In this work, we evaluate the recently published DINOv3-H+ vision transformer, pretrained on natural images, which we fine-tuned using low-rank adaptation (LoRA, 650k trainable parameters) and extensive augmentation. Despite the domain gap, DINOv3 transfers effectively to histopathology, achieving a balanced accuracy of 0.8871 on the preliminary test set. These results highlight the robustness of DINOv3 pretraining and show that, when combined with parameter-efficient fine-tuning, it provides a strong baseline for atypical mitosis classification in MIDOG 2025.",
    "score": 2,
    "reason": "该论文聚焦于医学图像中异常有丝分裂图像的分类，属于病理图像分析领域，与文档图像理解（DIU）无直接关联。尽管使用了DINOv3和LoRA等技术，但其应用场景（组织病理学）、任务目标（疾病诊断）与DIU差异显著，且未涉及文本识别、实体抽取或关系建模等核心DIU环节。虽然多模态预训练模型在跨域迁移方面具有参考价值，但该工作并非面向文档理解，无法直接应用于DIU任务，故不值得阅读。",
    "summary": "本研究针对MIDOG 2025挑战赛中的异常有丝分裂图像分类任务，采用在自然图像上预训练的DINOv3-H+模型，通过低秩适配（LoRA）和数据增强进行微调，在初步测试集上达到0.8871的平衡准确率。结果表明DINOv3具备良好的跨域迁移能力，可作为病理图像分类的强基线，但该工作不涉及文档理解、OCR、文本语义解析或结构化信息抽取，与DIU领域无直接应用价值。"
  },
  {
    "id": "http://arxiv.org/abs/2508.10950v2",
    "updated": "2025-08-28T17:44:54Z",
    "published": "2025-08-13T17:56:29Z",
    "title": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement",
    "authors": [
      "Xinyi Wang",
      "Michael Barnett",
      "Frederique Boonstra",
      "Yael Barnett",
      "Mariano Cabezas",
      "Arkiev D'Souza",
      "Matthew C. Kiernan",
      "Kain Kyle",
      "Meng Law",
      "Lynette Masters",
      "Zihao Tang",
      "Stephen Tisch",
      "Sicong Tu",
      "Anneke Van Der Walt",
      "Dongang Wang",
      "Fernando Calamante",
      "Weidong Cai",
      "Chenyu Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.10950v2.pdf",
    "comment": "24 pages, 5 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling technique that represents complex white matter fiber configurations, and a key step for subsequent brain tractography and connectome analysis. Its reliability and accuracy, however, heavily rely on the quality of the MRI acquisition and the subsequent estimation of the FODs at each voxel. Generating reliable FODs from widely available clinical protocols with single-shell and low-angular-resolution acquisitions remains challenging but could potentially be addressed with recent advances in deep learning-based enhancement techniques. Despite advancements, existing methods have predominantly been assessed on healthy subjects, which have proved to be a major hurdle for their clinical adoption. In this work, we validate a newly optimized enhancement framework, FastFOD-Net, across healthy controls and six neurological disorders. This accelerated end-to-end deep learning framework enhancing FODs with superior performance and delivering training/inference efficiency for clinical use ($60\\times$ faster comparing to its predecessor). With the most comprehensive clinical evaluation to date, our work demonstrates the potential of FastFOD-Net in accelerating clinical neuroscience research, empowering diffusion MRI analysis for disease differentiation, improving interpretability in connectome applications, and reducing measurement errors to lower sample size requirements. Critically, this work will facilitate the more widespread adoption of, and build clinical trust in, deep learning based methods for diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of real-world, clinical diffusion MRI data, comparable to that achievable with high-quality research acquisitions.",
    "score": 2,
    "reason": "该论文聚焦于扩散磁共振成像（Diffusion MRI）中纤维取向分布（FOD）的深度学习增强，属于医学影像分析领域，与文档图像理解（DIU）、视觉语言模型（VLM）、大语言模型（LLM）推理增强或智能体（Agent）等主题无直接关联。尽管其使用了深度学习提升效率，但应用场景、数据模态（MRI）和任务目标均与DIU无关，无法直接迁移至文档理解领域。",
    "summary": "本文提出FastFOD-Net，一种用于加速和提升扩散MRI中纤维取向分布（FOD）估计的端到端深度学习框架。在健康对照及六种神经疾病患者数据上进行了全面临床验证，相比前代方法提速60倍，显著提升临床可用性与可靠性，推动深度学习在真实世界脑成像分析中的落地。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21040v1",
    "updated": "2025-08-28T17:44:52Z",
    "published": "2025-08-28T17:44:52Z",
    "title": "FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator",
    "authors": [
      "Huynh Tong Dang Khoa",
      "Dang Hoai Nam",
      "Vo Nguyen Le Duy"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21040v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Labeled handwriting data is often scarce, limiting the effectiveness of recognition systems that require diverse, style-consistent training samples. Handwriting synthesis offers a promising solution by generating artificial data to augment training. However, current methods face two major limitations. First, most are built on conventional convolutional architectures, which struggle to model long-range dependencies and complex stroke patterns. Second, they largely ignore the crucial role of frequency information, which is essential for capturing fine-grained stylistic and structural details in handwriting. To address these challenges, we propose FW-GAN, a one-shot handwriting synthesis framework that generates realistic, writer-consistent text from a single example. Our generator integrates a phase-aware Wave-MLP to better capture spatial relationships while preserving subtle stylistic cues. We further introduce a frequency-guided discriminator that leverages high-frequency components to enhance the authenticity detection of generated samples. Additionally, we introduce a novel Frequency Distribution Loss that aligns the frequency characteristics of synthetic and real handwriting, thereby enhancing visual fidelity. Experiments on Vietnamese and English handwriting datasets demonstrate that FW-GAN generates high-quality, style-consistent handwriting, making it a valuable tool for augmenting data in low-resource handwriting recognition (HTR) pipelines. Official implementation is available at https://github.com/DAIR-Group/FW-GAN",
    "score": 5,
    "reason": "该论文提出FW-GAN用于手写体合成，虽在数据增强方面对文档识别有潜在价值，但其核心是基于Wave-MLP的生成模型，聚焦于单样本手写体生成，与DIU任务中OCR、实体识别、关系抽取等核心环节无直接关联。虽然生成数据可用于训练HTR模型，但属于上游数据生成技术，无法直接应用于DIU的端到端理解或推理阶段。且未涉及VLM、LLM推理增强或agent架构，不满足‘可直接迁移应用’的要求。尽管开源，但相关性较弱。",
    "summary": "FW-GAN提出一种基于相位感知Wave-MLP和频率引导判别器的手写体合成框架，通过引入频率分布损失提升生成样本的视觉真实感，支持从单个样本生成风格一致的手写文本，适用于低资源场景下的手写识别数据增强。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21035v1",
    "updated": "2025-08-28T17:39:30Z",
    "published": "2025-08-28T17:39:30Z",
    "title": "A multi-task neural network for atypical mitosis recognition under domain shift",
    "authors": [
      "Gennaro Percannella",
      "Mattia Sarno",
      "Francesco Tortorella",
      "Mario Vento"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21035v1.pdf",
    "comment": "Approach for MIDOG25 track 2",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recognizing atypical mitotic figures in histopathology images allows physicians to correctly assess tumor aggressiveness. Although machine learning models could be exploited for automatically performing such a task, under domain shift these models suffer from significative performance drops. In this work, an approach based on multi-task learning is proposed for addressing this problem. By exploiting auxiliary tasks, correlated to the main classification task, the proposed approach, submitted to the track 2 of the MItosis DOmain Generalization (MIDOG) challenge, aims to aid the model to focus only on the object to classify, ignoring the domain varying background of the image. The proposed approach shows promising performance in a preliminary evaluation conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25 challenge.",
    "score": 2,
    "reason": "该论文聚焦于病理图像中非典型有丝分裂的识别，属于医学图像分析领域，与文档图像理解（DIU）无直接关联。尽管提到了多任务学习和域泛化，但其应用场景、数据类型（组织切片图像）和任务目标（肿瘤恶性程度评估）均与DIU无关。虽为CCF A类会议（MIDOG挑战赛通常在CVPR等顶会中进行），但应用方向不匹配，无法直接迁移至DIU任务。因此不值得阅读。",
    "summary": "本文提出一种基于多任务学习的神经网络模型，用于在域偏移条件下识别病理图像中的非典型有丝分裂。通过引入相关辅助任务，帮助模型关注目标对象而非背景变化，已在三个数据集上验证了初步性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21033v1",
    "updated": "2025-08-28T17:38:30Z",
    "published": "2025-08-28T17:38:30Z",
    "title": "Mitosis detection in domain shift scenarios: a Mamba-based approach",
    "authors": [
      "Gennaro Percannella",
      "Mattia Sarno",
      "Francesco Tortorella",
      "Mario Vento"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21033v1.pdf",
    "comment": "Approach for MIDOG 2025 track 1",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Mitosis detection in histopathology images plays a key role in tumor assessment. Although machine learning algorithms could be exploited for aiding physicians in accurately performing such a task, these algorithms suffer from significative performance drop when evaluated on images coming from domains that are different from the training ones. In this work, we propose a Mamba-based approach for mitosis detection under domain shift, inspired by the promising performance demonstrated by Mamba in medical imaging segmentation tasks. Specifically, our approach exploits a VM-UNet architecture for carrying out the addressed task, as well as stain augmentation operations for further improving model robustness against domain shift. Our approach has been submitted to the track 1 of the MItosis DOmain Generalization (MIDOG) challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show large room for improvement for the proposed method.",
    "score": 2,
    "reason": "该论文聚焦于病理图像中的有丝分裂检测，属于医学图像分析领域，研究的是在域偏移场景下的模型泛化能力。虽然使用了Mamba架构并涉及数据增强，但其任务与文档图像理解（DIU）无直接关联，且未涉及OCR、实体识别、关系抽取或多模态推理等DIU核心环节。此外，论文内容与VLM、LLM推理增强、agent系统等前沿方向无关，无法直接迁移至DIU领域。因此不值得阅读。",
    "summary": "本文提出一种基于Mamba的VM-UNet架构用于在域偏移情况下进行病理图像中的有丝分裂检测，结合染色增强提升模型鲁棒性，并参与MIDOG 2025挑战赛。实验表明方法仍有较大改进空间。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21032v1",
    "updated": "2025-08-28T17:35:03Z",
    "published": "2025-08-28T17:35:03Z",
    "title": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets",
    "authors": [
      "Dale Decatur",
      "Thibault Groueix",
      "Wang Yifan",
      "Rana Hanocka",
      "Vladimir Kim",
      "Matheus Gadelha"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21032v1.pdf",
    "comment": "ICCV 2025. Project page: https://ddecatur.github.io/hierarchical-diffusion/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: https://ddecatur.github.io/hierarchical-diffusion/",
    "score": 2,
    "reason": "该论文聚焦于文本到图像扩散模型中的计算重用问题，旨在通过共享早期去噪步骤来提高大规模图像生成的效率。其核心贡献在于优化生成多个相关图像时的计算冗余，与DIU、VLM、LLM推理增强或agent系统无直接关联。虽然涉及多模态生成，但方向为图像生成而非文档理解，且未涉及文本理解、信息抽取、关系建模或推理链构建等DIU关键任务。因此，不适用于DIU领域的技术迁移，相关性较低。",
    "summary": "本文提出一种无需训练的计算重用方法，通过基于语义相似性的提示聚类，在扩散模型的早期去噪步骤中共享计算，从而降低生成一组相关图像的计算成本。该方法利用扩散模型的粗粒度到细粒度特性，并结合UnClip的文本-图像先验优化扩散步分配，显著提升效率并改善图像质量。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21019v1",
    "updated": "2025-08-28T17:20:01Z",
    "published": "2025-08-28T17:20:01Z",
    "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models",
    "authors": [
      "Jiaxiang Cheng",
      "Bing Ma",
      "Xuhua Ren",
      "Hongyi Jin",
      "Kai Yu",
      "Peng Zhang",
      "Wenyue Li",
      "Yuan Zhou",
      "Tianxiang Zheng",
      "Qinglin Lu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21019v1.pdf",
    "comment": "Project Page: https://pose-paper.github.io",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.",
    "score": 2,
    "reason": "该论文聚焦于视频扩散模型的采样效率优化，提出POSE框架实现单步生成高质量视频。尽管在技术上有创新性，但其核心任务是视频生成而非文档图像理解（DIU），且与视觉多模态模型（VLM）、大语言模型（LLM）推理增强（inference scaling）、智能体（agent）等直接相关方向无紧密联系。论文主题与DIU领域中OCR、实体识别、关系抽取、布局建模或视觉信息抽取等关键问题无关，也无法直接迁移至DIU任务中。因此不值得阅读。",
    "summary": "POSE提出一种两阶段对抗平衡蒸馏框架，用于加速大规模视频扩散模型的采样过程，实现单步生成高质量视频。通过稳定性预热和统一对抗均衡机制提升生成质量与一致性，并在条件生成中引入语义与帧间一致性约束。实验表明其显著降低延迟（100倍），优于现有方法。"
  },
  {
    "id": "http://arxiv.org/abs/2505.07818v4",
    "updated": "2025-08-28T17:19:45Z",
    "published": "2025-05-12T17:59:34Z",
    "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
    "authors": [
      "Zeyue Xue",
      "Jie Wu",
      "Yu Gao",
      "Fangyuan Kong",
      "Lingting Zhu",
      "Mengzhao Chen",
      "Zhiheng Liu",
      "Wei Liu",
      "Qiushan Guo",
      "Weilin Huang",
      "Ping Luo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.07818v4.pdf",
    "comment": "Project Page: https://dancegrpo.github.io/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in generative AI have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. While Reinforcement Learning (RL) has emerged as a promising approach for fine-tuning generative models, existing methods like DDPO and DPOK face fundamental limitations - particularly their inability to maintain stable optimization when scaling to large and diverse prompt sets, severely restricting their practical utility. This paper presents DanceGRPO, a framework that addresses these limitations through an innovative adaptation of Group Relative Policy Optimization (GRPO) for visual generation tasks. Our key insight is that GRPO's inherent stability mechanisms uniquely position it to overcome the optimization challenges that plague prior RL-based approaches on visual generation. DanceGRPO establishes several significant advances: First, it demonstrates consistent and stable policy optimization across multiple modern generative paradigms, including both diffusion models and rectified flows. Second, it maintains robust performance when scaling to complex, real-world scenarios encompassing three key tasks and four foundation models. Third, it shows remarkable versatility in optimizing for diverse human preferences as captured by five distinct reward models assessing image/video aesthetics, text-image alignment, video motion quality, and binary feedback. Our comprehensive experiments reveal that DanceGRPO outperforms baseline methods by up to 181\\% across multiple established benchmarks, including HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis.",
    "score": 3,
    "reason": "该论文聚焦于视觉生成中的强化学习对齐问题，提出DanceGRPO框架用于提升生成模型与人类偏好的一致性。虽然涉及视觉生成和RLHF，但其核心任务是图像/视频生成的优化，而非文档理解、信息抽取或结构化信息建模。尽管使用了多模态技术，但应用场景与DIU无关，且未提及任何与OCR、实体识别、关系抽取或文档布局相关的技术。此外，虽有项目页面和开源信息，但内容不直接适用于DIU领域。因此，相关性较低，仅在技术范式上略有参考价值。",
    "summary": "DanceGRPO提出一种基于Group Relative Policy Optimization（GRPO）的强化学习框架，用于提升视觉生成模型（如扩散模型和修正流）在多种复杂任务中的输出质量与人类偏好对齐。通过稳定优化机制，该方法在多个基准测试中显著优于现有方法，适用于图像/视频生成中的RLHF任务，但与文档图像理解无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2301.06375v2",
    "updated": "2025-08-28T17:12:29Z",
    "published": "2023-01-16T11:40:50Z",
    "title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset",
    "authors": [
      "Jeongkyun Park",
      "Jung-Wook Hwang",
      "Kwanghee Choi",
      "Seung-Hyun Lee",
      "Jun Hwan Ahn",
      "Rae-Hong Park",
      "Hyung-Min Park"
    ],
    "pdf_url": "https://arxiv.org/pdf/2301.06375v2.pdf",
    "comment": "Accepted to ICASSP 2024",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the OLKAVS dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis.",
    "score": 2,
    "reason": "该论文聚焦于韩语音频-视觉语音数据集的构建，属于多模态语音识别领域，与文档图像理解（DIU）无直接关联。尽管涉及多模态数据，但其核心任务是语音识别与唇读，而非文本理解、实体识别或关系抽取等DIU关键环节。同时，未涉及视觉语言模型（VLM）、推理增强（inference scaling）、agent系统等与DIU高度相关的技术方向。虽被ICASSP 2024接收（CCF B类），但应用范围不匹配，无法直接迁移至DIU任务。",
    "summary": "OLKAVS是一个大规模韩语音视频语音数据集，包含1,150小时来自1,107名说话者的多视角音频和对应文本，支持音频-视觉语音识别与唇读任务。研究验证了多视角与多模态训练优于单模态和单一视角方法。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21010v1",
    "updated": "2025-08-28T17:10:53Z",
    "published": "2025-08-28T17:10:53Z",
    "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Basura Fernando"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21010v1.pdf",
    "comment": "Project page: https://paritoshparmar.github.io/chainreaction/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/",
    "score": 4,
    "reason": "该论文聚焦于因果视频问答（Causal VideoQA），提出使用自然语言因果链作为中间表示来提升可解释性与推理能力。虽然其在推理机制设计上与DIU领域中对逻辑链和结构化推理的需求有一定关联，但核心任务是视频理解中的因果推理，而非文档图像理解。尽管引入了chain-of-thought类的结构化中间表示，但应用场景为视频而非文档图像，且未涉及OCR、实体识别、布局分析等DIU关键组件。此外，未提及与VLM、LLM推理扩展技术或agent系统的结合。因此虽有方法论上的启发意义，但直接迁移应用价值较低，仅具备弱相关性。",
    "summary": "本文提出ChainReaction框架，通过将因果推理过程显式建模为自然语言因果链，实现可解释的视频问答。系统分为因果链提取器（CCE）与因果链驱动的答案生成器（CCDA），并利用大语言模型自动生成高质量因果链标注。实验表明该方法在多个基准上优于SOTA，并显著提升可解释性与泛化能力。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20991v1",
    "updated": "2025-08-28T16:53:03Z",
    "published": "2025-08-28T16:53:03Z",
    "title": "ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts",
    "authors": [
      "Patryk Będkowski",
      "Jan Dubiński",
      "Filip Szatkowski",
      "Kamil Deja",
      "Przemysław Rokita",
      "Tomasz Trzciński"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20991v1.pdf",
    "comment": "Accepted at ECAI 2025 28th European Conference on Artificial Intelligence",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.",
    "score": 2,
    "reason": "该论文聚焦于粒子物理中的探测器模拟，使用混合生成专家架构加速蒙特卡洛仿真。虽然涉及生成模型和高效计算，但其应用场景（高能物理实验）与文档图像理解（DIU）无直接关联，且不涉及视觉-文本理解、布局分析、实体识别或关系抽取等核心DIU任务。尽管代码开源且被ECAI 2025接收（CCF-B类），但技术路径与DIU无关，无法直接迁移应用。因此不值得在DIU研究中重点关注。",
    "summary": "ExpertSim提出一种基于混合生成专家的深度学习方法，用于加速ALICE实验中零度量能器的探测器响应模拟。通过为不同数据子集分配专用专家，提升生成精度与效率，显著优于传统蒙特卡洛方法。代码已开源，发表于ECAI 2025。"
  },
  {
    "id": "http://arxiv.org/abs/2312.05407v4",
    "updated": "2025-08-28T16:47:53Z",
    "published": "2023-12-08T23:43:17Z",
    "title": "ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation",
    "authors": [
      "Md Shazid Islam",
      "Sayak Nag",
      "Arindam Dutta",
      "Miraj Ahmed",
      "Fahim Faisal Niloy",
      "Shreyangshu Bera",
      "Amit K. Roy-Chowdhury"
    ],
    "pdf_url": "https://arxiv.org/pdf/2312.05407v4.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Unsupervised domain adaptive segmentation typically relies on self-training using pseudo labels predicted by a pre-trained network on an unlabeled target dataset. However, the noisy nature of such pseudo-labels presents a major bottleneck in adapting a network to the distribution shift between source and target datasets. This challenge is exaggerated when the network encounters an incoming data stream in online fashion, where the network is constrained to adapt to incoming streams of target domain data in exactly one round of forward and backward passes. In this scenario, relying solely on inaccurate pseudo-labels can lead to low-quality segmentation, which is detrimental to medical image analysis where accuracy and precision are of utmost priority. We hypothesize that a small amount of pixel-level annotation obtained from an expert can address this problem, thereby enhancing the performance of domain adaptation of online streaming data, even in the absence of dedicated training data. We call our method ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation that adapts to each incoming data batch in an online setup, incorporating feedback from an expert through active learning. Through active learning, the most informative pixels in each image can be selected for expert annotation. However, the acquisition of pixel-level annotations across all images in a batch often leads to redundant information while increasing temporal overhead in online learning. To reduce the annotation acquisition time and make the adaptation process more online-friendly, we further propose a novel image-pruning strategy that selects the most useful subset of images from the current batch for active learning. Our proposed approach outperforms existing online adaptation approaches and produces competitive results compared to offline domain adaptive active learning methods.",
    "score": 2,
    "reason": "该论文聚焦于医学图像分割中的在线领域自适应，核心方法为引入专家指导的主动学习与图像剪枝策略。虽然涉及领域自适应和在线学习，但其研究对象为医学图像分割（medical image segmentation），而非文档图像理解（DIU）。所用任务、数据类型（医学影像）与技术场景（像素级分割）与DIU无直接关联，无法直接迁移至文档文本识别、实体抽取或关系建模等环节。此外，未涉及OCR、多模态模型、推理增强或智能体系统，与LLM/VLM中inference scaling或agent方向无关。因此，不值得在DIU研究中优先阅读。",
    "summary": "本文提出ODES方法，用于在线医学图像分割中的领域自适应，通过专家引导的主动学习和图像剪枝策略，在有限标注资源下提升模型对目标域数据的适应能力。该方法在每个批次中仅选择最具信息量的图像进行专家标注，以降低时间开销并提高分割精度。尽管在性能上优于现有在线方法，但其应用场景为医学图像，与文档图像理解无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2507.14743v3",
    "updated": "2025-08-28T16:45:48Z",
    "published": "2025-07-19T20:30:43Z",
    "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic",
    "authors": [
      "Joseph Raj Vishal",
      "Divesh Basina",
      "Rutuja Patil",
      "Manas Srinivas Gowda",
      "Katha Naik",
      "Yezhou Yang",
      "Bharatesh Chakravarthi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.14743v3.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Traffic monitoring is crucial for urban mobility, road safety, and intelligent transportation systems (ITS). Deep learning has advanced video-based traffic monitoring through video question answering (VideoQA) models, enabling structured insight extraction from traffic videos. However, existing VideoQA models struggle with the complexity of real-world traffic scenes, where multiple concurrent events unfold across spatiotemporal dimensions. To address these challenges, this paper introduces \\textbf{InterAct VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of real-world traffic footage collected from diverse intersections, segmented into 10-second video clips, with over 25,000 question-answer (QA) pairs covering spatiotemporal dynamics, vehicle interactions, incident detection, and other critical traffic attributes. State-of-the-art VideoQA models are evaluated on InterAct VideoQA, exposing challenges in reasoning over fine-grained spatiotemporal dependencies within complex traffic scenarios. Additionally, fine-tuning these models on InterAct VideoQA yields notable performance improvements, demonstrating the necessity of domain-specific datasets for VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to facilitate future research in real-world deployable VideoQA models for intelligent transportation systems. GitHub Repo: https://github.com/joe-rabbit/InterAct_VideoQA",
    "score": 3,
    "reason": "该论文聚焦于视频问答（VideoQA）在城市交通监控中的应用，虽涉及复杂场景下的推理任务，但其核心是视频理解而非文档图像理解。尽管提到‘reasoning-rich’和spatiotemporal推理，但应用场景（交通视频）与DIU（文档图像）差异较大，且未涉及OCR、实体识别、关系抽取或布局分析等DIU关键组件。虽然数据集公开且规模较大，具备一定参考价值，但直接应用于DIU的潜力极低，仅可作为跨模态推理技术的间接启发，不具紧密关联性。",
    "summary": "本文提出InterAct VideoQA数据集，包含8小时真实交通视频及2.5万个问答对，用于评估和提升视频问答模型在复杂交通场景中的推理能力。实验表明现有模型在时空依赖建模上表现不足，微调可显著提升性能。数据集已开源，旨在推动智能交通系统的可部署视频理解研究。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20987v1",
    "updated": "2025-08-28T16:44:40Z",
    "published": "2025-08-28T16:44:40Z",
    "title": "Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation",
    "authors": [
      "Chenfan Qu",
      "Yiwu Zhong",
      "Bin Li",
      "Lianwen Jin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20987v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Images manipulated using image editing tools can mislead viewers and pose significant risks to social security. However, accurately localizing the manipulated regions within an image remains a challenging problem. One of the main barriers in this area is the high cost of data acquisition and the severe lack of high-quality annotated datasets. To address this challenge, we introduce novel methods that mitigate data scarcity by leveraging readily available web data. We utilize a large collection of manually forged images from the web, as well as automatically generated annotations derived from a simpler auxiliary task, constrained image manipulation localization. Specifically, we introduce a new paradigm CAAAv2, which automatically and accurately annotates manipulated regions at the pixel level. To further improve annotation quality, we propose a novel metric, QES, which filters out unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a large-scale, diverse, and high-quality dataset containing 246,212 manually forged images with pixel-level mask annotations. This is over 120x larger than existing handcrafted datasets like IMD20. Additionally, we introduce Object Jitter, a technique that further enhances model training by generating high-quality manipulation artifacts. Building on these advances, we develop a new model, Web-IML, designed to effectively leverage web-scale supervision for the image manipulation localization task. Extensive experiments demonstrate that our approach substantially alleviates the data scarcity problem and significantly improves the performance of various models on multiple real-world forgery benchmarks. With the proposed web supervision, Web-IML achieves a striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1 average IoU points. The dataset and code will be made publicly available at https://github.com/qcf-568/MIML.",
    "score": 2,
    "reason": "该论文聚焦于图像篡改定位（image manipulation localization），属于计算机视觉中的图像取证方向，与文档图像理解（DIU）无直接关联。尽管其提出的数据集和方法在数据生成与标注方面有创新，但核心任务是检测图像编辑痕迹，而非解析文档内容、实体或关系。此外，论文未涉及OCR、实体识别、关系抽取等DIU关键环节，也未使用VLM或LLM进行推理增强，且不涉及inference scaling、reasoning技术或agent系统。因此，无法直接应用于DIU领域，相关性极低。",
    "summary": "本文提出一种基于网络数据的弱监督图像篡改定位方法，通过类别感知自动标注（CAAAv2）和新提出的QES过滤机制构建大规模像素级标注数据集MIMLv2，并设计Web-IML模型利用网络规模监督提升篡改区域定位性能。实验表明该方法显著优于现有SOTA，但研究目标为图像伪造检测，与文档图像理解无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20981v1",
    "updated": "2025-08-28T16:36:02Z",
    "published": "2025-08-28T16:36:02Z",
    "title": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection",
    "authors": [
      "Jiajie Li",
      "Boyang Sun",
      "Luca Di Giammarino",
      "Hermann Blum",
      "Marc Pollefeys"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20981v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Reliable localization is critical for robot navigation, yet most existing systems implicitly assume that all viewing directions at a location are equally informative. In practice, localization becomes unreliable when the robot observes unmapped, ambiguous, or uninformative regions. To address this, we present ActLoc, an active viewpoint-aware planning framework for enhancing localization accuracy for general robot navigation tasks. At its core, ActLoc employs a largescale trained attention-based model for viewpoint selection. The model encodes a metric map and the camera poses used during map construction, and predicts localization accuracy across yaw and pitch directions at arbitrary 3D locations. These per-point accuracy distributions are incorporated into a path planner, enabling the robot to actively select camera orientations that maximize localization robustness while respecting task and motion constraints. ActLoc achieves stateof-the-art results on single-viewpoint selection and generalizes effectively to fulltrajectory planning. Its modular design makes it readily applicable to diverse robot navigation and inspection tasks.",
    "score": 3,
    "reason": "该论文聚焦于机器人导航中的主动视角选择以提升定位精度，虽涉及视觉感知与空间推理，但核心任务是机器人运动规划与地图构建，与文档图像理解（DIU）无直接关联。其方法未涉及文本识别、实体抽取或关系建模，也未使用多模态模型处理文档内容，且不涉及VLM/LLM的inference scaling或agent系统设计。尽管属于计算机视觉领域，但应用场景和核心技术均与DIU无关，无法直接迁移应用。",
    "summary": "ActLoc提出一种基于大规模训练注意力模型的主动视角选择框架，用于提升机器人在导航过程中的定位准确性。通过预测不同视角下的定位置信度分布，并将其融入路径规划，使机器人能自主选择最优观测方向，从而增强定位鲁棒性。该方法在单视角选择和轨迹规划中均达到SOTA性能，具有良好的泛化能力。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20965v1",
    "updated": "2025-08-28T16:22:54Z",
    "published": "2025-08-28T16:22:54Z",
    "title": "DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes",
    "authors": [
      "Yajiao Xiong",
      "Xiaoyu Zhou",
      "Yongtao Wan",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20965v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We present DrivingGaussian++, an efficient and effective framework for realistic reconstructing and controllable editing of surrounding dynamic autonomous driving scenes. DrivingGaussian++ models the static background using incremental 3D Gaussians and reconstructs moving objects with a composite dynamic Gaussian graph, ensuring accurate positions and occlusions. By integrating a LiDAR prior, it achieves detailed and consistent scene reconstruction, outperforming existing methods in dynamic scene reconstruction and photorealistic surround-view synthesis. DrivingGaussian++ supports training-free controllable editing for dynamic driving scenes, including texture modification, weather simulation, and object manipulation, leveraging multi-view images and depth priors. By integrating large language models (LLMs) and controllable editing, our method can automatically generate dynamic object motion trajectories and enhance their realism during the optimization process. DrivingGaussian++ demonstrates consistent and realistic editing results and generates dynamic multi-view driving scenarios, while significantly enhancing scene diversity. More results and code can be found at the project site: https://xiong-creator.github.io/DrivingGaussian_plus.github.io",
    "score": 2,
    "reason": "该论文聚焦于自动驾驶场景的动态三维重建与可控编辑，虽引入了LLM用于生成动态物体运动轨迹，但其核心任务是3D场景建模与渲染，与文档图像理解（DIU）无直接关联。尽管使用了LLM，但应用场景、数据模态（街景vs文档图像）和目标（视觉合成vs信息抽取）均不匹配，无法直接迁移至DIU领域。同时，论文未涉及OCR、实体识别、关系抽取或文档布局分析等关键DIU任务，也未使用多模态模型处理文档结构。因此，不值得在DIU研究中优先阅读。",
    "summary": "DrivingGaussian++ 提出一种基于增量3D高斯和动态高斯图的框架，用于实现自动驾驶场景的逼真重建与无需训练的可控编辑，结合LiDAR先验和多视图图像提升细节一致性，并利用LLM生成动态物体运动轨迹以增强真实感。该方法主要面向动态驾驶环境的可视化与仿真，不适用于文档图像理解任务。"
  },
  {
    "id": "http://arxiv.org/abs/2412.14195v3",
    "updated": "2025-08-28T16:18:41Z",
    "published": "2024-12-13T11:29:05Z",
    "title": "A multimodal dataset for understanding the impact of mobile phones on remote online virtual education",
    "authors": [
      "Roberto Daza",
      "Alvaro Becerra",
      "Ruth Cobos",
      "Julian Fierrez",
      "Aythami Morales"
    ],
    "pdf_url": "https://arxiv.org/pdf/2412.14195v3.pdf",
    "comment": "Published in Scientific Data (Nature). GitHub repository of the dataset at: https://github.com/BiDAlab/IMPROVE",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This work presents the IMPROVE dataset, a multimodal resource designed to evaluate the effects of mobile phone usage on learners during online education. It includes behavioral, biometric, physiological, and academic performance data collected from 120 learners divided into three groups with different levels of phone interaction, enabling the analysis of the impact of mobile phone usage and related phenomena such as nomophobia. A setup involving 16 synchronized sensors-including EEG, eye tracking, video cameras, smartwatches, and keystroke dynamics-was used to monitor learner activity during 30-minute sessions involving educational videos, document reading, and multiple-choice tests. Mobile phone usage events, including both controlled interventions and uncontrolled interactions, were labeled by supervisors and refined through a semi-supervised re-labeling process. Technical validation confirmed signal quality, and statistical analyses revealed biometric changes associated with phone usage. The dataset is publicly available for research through GitHub and Science Data Bank, with synchronized recordings from three platforms (edBB, edX, and LOGGE), provided in standard formats (.csv, .mp4, .wav, and .tsv), and accompanied by a detailed guide.",
    "score": 2,
    "reason": "该论文提出的是一个用于研究手机使用对远程在线教育影响的多模态数据集，虽然涉及多模态数据和行为分析，但其核心目标是教育心理学与人机交互研究，而非文档图像理解（DIU）。数据内容聚焦于学习者的行为、生理信号（如EEG、眼动）和键盘输入，与文档图像中的文本识别、实体抽取、关系建模等任务无直接关联。尽管数据集开源且发表在Nature子刊上，具备高质量，但应用场景和任务目标与DIU领域无关，无法直接迁移应用。因此不值得阅读。",
    "summary": "IMPROVE数据集收集了120名学习者在30分钟在线学习任务中来自16个同步传感器（包括EEG、眼动追踪、视频、智能手表、击键动态等）的多模态数据，旨在分析移动设备使用对学习表现的影响。数据涵盖行为、生物特征、生理和学业成绩，通过半监督重标注确保标签质量，已公开发布于GitHub和Science Data Bank，支持多种标准格式。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20955v1",
    "updated": "2025-08-28T16:17:19Z",
    "published": "2025-08-28T16:17:19Z",
    "title": "E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections",
    "authors": [
      "Fang Wang",
      "Huitao Li",
      "Wenhan Chao",
      "Zheng Zhuo",
      "Yiran Ji",
      "Chang Peng",
      "Yupeng Sun"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20955v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Many high-performance networks were not designed with lightweight application scenarios in mind from the outset, which has greatly restricted their scope of application. This paper takes ConvNeXt as the research object and significantly reduces the parameter scale and network complexity of ConvNeXt by integrating the Cross Stage Partial Connections mechanism and a series of optimized designs. The new network is named E-ConvNeXt, which can maintain high accuracy performance under different complexity configurations. The three core innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network (CSPNet) with ConvNeXt and adjusting the network structure, which reduces the model's network complexity by up to 80%; (2) Optimizing the Stem and Block structures to enhance the model's feature expression capability and operational efficiency; (3) Replacing Layer Scale with channel attention. Experimental validation on ImageNet classification demonstrates E-ConvNeXt's superior accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at 0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer learning tests on object detection tasks further confirm its generalization capability.",
    "score": 3,
    "reason": "该论文提出E-ConvNeXt，一种轻量级卷积网络变体，通过跨阶段部分连接等技术降低模型复杂度。虽然在效率和精度平衡上表现良好，但其核心贡献集中在通用视觉任务（如ImageNet分类、目标检测）的网络结构优化，与文档图像理解（DIU）、多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）等关键方向无直接关联。未提及OCR、实体识别、关系抽取、布局建模或视觉语言对齐等DIU核心组件，也未涉及LLM推理技术或agent架构。因此，虽具工程价值，但无法直接应用于DIU领域，相关性较低。",
    "summary": "E-ConvNeXt是一种基于ConvNeXt的轻量级网络变体，通过引入跨阶段部分连接机制、优化Stem和Block结构，并用通道注意力替代Layer Scale，显著降低模型复杂度（最高达80%），同时保持高精度。在ImageNet分类和目标检测任务上表现出色，尤其在低计算量下具有优异的准确率-效率权衡。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20954v1",
    "updated": "2025-08-28T16:16:40Z",
    "published": "2025-08-28T16:16:40Z",
    "title": "Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement",
    "authors": [
      "Amir Jmal",
      "Chaima Chtourou",
      "Mahdi Louati",
      "Abdelaziz Kallel",
      "Houda Khmila"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20954v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In the context of proven climate change, maintaining olive biodiversity through early anomaly detection and treatment using remote sensing technology is crucial, offering effective management solutions. This paper presents an innovative approach to olive tree segmentation from satellite images. By leveraging foundational models and advanced segmentation techniques, the study integrates the Segment Anything Model (SAM) to accurately identify and segment olive trees in agricultural plots. The methodology includes SAM segmentation and corrections based on trees alignement in the field and a learanble constraint about the shape and the size. Our approach achieved a 98\\% accuracy rate, significantly surpassing the initial SAM performance of 82\\%.",
    "score": 2,
    "reason": "该论文聚焦于卫星图像中的橄榄树分割，属于遥感图像处理领域，与文档图像理解（DIU）无直接关联。尽管使用了SAM模型，但应用场景（农业遥感）和任务目标（树体分割）与DIU的文本识别、实体抽取、关系建模等核心问题不匹配，无法直接迁移应用。此外，未提及多模态模型、推理增强或agent相关技术，也未涉及文档布局、表格或视觉信息抽取等DIU关键要素。",
    "summary": "本文提出一种基于SAM和多阶段优化的橄榄树卫星图像分割方法，利用田间树木排列规律和可学习的形状尺寸约束提升分割精度，最终达到98%准确率，显著优于原始SAM的82%。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20920v1",
    "updated": "2025-08-28T15:50:29Z",
    "published": "2025-08-28T15:50:29Z",
    "title": "COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans",
    "authors": [
      "Enrico Martini",
      "Ho Jin Choi",
      "Nadia Figueroa",
      "Nicola Bombieri"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20920v1.pdf",
    "comment": "Submitted to Information Fusion",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH.",
    "score": 2,
    "reason": "该论文聚焦于多视角人体姿态估计与跟踪，属于计算机视觉中的动作识别与三维重建方向，与文档图像理解（DIU）无直接关联。尽管其使用凸优化和分布式处理的技术可能在某些计算架构上具有启发性，但其核心任务、数据类型（人体运动）与DIU领域完全不重合，无法直接迁移应用。此外，未涉及OCR、实体识别、关系抽取或VLM/LLM推理增强等关键DIU技术，也不属于agent或inference scaling相关研究。因此不值得阅读。",
    "summary": "COMETH提出一种轻量级多视角人体姿态融合算法，利用生物力学约束和凸优化实现空间一致性，并通过状态观测器提升时间一致性，在工业场景中实现高精度实时人体运动追踪。代码已开源，适用于边缘设备部署。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20919v1",
    "updated": "2025-08-28T15:50:27Z",
    "published": "2025-08-28T15:50:27Z",
    "title": "Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement",
    "authors": [
      "Sara Krauss",
      "Ellena Spieß",
      "Daniel Hieber",
      "Frank Kramer",
      "Johannes Schobel",
      "Dominik Müller"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20919v1.pdf",
    "comment": "Submission as part of the MICCAI MIDOG25 challenge",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Mitotic figures (MFs) are relevant biomarkers in tumor grading. Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult, as manual annotation is time-consuming and subjective. In this work an ensemble of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it reduced sensitivity and overall performance. The results show that deep ensembles perform well for AMF classification. RBR can increase specific metrics but requires further research.",
    "score": 2,
    "reason": "该论文聚焦于医学图像中有丝分裂图的分类，属于计算机视觉在病理学中的应用，与文档图像理解（DIU）无直接关联。尽管使用了深度集成学习和规则后处理，但其任务目标（肿瘤分级）和数据类型（病理切片图像）均不适用于DIU场景。此外，未涉及OCR、实体识别、关系抽取或多模态模型等DIU核心要素，也未触及VLM、LLM推理增强或agent系统相关内容，因此不值得阅读。",
    "summary": "本研究针对MICCAI MIDOG25挑战赛中的有丝分裂图分类任务，采用ConvNeXtBase深度神经网络的集成模型，并引入基于规则的精炼模块（RBR）以提升分类性能。在初步测试集上达到84.02%的平衡准确率，RBR虽提高特异性但降低了敏感性和整体表现，表明该方法在AMF识别中有效但需进一步优化。"
  },
  {
    "id": "http://arxiv.org/abs/2508.11803v3",
    "updated": "2025-08-28T15:47:29Z",
    "published": "2025-08-15T21:18:23Z",
    "title": "An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation",
    "authors": [
      "Azam Nouri"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11803v3.pdf",
    "comment": "5 pages, No figure",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This study investigates whether second-order geometric cues - planar curvature magnitude, curvature sign, and gradient orientation - are sufficient on their own to drive a multilayer perceptron (MLP) classifier for handwritten character recognition (HCR), offering an alternative to convolutional neural networks (CNNs). Using these three handcrafted feature maps as inputs, our curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89 percent on EMNIST letters. These results underscore the discriminative power of curvature-based representations for handwritten character images and demonstrate that the advantages of deep learning can be realized even with interpretable, hand-engineered features.",
    "score": 3,
    "reason": "该论文聚焦于手写字符识别，使用手工设计的几何特征（曲率和梯度方向）输入MLP进行分类，虽在MNIST/EMNIST上表现良好，但属于传统图像识别范畴，与文档图像理解（DIU）的核心任务如布局分析、实体识别、关系抽取等无直接关联。其方法未涉及多模态建模、推理增强或agent系统，且为纯视觉分类任务，无法直接迁移至DIU场景。尽管使用了可解释特征，但缺乏对文档结构、语义理解或上下文关系的建模，不满足DIU领域关键需求。",
    "summary": "本文提出一种基于平面曲率和梯度方向的MLP基线模型用于手写字符识别，利用手工设计的二阶几何特征作为输入，在MNIST和EMNIST数据集上分别达到97%和89%的准确率，验证了几何特征在手写体识别中的有效性，但未涉及文档级语义理解或复杂结构分析。"
  },
  {
    "id": "http://arxiv.org/abs/2508.11902v3",
    "updated": "2025-08-28T15:44:00Z",
    "published": "2025-08-16T04:17:39Z",
    "title": "A Sobel-Gradient MLP Baseline for Handwritten Character Recognition",
    "authors": [
      "Azam Nouri"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11902v3.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We revisit the classical Sobel operator to ask a simple question: Are first-order edge maps sufficient to drive an all-dense multilayer perceptron (MLP) for handwritten character recognition (HCR), as an alternative to convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory footprint and transparent features. Our findings highlight that much of the class-discriminative information in handwritten character images is already captured by first-order gradients, making edge-aware MLPs a compelling option for HCR.",
    "score": 3,
    "reason": "该论文研究的是手写字符识别中使用Sobel梯度作为输入的MLP基线方法，虽然在MNIST和EMNIST上表现良好，但属于传统CV任务中的特定技术改进。其与文档图像理解（DIU）的核心任务（如复杂文档布局理解、实体关系抽取、多模态推理）关联较弱；且未涉及OCR-free框架、多模态模型、inference scaling或agent系统等前沿方向。尽管方法简洁，但缺乏对DIU场景的直接应用潜力，也未触及VLM/LLM推理增强或智能体架构，因此相关性较低。",
    "summary": "本文提出一种仅使用Sobel梯度作为输入的全连接MLP用于手写字符识别，在MNIST和EMNIST数据集上分别达到98%和92%准确率，表明一阶边缘信息已包含足够判别性特征。该方法简单高效，内存占用低，但未扩展至复杂文档图像理解任务，与当前DIU前沿技术路径关联甚微。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20909v1",
    "updated": "2025-08-28T15:38:50Z",
    "published": "2025-08-28T15:38:50Z",
    "title": "Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation",
    "authors": [
      "Yifan Gao",
      "Haoyue Li",
      "Feng Yuan",
      "Xiaosong Wang",
      "Xin Gao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20909v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Foundation models pre-trained on large-scale natural image datasets offer a powerful paradigm for medical image segmentation. However, effectively transferring their learned representations for precise clinical applications remains a challenge. In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. Our architecture introduces an encoder built upon a frozen DINOv3 backbone, which employs a specialized adapter to fuse the model's rich semantic features with low-level spatial details. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. We conducted extensive experiments on seven diverse public medical image segmentation datasets. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Our framework proves to be highly scalable, with segmentation accuracy consistently improving as the backbone model size increases up to the 7-billion-parameter variant. The findings demonstrate that leveraging the superior, dense-pretrained features from a general-purpose foundation model provides a highly effective and parameter-efficient approach to advance the accuracy of medical image segmentation. The code is available at https://github.com/yifangao112/DinoUNet.",
    "score": 2,
    "reason": "该论文聚焦于医学图像分割，使用DINOv3作为骨干网络构建U-Net架构，虽涉及视觉基础模型的特征利用，但应用场景为医学影像，与文档图像理解（DIU）无直接关联。其方法未涉及OCR、实体识别、关系抽取或布局分析等DIU核心任务，且未使用多模态模型处理文本-视觉联合表示。尽管代码开源且基于前沿视觉模型，但技术路径与DIU领域无关，无法直接迁移应用。",
    "summary": "本文提出Dino U-Net，一种基于冻结DINOv3骨干网络的编码器-解码器结构，通过专用适配器融合语义特征与空间细节，并引入保真度感知投影模块（FAPM）提升特征质量，用于医学图像分割。在七个公开数据集上实现SOTA性能，证明了通用视觉基础模型在医疗场景中的有效性。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20892v1",
    "updated": "2025-08-28T15:20:35Z",
    "published": "2025-08-28T15:20:35Z",
    "title": "To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software",
    "authors": [
      "Loïc Stratil",
      "Felix Fent",
      "Esteban Rivera",
      "Markus Lienkamp"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20892v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Autonomous vehicle perception typically relies on modular pipelines that decompose the task into detection, tracking, and prediction. While interpretable, these pipelines suffer from error accumulation and limited inter-task synergy. Unified perception has emerged as a promising paradigm that integrates these sub-tasks within a shared architecture, potentially improving robustness, contextual reasoning, and efficiency while retaining interpretable outputs. In this survey, we provide a comprehensive overview of unified perception, introducing a holistic and systemic taxonomy that categorizes methods along task integration, tracking formulation, and representation flow. We define three paradigms -Early, Late, and Full Unified Perception- and systematically review existing methods, their architectures, training strategies, datasets used, and open-source availability, while highlighting future research directions. This work establishes the first comprehensive framework for understanding and advancing unified perception, consolidates fragmented efforts, and guides future research toward more robust, generalizable, and interpretable perception.",
    "score": 2,
    "reason": "该论文聚焦于自动驾驶领域的统一感知（unified perception），虽然其核心思想——整合多任务、减少误差累积、提升上下文推理能力——与DIU领域中对多模态融合与端到端建模的追求有一定启发性，但研究场景为自动驾驶，任务目标（如检测、跟踪、预测）与文档图像理解在数据模态、任务结构和应用需求上存在本质差异。文中提出的Early/Late/Full统一范式虽具理论价值，但缺乏可直接迁移至DIU的实证或技术方法。此外，未提及任何与OCR、实体识别、关系抽取或视觉-语言模型相关的技术，也无开源信息或顶会发表记录，因此不具直接应用价值。",
    "summary": "本文综述了自动驾驶领域的统一感知范式，提出一个系统性的分类框架，将感知任务整合为共享架构以提升鲁棒性和效率。通过分析不同集成策略与训练方法，归纳了Early、Late和Full三类统一感知范式，并梳理了相关数据集与开源实现。尽管为该领域提供了重要理论框架，但其应用场景与技术路径与文档图像理解（DIU）无直接关联，无法直接应用于DIU任务。"
  },
  {
    "id": "http://arxiv.org/abs/2501.10977v3",
    "updated": "2025-08-28T15:13:50Z",
    "published": "2025-01-19T07:53:39Z",
    "title": "SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-Learning in Virtual Reality",
    "authors": [
      "Roberto Daza",
      "Lin Shengkai",
      "Aythami Morales",
      "Julian Fierrez",
      "Katashi Nagao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2501.10977v3.pdf",
    "comment": "10 pages, 3 figures. Published in ACM Intl. Conf. on Multimedia Workshops (ACM MM Workshops 2025, I2M-MM 25). Also presented at IEEE/CVF Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW) and AAAI Workshop on Artificial Intelligence for Education (AI4EDU)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This work introduces SMARTe-VR, a platform for student monitoring in an immersive virtual reality environment designed for online education. SMARTe-VR aims to collect data for adaptive learning, focusing on facial biometrics and learning metadata. The platform allows instructors to create customized learning sessions with video lectures, featuring an interface with an AutoQA system to evaluate understanding, interaction tools (for example, textbook highlighting and lecture tagging), and real-time feedback. Furthermore, we released a dataset that contains 5 research challenges with data from 10 users in VR-based TOEIC sessions. This data set, which spans more than 25 hours, includes facial features, learning metadata, 450 responses, difficulty levels of the questions, concept tags, and understanding labels. Alongside the database, we present preliminary experiments using Item Response Theory models, adapted for understanding detection using facial features. Two architectures were explored: a Temporal Convolutional Network for local features and a Multilayer Perceptron for global features.",
    "score": 2,
    "reason": "该论文聚焦于VR环境中学生学习行为的监控与自适应反馈，核心内容为基于面部生物特征和学习元数据的分析，属于教育技术与人机交互领域。虽涉及视觉数据处理，但其应用场景（VR在线教育）与文档图像理解（DIU）无直接关联，且未涉及OCR、实体识别、关系抽取等DIU核心任务。尽管开源了数据集并用于Item Response Theory建模，但模型架构与DIU无关。此外，未使用多模态模型或推理增强技术，也未体现agent设计思想。因此，与DIU、VLM推理增强、LLM推理扩展及agent研究均无直接可迁移性。",
    "summary": "SMARTe-VR是一个面向虚拟现实在线教育的学生监控平台，通过面部生物特征和学习元数据实现自适应学习反馈。研究构建了一个包含10名用户超过25小时VR TOEIC学习数据的数据集，涵盖面部特征、问题回答、难度标签和理解标签，并采用TCN和MLP进行理解检测的初步实验。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20881v1",
    "updated": "2025-08-28T15:11:49Z",
    "published": "2025-08-28T15:11:49Z",
    "title": "Understanding and evaluating computer vision models through the lens of counterfactuals",
    "authors": [
      "Pushkar Shukla"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20881v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems. The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts. The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals. Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation.",
    "score": 3,
    "reason": "该论文聚焦于视觉模型的反事实推理，用于解释、审计和缓解偏差，虽涉及计算机视觉与可解释性，但核心是模型公平性与因果分析，未直接关联文档图像理解（DIU）、多模态模型（VLM）的端到端理解、推理增强（inference scaling）或智能体（agent）系统。其方法虽在理论上可间接启发DIU中的鲁棒性设计，但缺乏直接迁移路径，且未涉及OCR、实体识别、关系抽取等DIU关键任务。因此不具直接应用价值，仅具潜在启发意义。",
    "summary": "本文提出一系列基于反事实推理的框架，用于分析和缓解视觉分类器与文本生成图像模型中的偏见。CAVLI结合归因与概念级分析，量化模型对语义概念的依赖；ASAC通过对抗性反事实扰动提升公平性；TIBET和BiasConnect用于评估身份相关提示的偏见并构建因果图；InterMit则提供无训练的模块化偏差缓解方法。整体贡献在于建立可扩展的因果审计与公平性优化范式。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20877v1",
    "updated": "2025-08-28T15:07:04Z",
    "published": "2025-08-28T15:07:04Z",
    "title": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis",
    "authors": [
      "Dennis Slobodzian",
      "Karissa Tilbury",
      "Amir Kordijazi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20877v1.pdf",
    "comment": "21 pages, 17 figure",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms of cancer, with a five-year survival rate below 10% primarily due to late detection. This research develops and validates a deep learning framework for early PDAC detection through analysis of dual-modality imaging: autofluorescence and second harmonic generation (SHG). We analyzed 40 unique patient samples to create a specialized neural network capable of distinguishing between normal, fibrotic, and cancerous tissue. Our methodology evaluated six distinct deep learning architectures, comparing traditional Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs). Through systematic experimentation, we identified and overcome significant challenges in medical image analysis, including limited dataset size and class imbalance. The final optimized framework, based on a modified ResNet architecture with frozen pre-trained layers and class-weighted training, achieved over 90% accuracy in cancer detection. This represents a significant improvement over current manual analysis methods an demonstrates potential for clinical deployment. This work establishes a robust pipeline for automated PDAC detection that can augment pathologists' capabilities while providing a foundation for future expansion to other cancer types. The developed methodology also offers valuable insights for applying deep learning to limited-size medical imaging datasets, a common challenge in clinical applications.",
    "score": 2,
    "reason": "该论文聚焦于胰腺癌的早期检测，使用多模态医学影像（自体荧光与二次谐波生成）进行分类，属于医学图像分析领域。虽然涉及视觉深度学习框架和模型比较（CNN vs ViT），但其研究目标、数据类型（病理切片）、任务性质（组织分类）与文档图像理解（DIU）无直接关联。此外，论文未涉及OCR、实体识别、关系抽取、布局建模、表格处理等DIU核心要素，也未使用或借鉴VLM、LLM推理增强技术或agent架构。尽管采用深度学习方法，但应用场景和问题本质差异显著，无法直接迁移至DIU领域。因此不值得阅读。",
    "summary": "本研究提出一种基于双模态医学影像（自体荧光与SHG）的深度学习框架，用于早期胰腺导管腺癌（PDAC）检测。通过分析40个患者样本，对比六种神经网络架构，最终采用改进的ResNet并结合冻结预训练层与类别加权训练，实现超过90%的癌症检测准确率，为临床辅助诊断提供可行方案。"
  },
  {
    "id": "http://arxiv.org/abs/2503.11519v3",
    "updated": "2025-08-28T14:55:38Z",
    "published": "2025-03-14T15:42:42Z",
    "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models",
    "authors": [
      "Hao Cheng",
      "Erjia Xiao",
      "Yichi Wang",
      "Lingfeng Zhang",
      "Qiang Zhang",
      "Jiahang Cao",
      "Kaidi Xu",
      "Mengshu Sun",
      "Xiaoshuai Hao",
      "Jindong Gu",
      "Renjing Xu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.11519v3.pdf",
    "comment": "This paper is accepted by IJCAI2025 Workshop on Deepfake Detection, Localization, and Interpretability",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Current Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-Vision tasks, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have attracted significant attention. Large Vision Language Models (LVLMs) and I2I Generation Models (GMs) are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to produce disruptive outputs that are semantically aligned with those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of cross-vision tasks. However, the specific characteristics of the threats posed by visual prompts remain underexplored. In this paper, to comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we propose the Typographic Visual Prompts Injection Dataset and thoroughly evaluate the TVPI security risks on various open-source and closed-source LVLMs and I2I GMs under visual prompts with different target semantics, deepening the understanding of TVPI threats.",
    "score": 3,
    "reason": "该论文聚焦于视觉提示注入攻击（TVPI）的安全威胁，研究typographic visual prompts对跨模态生成模型的影响。虽然涉及视觉语言模型（VLM），但其核心是安全漏洞分析与对抗攻击，而非提升DIU任务性能或构建可应用的DIU系统。论文未涉及文档理解、实体识别、关系抽取等DIU关键环节，也未提出可用于DIU的新方法或技术。尽管在VLM领域有一定相关性，但其目标为发现攻击向量，而非改进理解能力，难以直接迁移至DIU任务。因此，不值得优先阅读。",
    "summary": "本文提出Typographic Visual Prompt Injection (TVPI)数据集，系统评估了在不同语义目标下，文字类视觉提示对大型视觉语言模型（LVLMs）和图像到图像生成模型（I2I GMs）的干扰效果，揭示了视觉提示在跨模态生成任务中的安全风险。研究重点在于攻击行为分析，而非模型能力增强或文档理解优化。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20851v1",
    "updated": "2025-08-28T14:46:24Z",
    "published": "2025-08-28T14:46:24Z",
    "title": "PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis",
    "authors": [
      "Ye Zhang",
      "Yu Zhou",
      "Jingwen Qi",
      "Yongbing Zhang",
      "Simon Puettmann",
      "Finn Wichmann",
      "Larissa Pereira Ferreira",
      "Lara Sichward",
      "Julius Keyl",
      "Sylvia Hartmann",
      "Shuo Zhao",
      "Hongxiao Wang",
      "Xiaowei Xu",
      "Jianxu Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20851v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Deep learning based automated pathological diagnosis has markedly improved diagnostic efficiency and reduced variability between observers, yet its clinical adoption remains limited by opaque model decisions and a lack of traceable rationale. To address this, recent multimodal visual reasoning architectures provide a unified framework that generates segmentation masks at the pixel level alongside semantically aligned textual explanations. By localizing lesion regions and producing expert style diagnostic narratives, these models deliver the transparent and interpretable insights necessary for dependable AI assisted pathology. Building on these advancements, we propose PathMR, a cell-level Multimodal visual Reasoning framework for Pathological image analysis. Given a pathological image and a textual query, PathMR generates expert-level diagnostic explanations while simultaneously predicting cell distribution patterns. To benchmark its performance, we evaluated our approach on the publicly available PathGen dataset as well as on our newly developed GADVR dataset. Extensive experiments on these two datasets demonstrate that PathMR consistently outperforms state-of-the-art visual reasoning methods in text generation quality, segmentation accuracy, and cross-modal alignment. These results highlight the potential of PathMR for improving interpretability in AI-driven pathological diagnosis. The code will be publicly available in https://github.com/zhangye-zoe/PathMR.",
    "score": 6,
    "reason": "该论文提出PathMR用于病理图像的多模态视觉推理，虽在多模态推理和可解释性方面有创新，但研究对象为医学病理图像，与文档图片理解（DIU）领域关联较弱。尽管其采用的多模态推理框架可能对DIU中的VLM应用有启发，但任务场景差异大（病理 vs 文档），且未涉及OCR、实体识别或关系抽取等DIU核心组件。虽然代码将开源，具备一定可复用性，但直接迁移至DIU的潜力有限。不满足‘极其紧密相关’的标准。",
    "summary": "PathMR是一种细胞级多模态视觉推理框架，用于病理图像分析，能够根据输入图像和文本查询生成专家级诊断说明并预测细胞分布模式。在PathGen和自建GADVR数据集上表现优于现有方法，强调了可解释性与跨模态对齐。代码将公开。"
  },
  {
    "id": "http://arxiv.org/abs/2503.12232v2",
    "updated": "2025-08-28T14:32:17Z",
    "published": "2025-03-15T18:56:29Z",
    "title": "L2RW+: A Comprehensive Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification",
    "authors": [
      "Yan Jiang",
      "Hao Yu",
      "Mengting Wei",
      "Zhaodong Sun",
      "Haoyu Chen",
      "Xu Cheng",
      "Guoying Zhao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.12232v2.pdf",
    "comment": "Extended Version of L2RW. We extend it from image to video data",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Visible-infrared person re-identification (VI-ReID) is a challenging task that aims to match pedestrian images captured under varying lighting conditions, which has drawn intensive research attention and achieved promising results. However, existing methods adopt the centralized training, ignoring the potential privacy concerns as the data is distributed across multiple devices or entities in reality. In this paper, we propose L2RW+, a benchmark that brings VI-ReID closer to real-world applications. The core rationale behind L2RW+ is that incorporating decentralized training into VI-ReID can address privacy concerns in scenarios with limited data-sharing constrains. Specifically, we design protocols and corresponding algorithms for different privacy sensitivity levels. In our new benchmark, we simulate the training under real-world data conditions that: 1) data from each camera is completely isolated, or 2) different data entities (e.g., data controllers of a certain region) can selectively share the data. In this way, we simulate scenarios with strict privacy restrictions, which is closer to real-world conditions. Comprehensive experiments show the feasibility and potential of decentralized VI-ReID training at both image and video levels. In particular, with increasing data scales, the performance gap between decentralized and centralized training decreases, especially in video-level VI-ReID. In unseen domains, decentralized training even achieves performance comparable to SOTA centralized methods. This work offers a novel research entry for deploying VI-ReID into real-world scenarios and can benefit the community. Code is available at: https://github.com/Joey623/L2RW.",
    "score": 2,
    "reason": "该论文聚焦于可见光-红外行人重识别（VI-ReID）中的隐私保护与去中心化训练问题，属于计算机视觉领域中特定的行人识别任务。尽管其在数据隐私和分布式学习方面有创新，但与文档图像理解（DIU）、多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）等核心方向无直接关联。论文主题、应用场景和技术路径均不适用于DIU任务，无法直接迁移应用。因此，不值得阅读。",
    "summary": "L2RW+ 提出一个面向隐私保护的可见光-红外行人重识别基准，支持图像和视频级别的去中心化训练，模拟真实世界中数据隔离或选择性共享的场景，以解决数据隐私问题。实验表明，在大规模数据下，去中心化训练性能接近集中式方法，尤其在视频级任务中表现优异。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20835v1",
    "updated": "2025-08-28T14:28:33Z",
    "published": "2025-08-28T14:28:33Z",
    "title": "PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification",
    "authors": [
      "Hao Yang",
      "Qianyu Zhou",
      "Haijia Sun",
      "Xiangtai Li",
      "Xuequan Lu",
      "Lizhuang Ma",
      "Shuicheng Yan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20835v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Domain Generalization (DG) has been recently explored to enhance the generalizability of Point Cloud Classification (PCC) models toward unseen domains. Prior works are based on convolutional networks, Transformer or Mamba architectures, either suffering from limited receptive fields or high computational cost, or insufficient long-range dependency modeling. RWKV, as an emerging architecture, possesses superior linear complexity, global receptive fields, and long-range dependency. In this paper, we present the first work that studies the generalizability of RWKV models in DG PCC. We find that directly applying RWKV to DG PCC encounters two significant challenges: RWKV's fixed direction token shift methods, like Q-Shift, introduce spatial distortions when applied to unstructured point clouds, weakening local geometric modeling and reducing robustness. In addition, the Bi-WKV attention in RWKV amplifies slight cross-domain differences in key distributions through exponential weighting, leading to attention shifts and degraded generalization. To this end, we propose PointDGRWKV, the first RWKV-based framework tailored for DG PCC. It introduces two key modules to enhance spatial modeling and cross-domain robustness, while maintaining RWKV's linear efficiency. In particular, we present Adaptive Geometric Token Shift to model local neighborhood structures to improve geometric context awareness. In addition, Cross-Domain key feature Distribution Alignment is designed to mitigate attention drift by aligning key feature distributions across domains. Extensive experiments on multiple benchmarks demonstrate that PointDGRWKV achieves state-of-the-art performance on DG PCC.",
    "score": 2,
    "reason": "该论文研究的是点云分类中的领域泛化问题，基于RWKV架构进行改进。虽然RWKV具有线性复杂度和长程依赖建模优势，但其应用场景为3D点云数据，与文档图像理解（DIU）在数据模态（点云 vs 图像文本）、任务目标（分类 vs 信息抽取）和结构特性（无序点集 vs 布局敏感文本）上均无直接关联。尽管涉及领域泛化与模型效率，但无法直接应用于DIU任务，且未涉及OCR、实体识别、关系抽取或多模态推理等核心DIU技术。因此不值得阅读。",
    "summary": "本文提出PointDGRWKV，一种用于点云分类领域泛化的RWKV架构改进方法。针对RWKV在处理无序点云时存在的空间畸变和跨域注意力漂移问题，引入自适应几何令牌偏移和跨域键特征分布对齐模块，提升局部几何建模能力和跨域鲁棒性，在多个基准上达到SOTA性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20830v1",
    "updated": "2025-08-28T14:25:32Z",
    "published": "2025-08-28T14:25:32Z",
    "title": "Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation",
    "authors": [
      "Krit Duangprom",
      "Tryphon Lambrou",
      "Binod Bhattarai"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20830v1.pdf",
    "comment": "Accepted to MICCAI 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This paper presents a novel pipeline for 2D keypoint estima- tion of surgical tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network (CNN) or Transformer-based approaches, which often suffer from overfitting in small-scale medical datasets, our method harnesses the generalization capabilities of pre-trained VLMs. We carefully design prompts to create an instruction-tuning dataset and use them to align visual features with semantic keypoint descriptions. Experimental results show that with only two epochs of fine tuning, the adapted VLM outperforms the baseline models, demonstrating the ef- fectiveness of LoRA in low-resource scenarios. This approach not only improves keypoint detection performance, but also paves the way for future work in 3D surgical hands and tools pose estimation.",
    "score": 6,
    "reason": "该论文使用VLM结合LoRA进行手术工具2D关键点估计，虽涉及VLM和低资源场景下的微调，但任务为医学图像中的关键点检测，与文档图像理解（DIU）在目标、数据模态和应用场景上差异较大。尽管VLM和LoRA技术有一定相关性，但其核心任务并非信息抽取或文档理解，且未提及可迁移至DIU的直接方法或架构设计。MICCAI 2025为顶会，略有加分，但应用关联性较弱。",
    "summary": "本文提出一种基于视觉语言模型（VLM）与低秩适配（LoRA）的手术工具2D关键点估计新方法。通过精心设计提示构建指令微调数据集，将视觉特征与语义关键点描述对齐，在仅两轮微调下即超越传统CNN/Transformer模型，尤其适用于小样本医疗数据场景。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19542v2",
    "updated": "2025-08-28T14:22:38Z",
    "published": "2025-08-27T03:29:35Z",
    "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning",
    "authors": [
      "Nannan Zhu",
      "Yonghao Dong",
      "Teng Wang",
      "Xueqian Li",
      "Shengjun Deng",
      "Yijia Wang",
      "Zheng Hong",
      "Tiantian Geng",
      "Guo Niu",
      "Hanyan Huang",
      "Xiongfei Yao",
      "Shuaiwei Jiao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19542v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation MLLMs. The data and evaluation code are available at https://github.com/Hokhim2/CVBench.",
    "score": 6,
    "reason": "该论文提出CVBench，一个用于评估多视频跨视频关系推理的基准，聚焦于跨视频对象关联、事件关联和复杂推理任务。虽然其核心目标是多视频理解，与DIU领域中的文档图像跨页面/跨文档信息整合有一定潜在关联性（如简历多页信息对齐、合同多条款跨段落关联），但整体研究场景仍以动态视频为主，与静态文档图像理解存在较大差异。此外，其模型评测主要针对GPT-4o等通用VLM，未直接涉及OCR或布局建模，也未探讨文档结构特性。尽管数据集开源且涵盖多领域，但可迁移性较弱，仅在‘跨模态信息融合’层面有间接启发，因此不推荐作为DIU核心研究方向的重点阅读。",
    "summary": "CVBench是首个系统评估多模态大模型在跨视频关系推理能力的基准，包含1000个问答对，覆盖跨视频对象关联、事件链链接及复杂推理三个层级，基于五个多样化视频数据集构建。实验表明当前主流MLLMs在因果推理上表现远低于人类水平，暴露出跨视频上下文保留与实体消歧能力不足的问题，为下一代MMLM架构设计提供诊断工具。代码与数据已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2503.08751v2",
    "updated": "2025-08-28T14:20:08Z",
    "published": "2025-03-11T13:50:22Z",
    "title": "Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning",
    "authors": [
      "Qi Wang",
      "Zhipeng Zhang",
      "Baao Xie",
      "Xin Jin",
      "Yunbo Wang",
      "Shiyu Wang",
      "Liaomo Zheng",
      "Xiaokang Yang",
      "Wenjun Zeng"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.08751v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, $\\textit{i.e.,}$ RL agents suffer from low sample efficiency in environments with variations. While various approaches have attempted to alleviate this issue by disentangled representation learning, these methods usually start learning from scratch without prior knowledge of the world. This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints. To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM). Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos. The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation. For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model. During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning. Experimental results validate the superiority of our approach on various benchmarks.",
    "score": 3,
    "reason": "该论文聚焦于强化学习中的世界模型构建，通过解耦表示从干扰视频中迁移语义知识。尽管其方法在视觉表征学习和跨域迁移方面有创新，但与文档图像理解（DIU）、多模态模型（VLM）、大语言模型推理增强（inference scaling）或智能体（agent）在DIU场景下的应用无直接关联。其核心任务为RL环境下的动作-状态建模，而非文档结构解析、实体识别或关系抽取等DIU核心问题。此外，未提及开源或顶会发表，相关性较低。",
    "summary": "本文提出Disentangled World Models（DisWM），一种基于离线预训练与在线微调的可解释模型化强化学习框架。通过在干扰视频上进行无动作视频预测预训练并引入解耦正则化，提取语义知识，并通过潜在空间蒸馏将知识迁移到在线世界模型中。在在线适应阶段，结合环境反馈（动作与奖励）进一步优化解耦表示。实验表明该方法在多个基准上优于现有方法，但其应用场景为强化学习而非文档理解。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20817v1",
    "updated": "2025-08-28T14:15:18Z",
    "published": "2025-08-28T14:15:18Z",
    "title": "FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning",
    "authors": [
      "He Li",
      "Xinyu Liu",
      "Weihang Kong",
      "Xingchen Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20817v1.pdf",
    "comment": "11 pages, 9 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Most visible and infrared image fusion (VIF) methods focus primarily on optimizing fused image quality. Recent studies have begun incorporating downstream tasks, such as semantic segmentation and object detection, to provide semantic guidance for VIF. However, semantic segmentation requires extensive annotations, while object detection, despite reducing annotation efforts compared with segmentation, faces challenges in highly crowded scenes due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd counting has gained increasing attention in recent years, no studies have integrated VIF and crowd counting into a unified framework. To address these challenges, we propose FusionCounting, a novel multi-task learning framework that integrates crowd counting into the VIF process. Crowd counting provides a direct quantitative measure of population density with minimal annotation, making it particularly suitable for dense scenes. Our framework leverages both input images and population density information in a mutually beneficial multi-task design. To accelerate convergence and balance tasks contributions, we introduce a dynamic loss function weighting strategy. Furthermore, we incorporate adversarial training to enhance the robustness of both VIF and crowd counting, improving the model's stability and resilience to adversarial attacks. Experimental results on public datasets demonstrate that FusionCounting not only enhances image fusion quality but also achieves superior crowd counting performance.",
    "score": 2,
    "reason": "该论文聚焦于可见光与红外图像融合（VIF）并引入人群计数作为多任务学习的引导信号，属于计算机视觉领域中的跨模态图像处理任务。尽管其使用了多任务学习和动态权重策略，但核心内容与文档图像理解（DIU）、视觉语言模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。其应用场景为安防、交通监控等非文档类场景，且未涉及文本识别、实体抽取、关系建模或可解释性推理等DIU关键环节。因此，不适用于DIU领域的技术迁移，仅在方法论上略有参考价值，但不足以推荐阅读。",
    "summary": "FusionCounting提出一种将人群计数融入可见光-红外图像融合的多任务学习框架，利用人群密度作为轻量级语义引导，提升融合图像质量与计数精度，并采用动态损失加权与对抗训练增强模型鲁棒性。实验表明该方法在公共数据集上优于现有方法，但应用场景为密集人群监控，与文档图像理解无关。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20813v1",
    "updated": "2025-08-28T14:13:26Z",
    "published": "2025-08-28T14:13:26Z",
    "title": "Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training",
    "authors": [
      "Tao Luo",
      "Han Wu",
      "Tong Yang",
      "Dinggang Shen",
      "Zhiming Cui"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20813v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate dental caries detection from panoramic X-rays plays a pivotal role in preventing lesion progression. However, current detection methods often yield suboptimal accuracy due to subtle contrast variations and diverse lesion morphology of dental caries. In this work, inspired by the clinical workflow where dentists systematically combine whole-image screening with detailed tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training network for accurate dental caries detection. Our DVCTNet starts with employing automated tooth detection to establish two complementary views: a global view from panoramic X-ray images and a local view from cropped tooth images. We then pretrain two vision foundation models separately on the two views. The global-view foundation model serves as the detection backbone, generating region proposals and global features, while the local-view model extracts detailed features from corresponding cropped tooth patches matched by the region proposals. To effectively integrate information from both views, we introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically fuses dual-view features, enhancing the detection pipeline by integrating the fused features back into the detection model for final caries detection. To rigorously evaluate our DVCTNet, we test it on a public dataset and further validate its performance on a newly curated, high-precision dental caries detection dataset, annotated using both intra-oral images and panoramic X-rays for double verification. Experimental results demonstrate DVCTNet's superior performance against existing state-of-the-art (SOTA) methods on both datasets, indicating the clinical applicability of our method. Our code and labeled dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.",
    "score": 3,
    "reason": "该论文聚焦于牙科龋齿检测，属于医学影像分析领域，尽管使用了双视图协同训练和视觉基础模型，但其任务与文档图像理解（DIU）无直接关联。所提出的GCV-Atten模块虽具创新性，但应用场景为口腔X光片分析，不适用于文本布局、实体识别或关系抽取等DIU核心任务。此外，未涉及OCR、多模态文档理解、推理增强或agent系统，无法直接迁移至DIU研究。因此相关性较低。",
    "summary": "本文提出DVCTNet，一种用于全景牙科X光片中龋齿检测的双视图协同训练网络。通过全局视图（全景图像）与局部视图（牙齿裁剪块）分别预训练视觉基础模型，并引入门控跨视图注意力模块融合特征，提升检测精度。在公开数据集和自建高精度数据集上验证，性能优于现有SOTA方法。代码与数据已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2410.01262v4",
    "updated": "2025-08-28T14:03:26Z",
    "published": "2024-10-02T06:16:06Z",
    "title": "Improving Fine-Grained Control via Aggregation of Multiple Diffusion Models",
    "authors": [
      "Conghan Yue",
      "Zhengwei Peng",
      "Shiyan Du",
      "Zhi Ji",
      "Chuangjian Cai",
      "Le Wan",
      "Dongyu Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2410.01262v4.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "While many diffusion models perform well when controlling particular aspects such as style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper introduces a novel training-free algorithm, independent of denoising network architectures, for fine-grained generation, called Aggregation of Multiple Diffusion Models (AMDM). The algorithm integrates features from multiple diffusion models into a specified model to activate particular features and enable fine-grained control. Experimental results demonstrate that AMDM significantly improves fine-grained control without training, validating its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional generation in diffusion models. Specifically, it allows us to fully utilize existing or develop new conditional diffusion models that control specific aspects, and then aggregate them using the AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs. Code is available at: https://github.com/Hammour-steak/AMDM.",
    "score": 2,
    "reason": "该论文聚焦于扩散模型的细粒度控制，提出通过聚合多个扩散模型来实现条件生成。虽然技术新颖且无需训练，但其核心目标是图像生成任务中的条件控制，与文档图像理解（DIU）无直接关联。其方法未涉及文本识别、实体抽取或关系建模等DIU关键环节，也未利用视觉-语言多模态能力或推理增强技术。尽管在计算机视觉领域有潜力，但无法直接迁移至DIU任务，尤其不涉及VLM、LLM推理增强或agent系统构建。因此，与指定关注方向无关。",
    "summary": "本文提出Aggregation of Multiple Diffusion Models (AMDM)，一种无需训练的算法，通过融合多个扩散模型的特征来实现细粒度图像生成控制。该方法不依赖特定网络结构，可有效提升风格、角色、交互等特定属性的控制能力，适用于图像生成场景。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2505.07817v2",
    "updated": "2025-08-28T13:58:02Z",
    "published": "2025-05-12T17:59:32Z",
    "title": "Pixel Motion as Universal Representation for Robot Control",
    "authors": [
      "Kanchana Ranasinghe",
      "Xiang Li",
      "E-Ro Nguyen",
      "Cristina Mata",
      "Jongwoo Park",
      "Michael S Ryoo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.07817v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo",
    "score": 3,
    "reason": "该论文聚焦于机器人控制中的视觉-语言-动作框架，使用像素运动作为中间表示，虽涉及视觉与语言的结合，但核心应用场景为机器人动作生成，与文档图像理解（DIU）无直接关联。其提出的像素运动表示和扩散模型虽具创新性，但不可直接迁移至文档结构解析、实体识别或关系抽取任务。此外，未提及多模态模型在文档场景的应用，也未涉及推理增强或智能体架构。因此，相关性较低。",
    "summary": "LangToMo提出一种双系统架构用于机器人控制：高阶系统2（基于图像扩散模型）从单帧图像生成文本引导的像素运动序列；低阶系统1通过运动到动作映射函数将这些序列转化为具体机器人动作。该方法利用弱监督方式提取像素运动表示，实现跨视频数据的通用训练，适用于稀疏高层策略与密集底层策略协同的控制任务。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20789v1",
    "updated": "2025-08-28T13:53:44Z",
    "published": "2025-08-28T13:53:44Z",
    "title": "Surfel-based 3D Registration with Equivariant SE(3) Features",
    "authors": [
      "Xueyang Kang",
      "Hang Zhao",
      "Kourosh Khoshelham",
      "Patrick Vandewalle"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20789v1.pdf",
    "comment": "5 pages, 4 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Point cloud registration is crucial for ensuring 3D alignment consistency of multiple local point clouds in 3D reconstruction for remote sensing or digital heritage. While various point cloud-based registration methods exist, both non-learning and learning-based, they ignore point orientations and point uncertainties, making the model susceptible to noisy input and aggressive rotations of the input point cloud like orthogonal transformation; thus, it necessitates extensive training point clouds with transformation augmentations. To address these issues, we propose a novel surfel-based pose learning regression approach. Our method can initialize surfels from Lidar point cloud using virtual perspective camera parameters, and learns explicit $\\mathbf{SE(3)}$ equivariant features, including both position and rotation through $\\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative transformation between source and target scans. The model comprises an equivariant convolutional encoder, a cross-attention mechanism for similarity computation, a fully-connected decoder, and a non-linear Huber loss. Experimental results on indoor and outdoor datasets demonstrate our model superiority and robust performance on real point-cloud scans compared to state-of-the-art methods.",
    "score": 2,
    "reason": "该论文研究的是基于Surfel的3D点云配准问题，属于三维重建与计算机视觉领域，核心方法为SE(3)等变卷积用于点云姿态估计。尽管其技术先进且在点云处理方面有创新，但与文档图像理解（DIU）、多模态模型（VLM）、大语言模型（LLM）推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。DIU关注的是文档图像中的文本、布局、语义信息抽取，而本文聚焦于激光雷达点云的几何配准，应用场景（遥感、数字遗产）和数据模态（3D点云）均不匹配。因此，无法直接应用于DIU领域，相关性极低。",
    "summary": "本文提出一种基于Surfel的3D点云配准方法，通过引入SE(3)等变卷积网络学习位置与旋转的联合特征，实现对源与目标扫描间的相对位姿预测。利用虚拟相机初始化Surfel并结合交叉注意力机制进行相似性计算，采用非线性Huber损失优化。在室内与室外数据集上验证了其对噪声和剧烈旋转的鲁棒性，优于现有方法。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20783v1",
    "updated": "2025-08-28T13:45:04Z",
    "published": "2025-08-28T13:45:04Z",
    "title": "Evaluating Compositional Generalisation in VLMs and Diffusion Models",
    "authors": [
      "Beth Pearson",
      "Bilal Boulbarss",
      "Michael Wray",
      "Martha Lewis"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20783v1.pdf",
    "comment": "11 pages including references, 6 figures. Accepted at IWCS 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "A fundamental aspect of the semantics of natural language is that novel meanings can be formed from the composition of previously known parts. Vision-language models (VLMs) have made significant progress in recent years, however, there is evidence that they are unable to perform this kind of composition. For example, given an image of a red cube and a blue cylinder, a VLM such as CLIP is likely to incorrectly label the image as a red cylinder or a blue cube, indicating it represents the image as a `bag-of-words' and fails to capture compositional semantics. Diffusion models have recently gained significant attention for their impressive generative abilities, and zero-shot classifiers based on diffusion models have been shown to perform competitively with CLIP in certain compositional tasks. In this work we explore whether the generative Diffusion Classifier has improved compositional generalisation abilities compared to discriminative models. We assess three models -- Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with attributes and relations in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Our results show that the Diffusion Classifier and ViLT perform well at concept binding tasks, but that all models struggle significantly with the relational GZSL task, underscoring the broader challenges VLMs face with relational reasoning. Analysis of CLIP embeddings suggests that the difficulty may stem from overly similar representations of relational concepts such as left and right. Code and dataset are available at: https://github.com/otmive/diffusion_classifier_clip",
    "score": 8,
    "reason": "该论文聚焦于视觉语言模型（VLMs）在组合泛化能力上的评估，特别是对象-属性-关系的绑定任务，这与DIU中对文档布局语义理解、字段间关系建模的核心挑战高度相关。研究对比了扩散分类器、CLIP和ViLT在零样本和广义零样本场景下的表现，揭示了当前VLMs在关系推理方面的显著瓶颈，这对构建能理解文档结构与语义关系的DIU系统具有重要启发。论文提出的方法可直接用于评估DIU中VLM的语义理解能力，且代码开源，增强了可复现性。虽未直接解决DIU任务，但其对关系推理缺陷的分析为改进DIU中VLM的性能提供了关键洞见。会议为IWCS 2025，虽非CCF A类，但主题高度契合。因此值得阅读。",
    "summary": "本文评估了视觉语言模型（VLMs）和扩散模型在组合泛化能力上的表现，重点考察它们在绑定对象、属性与关系方面的性能。通过在零样本学习（ZSL）和广义零样本学习（GZSL）设置下测试Diffusion Classifier、CLIP和ViLT，发现尽管扩散分类器和ViLT在概念绑定上表现较好，但所有模型在关系类GZSL任务上均表现不佳，表明当前VLMs在关系推理方面存在根本性挑战。作者分析认为，这可能源于CLIP嵌入中关系概念（如左/右）表示过于相似。代码与数据集已公开。"
  },
  {
    "id": "http://arxiv.org/abs/2308.09388v2",
    "updated": "2025-08-28T13:33:09Z",
    "published": "2023-08-18T08:40:38Z",
    "title": "Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey",
    "authors": [
      "Xin Li",
      "Yulin Ren",
      "Xin Jin",
      "Cuiling Lan",
      "Xingrui Wang",
      "Wenjun Zeng",
      "Xinchao Wang",
      "Zhibo Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2308.09388v2.pdf",
    "comment": "Accepted by IJCV",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question, \"whether diffusion model can boost image restoration\". To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods. Despite that, a comprehensive and enlightening survey on diffusion model-based image restoration remains scarce. In this paper, we are the first to present a comprehensive review of recent diffusion model-based methods on image restoration, encompassing the learning paradigm, conditional strategy, framework design, modeling strategy, and evaluation. Concretely, we first introduce the background of the diffusion model briefly and then present two prevalent workflows that exploit diffusion models in image restoration. Subsequently, we classify and emphasize the innovative designs using diffusion models for both IR and blind/real-world IR, intending to inspire future development. To evaluate existing methods thoroughly, we summarize the commonly-used dataset, implementation details, and evaluation metrics. Additionally, we present the objective comparison for open-sourced methods across three tasks, including image super-resolution, deblurring, and inpainting. Ultimately, informed by the limitations in existing works, we propose five potential and challenging directions for the future research of diffusion model-based IR, including sampling efficiency, model compression, distortion simulation and estimation, distortion invariant learning, and framework design.",
    "score": 2,
    "reason": "该论文聚焦于扩散模型在图像修复与增强中的应用，属于低层视觉任务，与文档图像理解（DIU）的核心目标（文本识别、实体抽取、关系建模）关联较弱。尽管图像质量提升可能间接辅助OCR，但本文未涉及文档结构、布局理解、文本语义解析或多模态推理等DIU关键环节。同时，论文未提及VLM、LLM推理增强技术或agent架构，也无直接迁移至DIU的潜力。虽被IJCV接收（加分项），但内容不满足‘可直接应用于DIU’的标准，故评分较低。",
    "summary": "本文是对基于扩散模型的图像修复与增强技术的全面综述，涵盖学习范式、条件策略、框架设计与评估方法，系统梳理了超分辨率、去模糊和图像修复等任务中的扩散模型应用，并提出未来研究方向如采样效率、模型压缩与失真不变学习。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20776v1",
    "updated": "2025-08-28T13:32:35Z",
    "published": "2025-08-28T13:32:35Z",
    "title": "Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML",
    "authors": [
      "Kuniko Paxton",
      "Koorosh Aslansefat",
      "Amila Akagić",
      "Dhavalkumar Thakker",
      "Yiannis Papadopoulos"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20776v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advancements in skin lesion classification models have significantly improved accuracy, with some models even surpassing dermatologists' diagnostic performance. However, in medical practice, distrust in AI models remains a challenge. Beyond high accuracy, trustworthy, explainable diagnoses are essential. Existing explainability methods have reliability issues, with LIME-based methods suffering from inconsistency, while CAM-based methods failing to consider all classes. To address these limitations, we propose Global Class Activation Probabilistic Map Evaluation, a method that analyses all classes' activation probability maps probabilistically and at a pixel level. By visualizing the diagnostic process in a unified manner, it helps reduce the risk of misdiagnosis. Furthermore, the application of SafeML enhances the detection of false diagnoses and issues warnings to doctors and patients as needed, improving diagnostic reliability and ultimately patient safety. We evaluated our method using the ISIC datasets with MobileNetV2 and Vision Transformers.",
    "score": 2,
    "reason": "该论文聚焦于皮肤病变分类中的可解释性与安全性，提出基于全局类激活概率图的评估方法和SafeML机制。虽然其在可解释性方面有创新，但研究领域为医学图像诊断（皮肤病变），与文档图像理解（DIU）无直接关联。尽管涉及视觉模型与解释性，但应用场景、任务目标与DIU差异显著，无法直接迁移至文档信息抽取、实体识别或关系抽取等核心环节。此外，未涉及VLM、LLM推理增强或agent技术，也未开源或发表于CCF A类会议。因此不相关。",
    "summary": "本文提出一种用于皮肤病变分类的全局类激活概率图评估方法（Global Class Activation Probabilistic Map Evaluation），通过概率化分析所有类别在像素级的激活情况，提升诊断过程的可解释性。结合SafeML框架，能检测并预警潜在误诊，增强医疗AI系统的可靠性。在ISIC数据集上使用MobileNetV2和Vision Transformer进行验证。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20773v1",
    "updated": "2025-08-28T13:29:21Z",
    "published": "2025-08-28T13:29:21Z",
    "title": "Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI",
    "authors": [
      "Christoforos N. Spartalis",
      "Theodoros Semertzidis",
      "Petros Daras",
      "Efstratios Gavves"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20773v1.pdf",
    "comment": "ICML 2025 workshop on Machine Unlearning for Generative AI",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.",
    "score": 2,
    "reason": "该论文研究生成式AI中的机器遗忘（Machine Unlearning），聚焦于扩散模型中对特定类别的信息删除，属于模型安全与隐私保护范畴。其核心方法基于信息论和扩散过程的早期步骤控制，与文档图像理解（DIU）、视觉语言模型（VLM）、推理增强（inference scaling）或智能体（agent）系统无直接关联。尽管ICML workshop具有一定学术影响力，但内容不涉及DIU任务中的OCR、实体识别、关系抽取或多模态理解，也无法直接迁移至DIU场景。因此，不值得阅读。",
    "summary": "SAFEMax提出一种基于信息论的扩散模型机器遗忘方法，通过最大化生成图像的熵来实现对特定类别的高效遗忘，利用扩散过程早期步骤中类信息的显著性实现遗忘与保留之间的平衡，相比现有方法在效率上有显著提升。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20765v1",
    "updated": "2025-08-28T13:19:49Z",
    "published": "2025-08-28T13:19:49Z",
    "title": "Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding",
    "authors": [
      "Gowreesh Mago",
      "Pascal Mettes",
      "Stevan Rudinac"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20765v1.pdf",
    "comment": "Under Review for IJCV",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The automatic understanding of video content is advancing rapidly. Empowered by deeper neural networks and large datasets, machines are increasingly capable of understanding what is concretely visible in video frames, whether it be objects, actions, events, or scenes. In comparison, humans retain a unique ability to also look beyond concrete entities and recognize abstract concepts like justice, freedom, and togetherness. Abstract concept recognition forms a crucial open challenge in video understanding, where reasoning on multiple semantic levels based on contextual information is key. In this paper, we argue that the recent advances in foundation models make for an ideal setting to address abstract concept understanding in videos. Automated understanding of high-level abstract concepts is imperative as it enables models to be more aligned with human reasoning and values. In this survey, we study different tasks and datasets used to understand abstract concepts in video content. We observe that, periodically and over a long period, researchers have attempted to solve these tasks, making the best use of the tools available at their disposal. We advocate that drawing on decades of community experience will help us shed light on this important open grand challenge and avoid ``re-inventing the wheel'' as we start revisiting it in the era of multi-modal foundation models.",
    "score": 2,
    "reason": "该论文聚焦于视频理解中的抽象概念识别，属于视频分析领域，与文档图像理解（DIU）无直接关联。尽管涉及多模态和高层语义推理，但其研究对象（视频内容、抽象概念如正义、自由）与文档图像的结构化信息抽取任务差异显著，且未提及可迁移至DIU的技术或数据集。虽提及基础模型（foundation models），但未针对视觉文档或布局理解进行讨论，无法直接应用于DIU场景。",
    "summary": "本文综述了视频理解中抽象概念识别的研究进展，探讨了基于上下文信息进行多层级语义推理的任务与数据集。作者认为大模型时代为解决这一挑战提供了契机，呼吁借鉴历史经验避免重复研究。研究主题与文档图像理解（DIU）无关，不具直接应用价值。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20762v1",
    "updated": "2025-08-28T13:17:35Z",
    "published": "2025-08-28T13:17:35Z",
    "title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer",
    "authors": [
      "Fachri Najm Noer Kartiman",
      " Rasim",
      "Yaya Wihardi",
      "Nurul Hasanah",
      "Oskar Natan",
      "Bambang Wahono",
      "Taufik Ibnu Salim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20762v1.pdf",
    "comment": "keywords-multitask learning, autonomous driving, end-to-end learning, skip connections, swin transformer, self-attention mechanism. 12 pages",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Focusing on the development of an end-to-end autonomous vehicle model with pixel-to-pixel context awareness, this research proposes the SKGE-Swin architecture. This architecture utilizes the Swin Transformer with a skip-stage mechanism to broaden feature representation globally and at various network levels. This approach enables the model to extract information from distant pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head Self-Attention (SW-MSA) mechanism and to retain critical information from the initial to the final stages of feature extraction, thereby enhancing its capability to comprehend complex patterns in the vehicle's surroundings. The model is evaluated on the CARLA platform using adversarial scenarios to simulate real-world conditions. Experimental results demonstrate that the SKGE-Swin architecture achieves a superior Driving Score compared to previous methods. Furthermore, an ablation study will be conducted to evaluate the contribution of each architectural component, including the influence of skip connections and the use of the Swin Transformer, in improving model performance.",
    "score": 2,
    "reason": "该论文聚焦于自动驾驶中的端到端路径预测与导航，使用Swin Transformer结合跳过阶段机制提升特征表示能力。虽然涉及视觉Transformer和自注意力机制，但其应用场景为自动驾驶，与文档图像理解（DIU）、视觉多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）在DIU中的应用无直接关联。论文未提及任何与文档理解、信息抽取、实体识别、关系抽取或基于LLM/VLM的DIU系统相关的内容，且任务目标与DIU完全不重合，无法直接迁移应用。因此不值得阅读。",
    "summary": "本文提出SKGE-Swin架构，用于自动驾驶中的端到端车辆路径预测与导航。通过引入跳过阶段的Swin Transformer结构，利用Shifted Window Multi-head Self-Attention机制增强远距离像素上下文感知能力，并在CARLA平台上验证了其在对抗性场景下的优越驾驶得分。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20760v1",
    "updated": "2025-08-28T13:16:55Z",
    "published": "2025-08-28T13:16:55Z",
    "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
    "authors": [
      "Jan Erik van Woerden",
      "Gertjan Burghouts",
      "Lotte Nijskens",
      "Alma M. Liezenga",
      "Sabina van Rooij",
      "Frank Ruis",
      "Hugo J. Kuijf"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20760v1.pdf",
    "comment": "To be presented at SPIE: Sensors + Imaging, Artificial Intelligence for Security and Defence Applications II",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Vision-language models (VLMs) like CLIP enable zero-shot classification by aligning images and text in a shared embedding space, offering advantages for defense applications with scarce labeled data. However, CLIP's robustness in challenging military environments, with partial occlusion and degraded signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP variants' robustness to occlusion using a custom dataset of 18 military vehicle classes and evaluate using Normalized Area Under the Curve (NAUC) across occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP models consistently outperform CNNs, (2) fine-grained, dispersed occlusions degrade performance more than larger contiguous occlusions, (3) despite improved accuracy, performance of linear-probed models sharply drops at around 35% occlusion, (4) by finetuning the model's backbone, this performance drop occurs at more than 60% occlusion. These results underscore the importance of occlusion-specific augmentations during training and the need for further exploration into patch-level sensitivity and architectural resilience for real-world deployment of CLIP.",
    "score": 4,
    "reason": "该论文研究CLIP在军事车辆分类中对遮挡的鲁棒性，虽涉及视觉语言模型（VLM）和实际应用场景，但其核心关注点是特定领域（军事）的图像分类任务，与文档图像理解（DIU）无直接关联。尽管提到了VLM和遮挡问题，但未涉及文档结构、文本识别、实体关系抽取等DIU核心环节，也未探讨inference scaling或agent技术。此外，会议为SPIE传感器与成像系列，非CCF A类顶会，且未提及开源。因此，相关性较低，仅在VLM鲁棒性方面有间接参考价值。",
    "summary": "本文评估CLIP模型在不同遮挡程度下对18类军事车辆的分类性能，发现Transformer架构优于CNN，分散遮挡比连续遮挡更具破坏性，并指出线性探针模型在35%遮挡时性能骤降，而微调主干网络可将此阈值提升至60%以上，强调了训练阶段引入遮挡增强的重要性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20758v1",
    "updated": "2025-08-28T13:15:37Z",
    "published": "2025-08-28T13:15:37Z",
    "title": "SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding",
    "authors": [
      "Jiawen Lin",
      "Shiran Bian",
      "Yihang Zhu",
      "Wenbin Tan",
      "Yachao Zhang",
      "Yuan Xie",
      "Yanyun Qu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20758v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using natural language descriptions. Although supervised methods achieve higher accuracy in constrained settings, zero-shot 3DVG holds greater promise for real-world applications since eliminating scene-specific training requirements. However, existing zero-shot methods face challenges of spatial-limited reasoning due to reliance on single-view localization, and contextual omissions or detail degradation. To address these issues, we propose SeqVLM, a novel zero-shot 3DVG framework that leverages multi-view real-world scene images with spatial information for target object reasoning. Specifically, SeqVLM first generates 3D instance proposals via a 3D semantic segmentation network and refines them through semantic filtering, retaining only semantic-relevant candidates. A proposal-guided multi-view projection strategy then projects these candidate proposals onto real scene image sequences, preserving spatial relationships and contextual details in the conversion process of 3D point cloud to images. Furthermore, to mitigate VLM computational overload, we implement a dynamic scheduling mechanism that iteratively processes sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to identify textually specified objects. Experiments on the ScanRefer and Nr3D benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%, respectively, which advance 3DVG toward greater generalization and real-world applicability. The code is available at https://github.com/JiawLin/SeqVLM.",
    "score": 6,
    "reason": "该论文聚焦于3D视觉定位（3DVG），虽使用了多视图序列与VLM进行推理，且涉及proposal-guided和动态调度机制，具备一定的跨模态推理思想，但其核心任务是3D场景中的目标定位，与文档图像理解（DIU）在任务目标、数据结构（点云 vs. 文档图像）、语义层级（物理空间定位 vs. 信息结构解析）上差异显著。尽管其多视图推理和动态调度策略可能对DIU中布局分析有启发意义，但直接迁移应用价值有限。未明确提及文档或表格处理，也未开源代码以外的DIU相关设计。因此仅部分相关，打分适中。",
    "summary": "SeqVLM提出一种基于多视图序列的零样本3D视觉定位框架，通过生成3D实例提案并利用多视角投影保留空间关系，结合动态调度机制降低VLM计算负担，实现对自然语言描述对象的精准定位，在ScanRefer和Nr3D数据集上达到SOTA性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20754v1",
    "updated": "2025-08-28T13:12:18Z",
    "published": "2025-08-28T13:12:18Z",
    "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting",
    "authors": [
      "Yuxi Hu",
      "Jun Zhang",
      "Kuangyi Chen",
      "Zhe Zhang",
      "Friedrich Fraundorfer"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20754v1.pdf",
    "comment": "Accepted to The 36th British Machine Vision Conference (BMVC 2025), Sheffield, UK",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.",
    "score": 2,
    "reason": "该论文聚焦于通用高斯泼溅（Gaussian Splatting）中的特征学习，属于三维视觉重建与新视角合成领域，与文档图像理解（DIU）、多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。其技术路径为基于图像的3D重建，不涉及文档结构解析、实体识别、关系抽取或跨模态推理任务，无法直接迁移至DIU场景。尽管被BMVC接收且开源，但相关性极低，故评分较低。",
    "summary": "本文提出C³-GS框架，通过引入上下文感知、跨维度和跨尺度约束，在无需场景级优化的情况下提升高斯泼溅的泛化能力。通过三个轻量模块增强特征融合，实现高质量的新视角合成。在多个基准数据集上表现优异，代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20751v1",
    "updated": "2025-08-28T13:11:24Z",
    "published": "2025-08-28T13:11:24Z",
    "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning",
    "authors": [
      "Yibin Wang",
      "Zhimin Li",
      "Yuhang Zang",
      "Yujie Zhou",
      "Jiazi Bu",
      "Chunyu Wang",
      "Qinglin Lu",
      "Cheng Jin",
      "Jiaqi Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20751v1.pdf",
    "comment": "Project Page: https://codegoat24.github.io/UnifiedReward/Pref-GRPO",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.",
    "score": 3,
    "reason": "该论文聚焦于文本到图像生成中的强化学习优化，提出基于成对偏好奖励的GRPO方法以缓解奖励劫持问题。虽然其提出的Pref-GRPO和UniGenBench在T2I领域具有创新性，但核心任务与文档图像理解（DIU）无直接关联。尽管引入了多模态大模型（MLLM）用于基准构建，但其应用场景为图像生成而非文档信息提取或理解。此外，论文未涉及OCR、实体识别、关系抽取等DIU关键环节，也未探索VLM在DIU中的端到端应用，更无agent设计或推理增强技术（如CoT、MCTS）的体现。因此，虽属多模态范畴，但技术路径与DIU目标不直接对齐，难以直接迁移应用。",
    "summary": "本文提出Pref-GRPO，一种基于成对偏好奖励的GRPO方法，用于稳定文本到图像生成过程，解决点状奖励模型易受微小分数差异影响导致的奖励劫持问题。通过在组内进行图像成对比较并使用胜率作为奖励信号，提升训练稳定性。同时构建了UniGenBench，一个包含600个提示的统一T2I评估基准，涵盖5大主题和20个子主题，通过MLLM进行多维度评估。实验表明该方法能更好区分图像质量细微差异，并揭示开源与闭源模型的优缺点。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20745v1",
    "updated": "2025-08-28T13:04:55Z",
    "published": "2025-08-28T13:04:55Z",
    "title": "Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification",
    "authors": [
      "Kaustubh Atey",
      "Sameer Anand Jha",
      "Gouranga Bala",
      "Amit Sethi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20745v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Atypical mitotic figures (AMFs) are important histopathological markers yet remain challenging to identify consistently, particularly under domain shift stemming from scanner, stain, and acquisition differences. We present a simple training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2. The approach (i) increases feature diversity via style perturbations inserted at early and mid backbone stages, (ii) aligns attention-refined features across sites using weak domain labels (Scanner, Origin, Species, Tumor) through an auxiliary alignment loss, and (iii) stabilizes predictions by distilling from an exponential moving average (EMA) teacher with temperature-scaled KL divergence. On the organizer-run preliminary leaderboard for atypical mitosis classification, our submission attains balanced accuracy of 0.8762, sensitivity of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs negligible inference-time overhead, relies only on coarse domain metadata, and delivers strong, balanced performance, positioning it as a competitive submission for the MIDOG 2025 challenge.",
    "score": 2,
    "reason": "该论文聚焦于医学图像中的异常有丝分裂分类，属于计算机视觉在病理学领域的应用。虽然涉及跨域鲁棒性（domain robustness）和特征对齐等技术，但其任务与文档图像理解（DIU）无直接关联，应用场景、数据模态（组织切片图像）和目标（病理诊断）均不匹配。此外，未涉及OCR、实体识别、关系抽取或多模态模型在文档理解中的应用，也未触及推理增强（inference scaling）、agent架构或VLM/LLM相关技术。因此，与DIU领域核心研究方向无直接迁移价值。",
    "summary": "本文提出一种用于跨域鲁棒性分类异常有丝分裂的训练策略，通过风格扰动增加特征多样性、利用弱标签进行注意力特征对齐，并通过EMA教师模型蒸馏稳定预测。在MIDOG 2025挑战赛中取得了优异性能，但研究内容局限于医学图像分析，与文档图像理解（DIU）无关。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20734v1",
    "updated": "2025-08-28T12:58:14Z",
    "published": "2025-08-28T12:58:14Z",
    "title": "CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network",
    "authors": [
      "Reza Akbari Movahed",
      "Abuzar Rezaee",
      "Arezoo Zakeri",
      "Colin Berry",
      "Edmond S. L. Ho",
      "Ali Gooya"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20734v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.",
    "score": 2,
    "reason": "该论文聚焦于心脏运动预测，使用贝叶斯递归神经网络进行心脏形态引导的形变配准，属于医学图像分析领域。虽然涉及图像序列建模和不确定性估计，但与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）技术无直接关联。其方法和技术栈无法直接迁移至DIU任务，因此不值得阅读。",
    "summary": "CardioMorphNet提出一种基于形状引导的贝叶斯递归深度学习框架，用于从短轴心脏磁共振图像中进行3D心脏形变配准，通过递归变分自编码器建模心动周期的时空依赖性，并利用分割图的递归配准来优化运动场估计，避免了依赖强度相似性损失，提升了心脏区域的运动预测精度与置信度评估能力。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20709v1",
    "updated": "2025-08-28T12:27:23Z",
    "published": "2025-08-28T12:27:23Z",
    "title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network",
    "authors": [
      "Chenhao Zhang",
      "Wei Gao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20709v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Neural Video Compression (NVC) has achieved remarkable performance in recent years. However, precise rate control remains a challenge due to the inherent limitations of learning-based codecs. To solve this issue, we propose a dynamic video compression framework designed for variable bitrate scenarios. First, to achieve variable bitrate implementation, we propose the Dynamic-Route Autoencoder with variable coding routes, each occupying partial computational complexity of the whole network and navigating to a distinct RD trade-off. Second, to approach the target bitrate, the Rate Control Agent estimates the bitrate of each route and adjusts the coding route of DRA at run time. To encompass a broad spectrum of variable bitrates while preserving overall RD performance, we employ the Joint-Routes Optimization strategy, achieving collaborative training of various routes. Extensive experiments on the HEVC and UVG datasets show that the proposed method achieves an average BD-Rate reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods while maintaining an average bitrate error of 1.66%, achieving Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and bitrate-constrained applications. Our code is available at https://git.openi.org.cn/OpenAICoding/DynamicDVC.",
    "score": 2,
    "reason": "该论文聚焦于神经视频压缩中的动态码率控制，核心贡献在于构建可变比特率的动态路由自编码器与码率控制代理。虽然涉及动态决策和运行时调整，但其应用场景为视频压缩，与文档图像理解（DIU）在任务目标、输入模态（视频 vs. 静态文档图像）、以及下游应用（编码优化 vs. 信息结构化理解）上均无直接关联。尽管使用了类似‘agent’的概念，但其本质是编码器内部的码率调控机制，不具备感知、规划、记忆等智能体核心特征，也无法迁移至DIU任务中。因此，与DIU、VLM推理增强或agent研究无直接相关性，不值得阅读。",
    "summary": "本文提出一种用于帧级自适应神经视频压缩的动态路由自编码器框架，通过多条不同计算复杂度的编码路径实现可变比特率，并引入码率控制代理实时调整编码路径以逼近目标码率。采用联合路径优化策略提升整体率失真复杂度性能，在HEVC和UVG数据集上实现显著的BD-Rate和BD-PSNR提升。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2412.08321v3",
    "updated": "2025-08-28T12:24:24Z",
    "published": "2024-12-11T11:57:05Z",
    "title": "TGOSPA Metric Parameters Selection and Evaluation for Visual Multi-object Tracking",
    "authors": [
      "Jan Krejčí",
      "Oliver Kost",
      "Ondřej Straka",
      "Yuxuan Xia",
      "Lennart Svensson",
      "Ángel F. García-Fernández"
    ],
    "pdf_url": "https://arxiv.org/pdf/2412.08321v3.pdf",
    "comment": "Submitted to Springer International Journal of Computer Vision",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Multi-object tracking algorithms are deployed in various applications, each with different performance requirements. For example, track switches pose significant challenges for offline scene understanding, as they hinder the accuracy of data interpretation. Conversely, in online surveillance applications, their impact is often minimal. This disparity underscores the need for application-specific performance evaluations that are both simple and mathematically sound. The trajectory generalized optimal sub-pattern assignment (TGOSPA) metric offers a principled approach to evaluate multi-object tracking performance. It accounts for localization errors, the number of missed and false objects, and the number of track switches, providing a comprehensive assessment framework. This paper illustrates the effective use of the TGOSPA metric in computer vision tasks, addressing challenges posed by the need for application-specific scoring methodologies. By exploring the TGOSPA parameter selection, we enable users to compare, comprehend, and optimize the performance of algorithms tailored for specific tasks, such as target tracking and training of detector or re-ID modules.",
    "score": 2,
    "reason": "该论文聚焦于多目标跟踪中的TGOSPA评估指标参数选择与应用，属于计算机视觉中目标跟踪领域的技术优化。虽然涉及视觉任务，但其核心是跟踪性能评估，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。论文未提及文档分析、信息抽取、布局理解、实体关系建模等DIU关键环节，也未使用或适配VLM/LLM进行推理或agent架构设计。因此，无法直接应用于DIU领域，相关性极低。",
    "summary": "本文研究轨迹广义最优子模式分配（TGOSPA）度量在多目标跟踪中的参数选择问题，旨在为不同应用场景提供可解释且数学严谨的性能评估方法。通过分析定位误差、漏检、误检和轨迹切换的影响，提出针对特定任务（如检测器或重识别模块训练）的评估优化策略，提升算法调优效率。"
  },
  {
    "id": "http://arxiv.org/abs/2503.02549v2",
    "updated": "2025-08-28T12:01:39Z",
    "published": "2025-03-04T12:20:06Z",
    "title": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
    "authors": [
      "Grzegorz Skorupko",
      "Fotios Avgoustidis",
      "Carlos Martín-Isla",
      "Lidia Garrucho",
      "Dimitri A. Kessler",
      "Esmeralda Ruiz Pujadas",
      "Oliver Díaz",
      "Maciej Bobowicz",
      "Katarzyna Gwoździewicz",
      "Xavier Bargalló",
      "Paulius Jaruševičius",
      "Richard Osuala",
      "Kaisar Kushibar",
      "Karim Lekadir"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.02549v2.pdf",
    "comment": "In review",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The nnU-Net framework has played a crucial role in medical image segmentation and has become the gold standard in multitudes of applications targeting different diseases, organs, and modalities. However, so far it has been used primarily in a centralized approach where the collected data is stored in the same location where nnU-Net is trained. This centralized approach has various limitations, such as potential leakage of sensitive patient information and violation of patient privacy. Federated learning has emerged as a key approach for training segmentation models in a decentralized manner, enabling collaborative development while prioritising patient privacy. In this paper, we propose FednnU-Net, a plug-and-play, federated learning extension of the nnU-Net framework. To this end, we contribute two federated methodologies to unlock decentralized training of nnU-Net, namely, Federated Fingerprint Extraction (FFE) and Asymmetric Federated Averaging (AsymFedAvg). We conduct a comprehensive set of experiments demonstrating high and consistent performance of our methods for breast, cardiac and fetal segmentation based on a multi-modal collection of 6 datasets representing samples from 18 different institutions. To democratize research as well as real-world deployments of decentralized training in clinical centres, we publicly share our framework at https://github.com/faildeny/FednnUNet .",
    "score": 2,
    "reason": "该论文聚焦于联邦学习在医学图像分割中的应用，核心是改进nnU-Net以支持隐私保护的分布式训练。虽然涉及视觉模型与多中心数据协作，但其研究对象为医学图像分割（medical image segmentation），而非文档图像理解（DIU）。任务目标、数据类型（医学影像 vs. 文档图像）和应用场景均不相关。尽管采用了联邦学习这一前沿范式，但其技术路径无法直接迁移至DIU领域，且未涉及OCR、实体识别、关系抽取或多模态推理等DIU核心环节。因此，与DIU、VLM推理增强或Agent系统无直接关联，不值得阅读。",
    "summary": "本文提出FednnU-Net，一种基于联邦学习的nnU-Net框架扩展，用于在保护患者隐私的前提下实现多中心医学图像分割。引入Federated Fingerprint Extraction（FFE）和Asymmetric Federated Averaging（AsymFedAvg）两种方法，在乳腺、心脏和胎儿图像上验证了性能。代码已开源，但研究领域为医学图像分割，与文档图像理解无关。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20691v1",
    "updated": "2025-08-28T11:50:22Z",
    "published": "2025-08-28T11:50:22Z",
    "title": "MobileCLIP2: Improving Multi-Modal Reinforced Training",
    "authors": [
      "Fartash Faghri",
      "Pavan Kumar Anasosalu Vasu",
      "Cem Koc",
      "Vaishaal Shankar",
      "Alexander Toshev",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20691v1.pdf",
    "comment": "TMLR August 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\\times$ smaller and improves on DFN ViT-L/14 at 2.5$\\times$ lower latency. We release our pretrained models (https://github.com/apple/ml-mobileclip) and the data generation code (https://github.com/apple/ml-mobileclip-dr). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing.",
    "score": 8,
    "reason": "MobileCLIP2提出了一种改进的多模态强化训练方法，显著提升了轻量级图像-文本模型在零样本任务上的性能，尤其在低延迟场景下表现突出。该工作与DIU密切相关，因其模型可直接用于文档图像中的视觉语义理解任务（如布局感知、图文对齐），且其高效的架构和开源策略使其易于集成到DIU系统中。尽管未明确提及DIU，但其在低延迟下实现SOTA零样本准确率的能力，为移动端或实时DIU应用提供了强有力的技术支撑。TMLR为顶刊，且论文已开源，加分明显。",
    "summary": "MobileCLIP2通过优化多模态强化训练，利用改进的CLIP教师模型和细调后的标题生成器，在保持极低延迟（3-15ms）和小参数量（50-150M）的前提下，显著提升了ImageNet-1k零样本分类准确率。相比MobileCLIP-B，MobileCLIP2-B提升2.2%，MobileCLIP2-S4在精度上媲美更大模型，同时体积减半、延迟降低2.5倍。论文开源了预训练模型与数据生成代码，支持分布式构建新强化数据集，具备高度可复现性和实用性。"
  },
  {
    "id": "http://arxiv.org/abs/2406.16464v6",
    "updated": "2025-08-28T11:35:48Z",
    "published": "2024-06-24T09:13:42Z",
    "title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection",
    "authors": [
      "Junjie Chen",
      "Hang Yu",
      "Subin Huang",
      "Sanmin Liu",
      "Linfeng Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.16464v6.pdf",
    "comment": "ACM TOMM (Under Review); Code and data are available at https://github.com/CoderChen01/InterCLIP-MEP",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Sarcasm in social media, often expressed through text-image combinations, poses challenges for sentiment analysis and intention mining. Current multi-modal sarcasm detection methods have been demonstrated to overly rely on spurious cues within the textual modality, revealing a limited ability to genuinely identify sarcasm through nuanced text-image interactions. To solve this problem, we propose InterCLIP-MEP, which introduces Interactive CLIP (InterCLIP) with an efficient training strategy to extract enriched text-image representations by embedding cross-modal information directly into each encoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a dynamic dual-channel memory that stores valuable test sample knowledge during inference, acting as a non-parametric classifier for robust sarcasm recognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP achieves state-of-the-art performance, with significant accuracy and F1 score improvements on MMSD and MMSD2.0. Our code is available at https://github.com/CoderChen01/InterCLIP-MEP.",
    "score": 3,
    "reason": "该论文聚焦于多模态讽刺检测，虽涉及视觉-文本交互，但应用场景为社交媒体讽刺识别，与文档图像理解（DIU）在任务目标、数据分布和语义需求上差异显著。其核心方法InterCLIP-MEP强调跨模态表示增强与动态记忆分类器，虽具创新性，但未针对文档结构、布局建模或信息抽取设计，无法直接应用于DIU中的实体识别、关系抽取等关键环节。此外，其模型目标为情感分析而非信息结构理解，且无任何提及对表格、布局、字段关系等DIU核心要素的支持。尽管代码开源且投稿至ACM TOMM（属CCF B类），但相关性不足，故仅给予较低评分。",
    "summary": "本文提出InterCLIP-MEP用于多模态讽刺检测，通过交互式CLIP架构增强文本与图像的联合表示，并引入动态双通道记忆预测器以提升推理鲁棒性。实验在MMSD和MMSD2.0数据集上达到SOTA性能，代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20670v1",
    "updated": "2025-08-28T11:22:15Z",
    "published": "2025-08-28T11:22:15Z",
    "title": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware Synthetic Image Detection",
    "authors": [
      "Anastasios Skoularikis",
      "Stefanos-Iordanis Papadopoulos",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20670v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 \"in the wild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to \"in the wild\" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.",
    "score": 3,
    "reason": "该论文聚焦于生成图像的意图识别（幽默、艺术、虚假信息），属于多模态内容安全领域，与DIU的核心任务（文档信息理解）关联较弱。尽管涉及视觉-语言模型和多模态数据，但其目标是检测生成内容的意图而非结构化信息抽取，无法直接迁移至DIU中的OCR、实体识别或关系抽取任务。此外，未提及开源或顶会接受，且研究方向偏离DIU主线，故仅低分推荐。",
    "summary": "本文提出S-HArM数据集，包含9,576个来自社交媒体的真实世界图文对，用于区分AI生成图像的意图（幽默、艺术、虚假信息）。通过三种提示策略构建合成数据，并对比多种多模态模型架构在意图分类上的表现，发现多模态引导数据有助于提升泛化能力，但整体性能仍受限于意图推断的复杂性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.11716v2",
    "updated": "2025-08-28T11:05:03Z",
    "published": "2025-08-14T17:30:36Z",
    "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Algorithms (FakeIDet2)",
    "authors": [
      "Javier Muñoz-Haro",
      "Ruben Tolosana",
      "Julian Fierrez",
      "Ruben Vera-Rodriguez",
      "Aythami Morales"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11716v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Remote user verification in Internet-based applications is becoming increasingly important nowadays. A popular scenario for it consists of submitting a picture of the user's Identity Document (ID) to a service platform, authenticating its veracity, and then granting access to the requested digital service. An ID is well-suited to verify the identity of an individual, since it is government issued, unique, and nontransferable. However, with recent advances in Artificial Intelligence (AI), attackers can surpass security measures in IDs and create very realistic physical and synthetic fake IDs. Researchers are now trying to develop methods to detect an ever-growing number of these AI-based fakes that are almost indistinguishable from authentic (bona fide) IDs. In this counterattack effort, researchers are faced with an important challenge: the difficulty in using real data to train fake ID detectors. This real data scarcity for research and development is originated by the sensitive nature of these documents, which are usually kept private by the ID owners (the users) and the ID Holders (e.g., government, police, bank, etc.). The main contributions of our study are: 1) We propose and discuss a patch-based methodology to preserve privacy in fake ID detection research. 2) We provide a new public database, FakeIDet2-db, comprising over 900K real/fake ID patches extracted from 2,000 ID images, acquired using different smartphone sensors, illumination and height conditions, etc. In addition, three physical attacks are considered: print, screen, and composite. 3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We release a standard reproducible benchmark that considers physical and synthetic attacks from popular databases in the literature.",
    "score": 6,
    "reason": "该论文聚焦于伪造身份文档的检测，属于文档安全领域，与DIU有一定关联（如文档真实性理解），但核心目标是识别伪造而非信息理解。其提出的patch-based隐私保护方法和公开数据集对DIU研究有间接价值，尤其在数据构建方面可借鉴。然而，论文未涉及OCR、实体识别或关系抽取等DIU核心任务，也未使用VLM或LLM进行推理增强，且未体现inference scaling、reasoning或agent技术的应用。虽然数据集具有潜力，但整体更偏向安全验证而非文档理解，直接迁移性较低，因此仅给予中等评分。",
    "summary": "本文提出了一种隐私感知的伪造身份文档检测方法FakeIDet2，通过patch级处理保护敏感信息，并构建了包含90万张真实/伪造ID图像块的公开数据集FakeIDet2-db，涵盖多种物理攻击类型（打印、屏幕、合成）。研究还提供了一个可复现的基准测试框架，用于评估对抗不同伪造攻击的能力。"
  },
  {
    "id": "http://arxiv.org/abs/2409.19573v2",
    "updated": "2025-08-28T11:02:53Z",
    "published": "2024-09-29T06:21:05Z",
    "title": "See then Tell: Enhancing Key Information Extraction with Vision Grounding",
    "authors": [
      "Shuhang Liu",
      "Zhenrong Zhang",
      "Pengfei Hu",
      "Jiefeng Ma",
      "Jun Du",
      "Qing Wang",
      "Jianshu Zhang",
      "Chenyu Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2409.19573v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In the digital era, the ability to understand visually rich documents that integrate text, complex layouts, and imagery is critical. Traditional Key Information Extraction (KIE) methods primarily rely on Optical Character Recognition (OCR), which often introduces significant latency, computational overhead, and errors. Current advanced image-to-text approaches, which bypass OCR, typically yield plain text outputs without corresponding vision grounding. In this paper, we introduce STNet (See then Tell Net), a novel end-to-end model designed to deliver precise answers with relevant vision grounding. Distinctively, STNet utilizes a unique <see> token to observe pertinent image areas, aided by a decoder that interprets physical coordinates linked to this token. Positioned at the outset of the answer text, the <see> token allows the model to first see-observing the regions of the image related to the input question-and then tell-providing articulated textual responses. To enhance the model's seeing capabilities, we collect extensive structured table recognition datasets. Leveraging the advanced text processing prowess of GPT-4, we develop the TVG (TableQA with Vision Grounding) dataset, which not only provides text-based Question Answering (QA) pairs but also incorporates precise vision grounding for these pairs. Our approach demonstrates substantial advancements in KIE performance, achieving state-of-the-art results on publicly available datasets such as CORD, SROIE, and DocVQA. The code will also be made publicly available.",
    "score": 9,
    "reason": "该论文提出STNet，一种端到端的视觉引导关键信息抽取模型，通过引入<see> token实现视觉定位与文本生成的联合建模，显著提升了KIE任务中答案的可解释性和视觉 grounding 能力。其核心思想与DIU高度契合，尤其在减少OCR依赖、增强视觉理解方面具有创新性。所构建的TVG数据集（TableQA with Vision Grounding）为未来视觉-语言对齐研究提供了高质量资源，且明确提到代码将开源，具备良好复现性。虽未在顶会发表，但方法极具前沿性，直接服务于DIU中‘关系抽取’与‘视觉理解’的关键挑战，值得重点关注。",
    "summary": "本文提出STNet，一种用于关键信息抽取（KIE）的端到端模型，通过引入<see> token机制，使模型在生成答案前先‘观察’图像中相关区域，并结合物理坐标进行精准视觉定位。该方法避免了传统OCR流程，同时实现了文本回答与视觉区域的强关联。基于GPT-4构建的TVG数据集进一步支持了该方法的训练与评估。实验在CORD、SROIE和DocVQA等主流数据集上达到SOTA性能，代码计划开源，对DIU领域具有重要参考价值。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20655v1",
    "updated": "2025-08-28T11:01:33Z",
    "published": "2025-08-28T11:01:33Z",
    "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
    "authors": [
      "Sihan Yang",
      "Chenhang Cui",
      "Zihao Zhao",
      "Yiyang Zhou",
      "Weilong Yan",
      "Ying Wei",
      "Huaxiu Yao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20655v1.pdf",
    "comment": "EMNLP 2025 Findings",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations--where generated outputs are not grounded in the visual input--and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs.",
    "score": 8,
    "reason": "该论文提出基于去偏自评估的对齐方法，直接针对LVLMs中的幻觉问题，这在DIU任务中至关重要。DIU依赖于视觉与语言的精准对齐，幻觉会严重影响实体识别和关系抽取的准确性。该方法无需外部标注，具备高可扩展性，且能增强模型安全性，与DIU目标高度契合。EMNLP 2025 Findings为顶会，加分；虽未明确提及开源，但方法具有强迁移潜力，尤其适用于构建鲁棒的DIU智能体系统。",
    "summary": "本文提出一种无需外部数据的去偏自评估机制，用于提升大视觉语言模型（LVLM）的模态对齐能力。通过模型内部生成自判断分数，自动优化推理过程与偏好训练，有效减少幻觉并提升安全性。实验表明该方法显著优于传统对齐策略，为构建可靠、自主的文档理解系统提供了关键技术支撑。"
  },
  {
    "id": "http://arxiv.org/abs/2503.19065v2",
    "updated": "2025-08-28T10:56:18Z",
    "published": "2025-03-24T18:51:55Z",
    "title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation",
    "authors": [
      "Zhongyu Yang",
      "Jun Chen",
      "Dannong Xu",
      "Junjie Fei",
      "Xiaoqian Shen",
      "Liangbing Zhao",
      "Chun-Mei Feng",
      "Mohamed Elhoseiny"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.19065v2.pdf",
    "comment": "ICCV 2025, Project in https://wikiautogen.github.io/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. Our code and examples are available at https://wikiautogen.github.io/ .",
    "score": 8,
    "reason": "该论文提出WikiAutoGen，一个面向多模态维基风格文章生成的系统，引入图像与文本联合生成，并通过多视角自我反思机制提升内容准确性与连贯性。其核心思想——结合视觉与语言信息进行知识生成——与DIU中对文档语义理解、布局与图文关系建模的需求高度契合。此外，其提出的WikiSeek基准可为DIU中的多模态信息抽取提供评估支持。虽非直接解决DIU任务，但其在多模态内容融合、事实一致性增强方面的技术可直接迁移至DIU中的视觉信息抽取与实体关系推理环节。ICCV 2025为CCF A类会议，且项目已开源，具备较强可复现性与研究价值。",
    "summary": "WikiAutoGen提出一种多模态维基风格文章自动生成系统，通过整合文本与图像信息，并引入多视角自我反思机制提升生成内容的准确性与丰富性。作者构建了WikiSeek基准用于评估多模态知识生成能力，实验表明该方法显著优于现有文本-only方法。代码与示例已公开。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20640v1",
    "updated": "2025-08-28T10:38:13Z",
    "published": "2025-08-28T10:38:13Z",
    "title": "CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models",
    "authors": [
      "Ayan Banerjee",
      "Fernando Vilariño",
      "Josep Lladós"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20640v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the \"style-first, identity-after\" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.",
    "score": 2,
    "reason": "该论文聚焦于使用扩散模型生成具有面部身份保留的涂鸦艺术，核心贡献在于风格迁移与身份一致性之间的平衡。虽然涉及视觉生成和人脸处理，但其应用领域为艺术创作而非文档理解，且未涉及OCR、实体识别、关系抽取或DIU相关任务。尽管使用了CLIP和LoRA等多模态技术，但其目标与DIU无直接关联，也无法直接迁移至文档信息抽取或视觉推理场景。因此，与DIU、VLM推理增强或Agent系统无关，不值得阅读。",
    "summary": "CraftGraffiti提出一种端到端的文本引导涂鸦生成框架，通过LoRA微调扩散变换器实现风格迁移，并引入基于身份嵌入的自注意力机制以保持面部特征一致性。利用CLIP引导提示扩展实现姿态动态调整，验证了‘先风格后身份’范式在减少属性漂移方面的优势。实验显示其在面部一致性和美学偏好上表现优异，已在实际艺术节部署。"
  },
  {
    "id": "http://arxiv.org/abs/2508.18984v2",
    "updated": "2025-08-28T10:31:44Z",
    "published": "2025-08-26T12:32:55Z",
    "title": "Enhancing Document VQA Models via Retrieval-Augmented Generation",
    "authors": [
      "Eric López",
      "Artemis Llabrés",
      "Ernest Valveny"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.18984v2.pdf",
    "comment": "Accepted at Workshop on Machine Learning in Document Analysis and Recognition (ICDAR WML 2025), Wuhan, China",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Document Visual Question Answering (Document VQA) must cope with documents that span dozens of pages, yet leading systems still concatenate every page or rely on very large vision-language models, both of which are memory-hungry. Retrieval-Augmented Generation (RAG) offers an attractive alternative, first retrieving a concise set of relevant segments before generating answers from this selected evidence. In this paper, we systematically evaluate the impact of incorporating RAG into Document VQA through different retrieval variants - text-based retrieval using OCR tokens and purely visual retrieval without OCR - across multiple models and benchmarks. Evaluated on the multi-page datasets MP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the \"concatenate-all-pages\" baseline by up to +22.5 ANLS, while the visual variant achieves +5.0 ANLS improvement without requiring any text extraction. An ablation confirms that retrieval and reranking components drive most of the gain, whereas the layout-guided chunking strategy - proposed in several recent works to leverage page structure - fails to help on these datasets. Our experiments demonstrate that careful evidence selection consistently boosts accuracy across multiple model sizes and multi-page benchmarks, underscoring its practical value for real-world Document VQA.",
    "score": 8,
    "reason": "该论文聚焦于文档VQA中的关键挑战——多页文档的高效处理，提出使用RAG框架通过检索相关段落来提升模型性能，显著优于传统全页拼接或大模型方案。其文本中心和纯视觉两种检索变体均有效，尤其纯视觉检索无需OCR，契合OCR-free DIU趋势。实验覆盖多个主流数据集，结果具有说服力。虽为ICDAR WML会议（非CCF A类），但主题高度契合DIU任务中的信息检索与精准生成环节，且可直接应用于DIU系统中作为核心组件。未提开源，略有扣分。",
    "summary": "本文提出一种基于检索增强生成（RAG）的文档视觉问答方法，通过检索相关文档片段而非拼接全部页面来提升效率与精度。在MP-DocVQA、DUDE和InfographicVQA等多页数据集上，文本检索版本相比基线提升高达22.5 ANLS，视觉检索版本亦实现+5.0 ANLS提升，且无需依赖OCR。消融实验证明检索与重排序是主要增益来源，布局分块策略效果不佳。该方法为大规模文档理解提供了高效、可扩展的新范式。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20626v1",
    "updated": "2025-08-28T10:19:06Z",
    "published": "2025-08-28T10:19:06Z",
    "title": "ArtFace: Towards Historical Portrait Face Identification via Model Adaptation",
    "authors": [
      "Francois Poh",
      "Anjith George",
      "Sébastien Marcel"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20626v1.pdf",
    "comment": "4 pages, 3 figures. ArtMetrics @ ICCV 2025 (non-archival). Paper page at https://www.idiap.ch/paper/artface/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Identifying sitters in historical paintings is a key task for art historians, offering insight into their lives and how they chose to be seen. However, the process is often subjective and limited by the lack of data and stylistic variations. Automated facial recognition is capable of handling challenging conditions and can assist, but while traditional facial recognition models perform well on photographs, they struggle with paintings due to domain shift and high intra-class variation. Artistic factors such as style, skill, intent, and influence from other works further complicate recognition. In this work, we investigate the potential of foundation models to improve facial recognition in artworks. By fine-tuning foundation models and integrating their embeddings with those from conventional facial recognition networks, we demonstrate notable improvements over current state-of-the-art methods. Our results show that foundation models can bridge the gap where traditional methods are ineffective. Paper page at https://www.idiap.ch/paper/artface/",
    "score": 3,
    "reason": "该论文聚焦于历史肖像画中人物面部识别，虽涉及视觉识别与模型适应，但应用场景为艺术史领域，与文档图像理解（DIU）无直接关联。尽管使用了基础模型进行微调，其核心任务是艺术品中的身份识别，而非文档结构、文本语义或字段关系抽取。此外，未提及OCR、实体识别或关系抽取等DIU关键环节，也未利用布局、表格或多模态推理等DIU相关技术。虽然在方法上可能有间接启发，但无法直接迁移至DIU场景，故不具直接应用价值。",
    "summary": "本文提出ArtFace方法，通过微调基础模型并融合传统人脸识别网络的嵌入特征，提升历史绘画中人物面部的识别性能。研究针对艺术风格带来的域偏移和类内差异挑战，展示了基础模型在跨域人脸识别中的潜力，但应用场景为艺术史领域，与文档图像理解无关。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20623v1",
    "updated": "2025-08-28T10:15:38Z",
    "published": "2025-08-28T10:15:38Z",
    "title": "AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images",
    "authors": [
      "Shiqi Xin",
      "Xiaolin Zhang",
      "Yanbin Liu",
      "Peng Zhang",
      "Caifeng Shan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20623v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.",
    "score": 2,
    "reason": "该论文聚焦于从正面图像生成完整3D头像的后脑部分，属于三维重建与数字人生成领域。虽然涉及视觉建模和多视角一致性问题，但其核心目标是人物头像的几何完整性，与文档图像理解（DIU）无直接关联。尽管使用了Gaussian Splatting等先进视觉技术，但应用场景、任务目标和数据形式均不匹配。未提及OCR、实体识别、关系抽取或文档布局分析，也未涉及多模态模型在文档理解中的应用。因此，无法直接迁移至DIU任务，不值得阅读。",
    "summary": "本文提出AvatarBack框架，通过生成后视伪图像并结合自适应空间对齐策略，实现从单张正面图像重建完整的3D高斯头像，显著提升后脑区域的几何一致性和视觉真实感。实验表明该方法在多个数据集上优于现有方法，且生成的头像具备良好动画性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20622v1",
    "updated": "2025-08-28T10:13:33Z",
    "published": "2025-08-28T10:13:33Z",
    "title": "Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications",
    "authors": [
      "Immanuel Roßteutscher",
      "Klaus S. Drese",
      "Thorsten Uphues"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20622v1.pdf",
    "comment": "Submitted to IEEE Access. This is a preprint version. 14 pages, 6 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We investigated the adaptation and performance of Masked Autoencoders (MAEs) with Vision Transformer (ViT) architectures for self-supervised representation learning on one-dimensional (1D) ultrasound signals. Although MAEs have demonstrated significant success in computer vision and other domains, their use for 1D signal analysis, especially for raw ultrasound data, remains largely unexplored. Ultrasound signals are vital in industrial applications such as non-destructive testing (NDT) and structural health monitoring (SHM), where labeled data are often scarce and signal processing is highly task-specific. We propose an approach that leverages MAE to pre-train on unlabeled synthetic ultrasound signals, enabling the model to learn robust representations that enhance performance in downstream tasks, such as time-of-flight (ToF) classification. This study systematically investigated the impact of model size, patch size, and masking ratio on pre-training efficiency and downstream accuracy. Our results show that pre-trained models significantly outperform models trained from scratch and strong convolutional neural network (CNN) baselines optimized for the downstream task. Additionally, pre-training on synthetic data demonstrates superior transferability to real-world measured signals compared with training solely on limited real datasets. This study underscores the potential of MAEs for advancing ultrasound signal analysis through scalable, self-supervised learning.",
    "score": 2,
    "reason": "该论文研究的是基于掩码自编码器（MAE）在一维超声信号上的自监督表示学习，属于音频/信号处理领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。尽管MAE和ViT是相关技术，但其应用场景为工业超声信号分析，无法直接迁移至DIU任务。此外，论文未涉及文档、布局、表格、OCR、实体识别或关系抽取等核心DIU内容，也未使用多模态或LLM技术，因此不值得阅读。",
    "summary": "本文探索了将掩码自编码器（MAE）与视觉Transformer（ViT）架构应用于一维超声信号的自监督表征学习，通过在合成数据上预训练，提升时间飞行（ToF）分类等下游任务性能。实验表明，预训练模型优于从头训练和CNN基线，且在真实信号上具有更强的泛化能力。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20621v1",
    "updated": "2025-08-28T10:11:24Z",
    "published": "2025-08-28T10:11:24Z",
    "title": "Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification",
    "authors": [
      "Smriti Joshi",
      "Lidia Garrucho",
      "Richard Osuala",
      "Oliver Diaz",
      "Karim Lekadir"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20621v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Breast cancer is one of the leading causes of cancer-related mortality in women, and early detection is essential for improving outcomes. Magnetic resonance imaging (MRI) is a highly sensitive tool for breast cancer detection, particularly in women at high risk or with dense breast tissue, where mammography is less effective. The ODELIA consortium organized a multi-center challenge to foster AI-based solutions for breast cancer diagnosis and classification. The dataset included 511 studies from six European centers, acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study was labeled for the left and right breast as no lesion, benign lesion, or malignant lesion. We developed a SwinUNETR-based deep learning framework that incorporates breast region masking, extensive data augmentation, and ensemble learning to improve robustness and generalizability. Our method achieved second place on the challenge leaderboard, highlighting its potential to support clinical breast MRI interpretation. We publicly share our codebase at https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.",
    "score": 2,
    "reason": "该论文聚焦于MRI图像中的乳腺癌分类，属于医学影像分析领域，与文档图像理解（DIU）无直接关联。尽管使用了SwinUNETR架构并涉及多模态信息处理，但其任务目标、数据类型（医学影像）和应用场景均与DIU无关。虽有开源代码，但无法直接迁移至DIU任务中。此外，未涉及视觉语言模型（VLM）、推理增强（inference scaling）、agent或文档理解相关技术，因此不值得阅读。",
    "summary": "本文提出一种基于掩码引导的多通道SwinUNETR框架，用于乳腺MRI图像中的病变分类（良性/恶性/无病灶），在ODELIA多中心乳腺癌MRI挑战赛中获得第二名。通过引入乳腺区域掩码、数据增强和集成学习提升模型鲁棒性，代码已公开。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20615v1",
    "updated": "2025-08-28T10:02:06Z",
    "published": "2025-08-28T10:02:06Z",
    "title": "EmoCAST: Emotional Talking Portrait via Emotive Text Description",
    "authors": [
      "Yiguo Jiang",
      "Xiaodong Cun",
      "Yong Zhang",
      "Yudian Zheng",
      "Fan Tang",
      "Chi-Man Pun"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20615v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are primarily collected in lab settings, further exacerbating these shortcomings. Consequently, these limitations substantially hinder practical applications in real-world scenarios. To address these challenges, we propose EmoCAST, a diffusion-based framework with two key modules for precise text-driven emotional synthesis. In appearance modeling, emotional prompts are integrated through a text-guided decoupled emotive module, enhancing the spatial knowledge to improve emotion comprehension. To improve the relationship between audio and emotion, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide more precise facial motion synthesis. Additionally, we construct an emotional talking head dataset with comprehensive emotive text descriptions to optimize the framework's performance. Based on the proposed dataset, we propose an emotion-aware sampling training strategy and a progressive functional training strategy that further improve the model's ability to capture nuanced expressive features and achieve accurate lip-synchronization. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST",
    "score": 2,
    "reason": "该论文聚焦于情感驱动的说话头视频生成，属于计算机视觉中的语音-表情合成方向。虽然涉及文本描述与视觉生成的多模态交互，但其核心任务是生成具有情感表达的人脸视频，与文档图像理解（DIU）在任务目标、数据形式和应用场景上均无直接关联。尽管使用了文本引导的扩散模型，但其技术路径（如音频-表情对齐、面部运动生成）无法直接迁移至DIU领域。此外，未提及任何与OCR、实体识别、关系抽取或文档布局相关的内容，也未涉及VLM推理增强或agent系统设计。因此，不值得阅读。",
    "summary": "EmoCAST提出一种基于扩散模型的情感驱动说话头视频生成框架，通过解耦情感模块和情感音频注意力机制，实现文本描述控制下的高保真、情感丰富且唇音同步的视频生成。同时构建了一个包含丰富情感文本描述的数据集，并提出情感感知采样和渐进式训练策略以提升表达细节。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20613v1",
    "updated": "2025-08-28T10:00:39Z",
    "published": "2025-08-28T10:00:39Z",
    "title": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization",
    "authors": [
      "Yixiang Qiu",
      "Yanhan Liu",
      "Hongyao Yu",
      "Hao Fang",
      "Bin Chen",
      "Shu-Tao Xia",
      "Ke Xu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20613v1.pdf",
    "comment": "10 pages, 5 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs.",
    "score": 2,
    "reason": "该论文研究的是分裂推理（Split Inference）中的隐私风险，提出一种基于GAN的数据重建攻击方法。虽然涉及深度学习模型的隐私问题，但其核心是攻击性研究，聚焦于中间特征的逆向恢复，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型（LLM）推理增强（inference scaling）或智能体（agent）系统无直接关联。其技术路径（GAN重构、L1-ball约束）无法直接迁移至DIU任务中的信息抽取、关系建模或推理优化。因此，尽管在计算机视觉领域有一定价值，但对本研究方向无实质性帮助。",
    "summary": "本文提出一种基于GAN的新型数据重建攻击框架，通过渐进式特征优化（PFO）提升分裂推理中中间特征的重建质量，尤其在高分辨率和深层网络下表现优异。通过引入L1-ball约束稳定训练过程，显著提升重建图像的语义保真度和真实感。实验表明该方法在多种场景下优于现有攻击方法，揭示了当前分裂推理在隐私保护方面的脆弱性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20612v1",
    "updated": "2025-08-28T10:00:23Z",
    "published": "2025-08-28T10:00:23Z",
    "title": "Physics Informed Generative Models for Magnetic Field Images",
    "authors": [
      "Aye Phyu Phyu Aung",
      "Lucas Lum",
      "Zhansen Shi",
      "Wen Qiu",
      "Bernice Zee",
      "JM Chin",
      "Yeow Kheng Lim",
      "J. Senthilnath"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20612v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process.",
    "score": 2,
    "reason": "该论文聚焦于磁力场成像（MFI）的生成模型，用于半导体缺陷检测，属于物理信息驱动的生成建模在工业检测中的应用。尽管其方法涉及物理约束与扩散模型，但研究对象为特定工业场景下的磁力场图像，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。其数据类型（磁力场图像）和任务目标（缺陷定位）与DIU领域无关，无法直接迁移至文档理解场景，因此不值得阅读。",
    "summary": "本文提出PI-GenMFI，一种融合物理先验的扩散生成模型，用于合成磁力场成像（MFI）数据，以解决半导体制造中MFI数据稀缺的问题。模型针对常见缺陷类型（如电源短路）生成合成MFI图像，用于提升缺陷定位效率。通过与SOTA生成模型对比，并结合专家评估与多种信号处理指标验证性能，结果表明该方法在生成真实感图像方面表现良好。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20605v1",
    "updated": "2025-08-28T09:51:14Z",
    "published": "2025-08-28T09:51:14Z",
    "title": "Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction",
    "authors": [
      "Karl-Philippe Beaudet",
      "Sidaty El Hadramy",
      "Philippe C Cattin",
      "Juan Verde",
      "Stéphane Cotin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20605v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Intraoperative ultrasound images are inherently challenging to interpret in liver surgery due to the limited field of view and complex anatomical structures. Bridging the gap between preoperative and intraoperative data is crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS) offers a potential solution by enabling the reconstruction of the entire organ, which facilitates registration between preoperative computed tomography (CT) scans and intraoperative IVUS images. In this work, we propose an optimization-based calibration method using a 3D-printed phantom for accurate 3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise alignment of tracked IVUS data with preoperative CT images, improving intraoperative navigation. We validated our method using in vivo swine liver images, achieving a calibration error from 0.88 to 1.80 mm and a registration error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT scan. Our method provides a reliable and accurate means of calibration and volume reconstruction. It can be used to register intraoperative ultrasound images with preoperative CT images in the context of liver surgery, and enhance intraoperative guidance.",
    "score": 1,
    "reason": "该论文研究的是血管内超声（IVUS）在肝脏手术中的三维重建与校准，属于医学影像处理领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型（LLM）推理增强或智能体（agent）技术无直接关联。其核心方法为基于优化的物理校准，应用场景为外科手术导航，无法直接迁移至文档分析任务。因此不相关。",
    "summary": "本文提出一种基于3D打印体模的优化校准方法，用于实现术中血管内超声（IVUS）三维体积重建，并与术前CT图像进行配准，以提升肝脏手术中的导航精度。实验在猪肝体内数据上验证，校准误差为0.88–1.80 mm，配准误差为3.40–5.71 mm。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20604v1",
    "updated": "2025-08-28T09:49:27Z",
    "published": "2025-08-28T09:49:27Z",
    "title": "Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion",
    "authors": [
      "Zheng Qin",
      "Yabing Wang",
      "Minghui Yang",
      "Sanping Zhou",
      "Ming Yang",
      "Le Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20604v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Generating 3D human motions from text is a challenging yet valuable task. The key aspects of this task are ensuring text-motion consistency and achieving generation diversity. Although recent advancements have enabled the generation of precise and high-quality human motions from text, achieving diversity in the generated motions remains a significant challenge. In this paper, we aim to overcome the above challenge by designing a simple yet effective text-to-motion generation method, \\textit{i.e.}, Diverse-T2M. Our method introduces uncertainty into the generation process, enabling the generation of highly diverse motions while preserving the semantic consistency of the text. Specifically, we propose a novel perspective that utilizes noise signals as carriers of diversity information in transformer-based methods, facilitating a explicit modeling of uncertainty. Moreover, we construct a latent space where text is projected into a continuous representation, instead of a rigid one-to-one mapping, and integrate a latent space sampler to introduce stochastic sampling into the generation process, thereby enhancing the diversity and uncertainty of the outputs. Our results on text-to-motion generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our method significantly enhances diversity while maintaining state-of-the-art performance in text consistency.",
    "score": 2,
    "reason": "该论文聚焦于文本到3D人体动作生成，涉及不确定性建模与多样性提升，但其核心任务为动作生成而非文档图像理解（DIU）。尽管引入了噪声和潜在空间采样等技术，这些方法与DIU中的OCR、实体识别或关系抽取无直接关联。此外，未涉及多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）相关技术，且研究领域与DIU无关。因此不值得阅读。",
    "summary": "本文提出Diverse-T2M方法，通过在Transformer框架中引入噪声信号作为多样性载体，并构建连续的文本-动作潜在空间与随机采样机制，以提升从文本生成3D人体动作的多样性，同时保持语义一致性。在HumanML3D和KIT-ML数据集上验证了其优越性，但研究主题与文档图像理解无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19650v2",
    "updated": "2025-08-28T09:44:26Z",
    "published": "2025-08-27T07:58:16Z",
    "title": "Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models",
    "authors": [
      "Hou Xia",
      "Zheren Fu",
      "Fangcan Ling",
      "Jiajun Li",
      "Yi Tu",
      "Zhendong Mao",
      "Yongdong Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19650v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Large video language models (LVLMs) have made notable progress in video understanding, spurring the development of corresponding evaluation benchmarks. However, existing benchmarks generally assess overall performance across entire video sequences, overlooking nuanced behaviors such as contextual positional bias, a critical yet under-explored aspect of LVLM performance. We present Video-LevelGauge, a dedicated benchmark designed to systematically assess positional bias in LVLMs. We employ standardized probes and customized contextual setups, allowing flexible control over context length, probe position, and contextual types to simulate diverse real-world scenarios. In addition, we introduce a comprehensive analysis method that combines statistical measures with morphological pattern recognition to characterize bias. Our benchmark comprises 438 manually curated videos spanning multiple types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended questions, validated for their effectiveness in exposing positional bias. Based on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and open-source models. Our findings reveal significant positional biases in many leading open-source models, typically exhibiting head or neighbor-content preferences. In contrast, commercial models such as Gemini2.5-Pro show impressive, consistent performance across entire video sequences. Further analyses on context length, context variation, and model scale provide actionable insights for mitigating bias and guiding model enhancement.https://github.com/Cola-any/Video-LevelGauge",
    "score": 4,
    "reason": "该论文聚焦于大视频语言模型中的上下文位置偏差问题，虽涉及多模态模型性能分析，但研究对象为视频理解而非文档图像理解（DIU）。尽管其提出的benchmark和分析方法在理论上可能对视觉语言模型的鲁棒性评估有借鉴意义，但应用场景（视频 vs 文档）差异显著，难以直接迁移至DIU任务。此外，未提及开源或顶会发表，且与inference scaling、reasoning增强、agent系统等核心关注方向无直接关联，因此相关性较低。",
    "summary": "本文提出Video-LevelGauge，一个用于系统评估大视频语言模型（LVLMs）中上下文位置偏差的基准。通过438个精心设计的视频和1,177道多项选择题及120道开放题，对27个主流LVLMs进行测试，发现多数开源模型存在头部或邻近内容偏好，而商业模型如Gemini 2.5-Pro表现更稳定。研究提供了一种结合统计与形态模式识别的分析方法，为提升视频模型鲁棒性提供指导。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20600v1",
    "updated": "2025-08-28T09:43:59Z",
    "published": "2025-08-28T09:43:59Z",
    "title": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction",
    "authors": [
      "Kian Anvari Hamedani",
      "Narges Razizadeh",
      "Shahabedin Nabavi",
      "Mohsen Ebrahimi Moghaddam"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20600v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.",
    "score": 2,
    "reason": "该论文聚焦于心脏MRI图像重建，属于医学影像处理领域，与文档图像理解（DIU）、多模态模型（VLM）、大语言模型（LLM）推理增强或智能体（agent）技术无直接关联。其方法基于GAN和迭代优化的卷积网络，虽在图像重建性能上表现优异，但应用场景、技术路径与DIU核心任务（文本识别、实体抽取、关系建模）不具迁移性。不属于目标领域相关论文。",
    "summary": "GENRE-CMR提出一种基于生成对抗网络的残差深度展开架构，用于加速心血管磁共振成像的重建。通过引入边缘感知区域损失和统计分布对齐损失，提升重建质量与跨采集协议的泛化能力，在未见数据上达到0.9552 SSIM和38.90 dB PSNR，优于现有方法。"
  },
  {
    "id": "http://arxiv.org/abs/2407.11691v4",
    "updated": "2025-08-28T09:40:49Z",
    "published": "2024-07-16T13:06:15Z",
    "title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
    "authors": [
      "Haodong Duan",
      "Xinyu Fang",
      "Junming Yang",
      "Xiangyu Zhao",
      "Yuxuan Qiao",
      "Mo Li",
      "Amit Agarwal",
      "Zhe Chen",
      "Lin Chen",
      "Yuan Liu",
      "Yubo Ma",
      "Hailong Sun",
      "Yifan Zhang",
      "Shiyin Lu",
      "Tack Hwa Wong",
      "Weiyun Wang",
      "Peiheng Zhou",
      "Xiaozhe Li",
      "Chaoyou Fu",
      "Junbo Cui",
      "Jixuan Chen",
      "Enxin Song",
      "Song Mao",
      "Shengyuan Ding",
      "Tianhao Liang",
      "Zicheng Zhang",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Pan Zhang",
      "Jiaqi Wang",
      "Dahua Lin",
      "Kai Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2407.11691v4.pdf",
    "comment": "Updated on 2025.08.28, data cut down to 2025.06.30",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 200+ different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 80 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released on https://github.com/open-compass/VLMEvalKit and is actively maintained.",
    "score": 9,
    "reason": "该论文提出VLMEvalKit，一个面向大视觉多模态模型（VLM）的开源评估工具包，直接服务于DIU领域中VLM性能评估的核心需求。其支持超过200个模型和80+多模态基准，具备高度可扩展性与易用性，尤其适合用于评测DIU任务中新兴的端到端VLM系统。开源且持续维护，对研究者复现结果、比较算法性能极为关键。虽然非直接解决DIU任务本身，但作为评估基础设施，是开展高质量DIU研究的必备工具，尤其在推动基于VLM的DIU系统开发中具有不可替代的作用。CCF A类会议（CVPR）接受，加分显著。",
    "summary": "VLMEvalKit是一个基于PyTorch的开源工具包，用于评估大型多模态模型，支持超过200种模型和80多个多模态基准，提供统一接口以简化数据准备、分布式推理、后处理和指标计算。该工具包已用于构建OpenVLM Leaderboard，促进多模态学习研究的可复现性。项目开源并持续维护，适用于DIU领域中VLM系统的性能评估与对比。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20595v1",
    "updated": "2025-08-28T09:34:53Z",
    "published": "2025-08-28T09:34:53Z",
    "title": "Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations",
    "authors": [
      "Mengxiao Huang",
      "Minglei Shu",
      "Shuwang Zhou",
      "Zhaoyang Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20595v1.pdf",
    "comment": "Accepted to IEEE IJCNN 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Deepfake technology, driven by Generative Adversarial Networks (GANs), poses significant risks to privacy and societal security. Existing detection methods are predominantly passive, focusing on post-event analysis without preventing attacks. To address this, we propose an active defense method based on low-frequency perceptual perturbations to disrupt face swapping manipulation, reducing the performance and naturalness of generated content. Unlike prior approaches that used low-frequency perturbations to impact classification accuracy,our method directly targets the generative process of deepfake techniques. We combine frequency and spatial domain features to strengthen defenses. By introducing artifacts through low-frequency perturbations while preserving high-frequency details, we ensure the output remains visually plausible. Additionally, we design a complete architecture featuring an encoder, a perturbation generator, and a decoder, leveraging discrete wavelet transform (DWT) to extract low-frequency components and generate perturbations that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW demonstrate significant reductions in face-swapping effectiveness, improved defense success rates, and preservation of visual quality.",
    "score": 2,
    "reason": "该论文研究的是对抗性攻击与防御在人脸交换（face swapping）中的应用，核心是通过低频感知扰动干扰生成过程。虽然涉及视觉内容篡改与防御，但其目标是破坏深度伪造技术，而非文档图像理解（DIU）。论文主题与DIU无直接关联，未涉及OCR、实体识别、关系抽取或文档结构理解等关键任务。此外，未使用VLM/LLM进行推理增强，也未涉及inference scaling、reasoning链或agent系统。尽管发表于IEEE IJCNN（非CCF A类），但内容与DIU领域无关，无法直接迁移应用。",
    "summary": "本文提出一种主动防御方法，利用低频感知扰动破坏人脸交换的生成过程，通过离散小波变换提取低频成分并注入人工伪影，在保持视觉自然性的同时降低生成内容的质量与真实性。实验表明该方法能有效削弱深度伪造效果。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20594v1",
    "updated": "2025-08-28T09:32:51Z",
    "published": "2025-08-28T09:32:51Z",
    "title": "UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching",
    "authors": [
      "Yuqi Han",
      "Songqian Zhang",
      "Weijian Su",
      "Ke Li",
      "Jiayu Yang",
      "Jinli Suo",
      "Qiang Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20594v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The thermal camera excels at perceiving outdoor environments under low-light conditions, making it ideal for applications such as nighttime autonomous driving and unmanned navigation. However, thermal cameras encounter challenges when capturing signage from objects made of similar materials, which can pose safety risks for accurately understanding semantics in autonomous driving systems. In contrast, the neuromorphic vision camera, also known as an event camera, detects changes in light intensity asynchronously and has proven effective in high-speed, low-light traffic environments. Recognizing the complementary characteristics of these two modalities, this paper proposes UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage in low-illumination environments, targeting elements such as license plates and roadblock indicators. To address the signage blind spots of thermal imaging and the non-uniform sampling of event cameras, we developed a dual-boosting mechanism that fuses thermal frames and event signals for consistent signage representation over time. The proposed method utilizes thermal frames to provide accurate motion cues as temporal references for aligning the uneven event signals. At the same time, event signals contribute subtle signage content to the raw thermal frames, enhancing the overall understanding of the environment. The proposed method is validated on datasets collected from real-world scenarios, demonstrating superior quality in traffic signage sketching and improved detection accuracy at the perceptual level.",
    "score": 2,
    "reason": "该论文聚焦于热成像与事件相机在交通标志增强中的融合，属于计算机视觉领域中特定场景下的多模态图像处理。虽然涉及多模态数据融合，但其应用目标是夜间自动驾驶中的交通标志识别，与文档图像理解（DIU）无直接关联。研究内容不涉及OCR、实体识别、关系抽取等DIU核心任务，也未使用或适配VLM/LLM进行推理或agent架构。尽管方法上存在多模态融合的思想，但应用场景和任务本质差异大，无法直接迁移至DIU领域。因此不值得阅读。",
    "summary": "本文提出UTA-Sign，一种基于热成像与事件相机的无监督视频增强方法，用于提升低光照环境下交通标志的感知能力。通过融合热帧的运动信息与事件信号的细节内容，实现对交通标志的连续清晰刻画，提升检测精度。实验在真实场景数据集上验证了有效性。"
  },
  {
    "id": "http://arxiv.org/abs/2507.01201v5",
    "updated": "2025-08-28T09:29:38Z",
    "published": "2025-07-01T21:43:50Z",
    "title": "Escaping Plato's Cave: JAM for Aligning Independently Trained Vision and Language Models",
    "authors": [
      "Lauren Hyoseo Yoon",
      "Yisong Yue",
      "Been Kim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.01201v5.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Independently trained vision and language models inhabit disjoint representational spaces, shaped by their respective modalities, objectives, and architectures. The Platonic Representation Hypothesis (PRH) suggests these models may nonetheless converge toward a shared statistical model of reality. This raises a fundamental question: can we move beyond post-hoc detection of such alignment and explicitly optimize for it? We argue this challenge is most critical in fine-grained contextual distinctions-where multiple descriptions share global semantics but differ in subtle compositional details. We address this with the Joint Autoencoder Modulator (JAM), which aligns frozen unimodal models by jointly training modality-specific autoencoders with coordinated reconstruction and cross-modal alignment objectives. We systematically evaluate JAM across three design axes: (i) alignment objectives, introducing our multimodal Spread Loss that outperforms classic contrastive methods; (ii) the layer depth at which alignment is most effective; and (iii) the role of foundation model scale in representational convergence. Our findings show that JAM reliably induces alignment even across independently trained representations, offering both theoretical insight into the structure of shared semantics and practical guidance for transforming generalist unimodal foundations into specialist multimodal models.",
    "score": 8,
    "reason": "该论文提出JAM方法，通过联合训练模态特定的自编码器来对齐独立训练的视觉与语言模型，解决了多模态表示空间不一致的问题。其核心思想——在冻结预训练模型基础上实现跨模态对齐——对DIU任务极具价值，尤其适用于构建基于VLM的端到端文档理解系统。提出的多模态Spread Loss在细粒度语义区分上表现优异，可直接应用于提升DIU中布局与内容语义对齐的能力。虽然未明确提及DIU，但其技术路径可直接迁移至视觉-文本对齐环节（如OCR后文本与图像布局的融合），是当前VLM对齐前沿工作，值得深入阅读。",
    "summary": "本文提出Joint Autoencoder Modulator (JAM)，一种用于对齐独立训练的视觉与语言模型的方法。通过联合训练模态特定的自编码器，并引入跨模态对齐目标与多模态Spread Loss，有效提升了不同模态表示之间的对齐能力，尤其在细粒度语义差异识别上表现突出。实验验证了其在不同层深度和模型规模下的有效性，为构建强对齐的多模态基础模型提供了理论与实践指导。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20586v1",
    "updated": "2025-08-28T09:25:52Z",
    "published": "2025-08-28T09:25:52Z",
    "title": "FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models",
    "authors": [
      "Zheng Chong",
      "Yanwei Lei",
      "Shiyue Zhang",
      "Zhuandi He",
      "Zhen Wang",
      "Xujie Zhang",
      "Xiao Dong",
      "Yiling Wu",
      "Dongmei Jiang",
      "Xiaodan Liang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20586v1.pdf",
    "comment": "16 pages, 10 figures, 5 tables",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.",
    "score": 2,
    "reason": "该论文聚焦于虚拟试穿（virtual try-on）任务，属于图像生成与风格迁移领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。尽管其提出缓存扩散模型以提升效率的技术思路有一定启发性，但应用场景和核心目标与DIU完全无关，无法直接迁移至文档信息抽取、实体识别或关系建模等任务中。因此不值得阅读。",
    "summary": "FastFit提出一种基于可缓存扩散架构的高速多参考虚拟试穿方法，通过半注意力机制和类别嵌入替代时间步嵌入，实现参考特征的一次计算多次复用，显著提升推理效率（平均3.5倍加速）。同时构建了DressCode-MR数据集用于复杂多参考试穿研究。"
  },
  {
    "id": "http://arxiv.org/abs/2504.17991v2",
    "updated": "2025-08-28T09:23:39Z",
    "published": "2025-04-25T00:22:17Z",
    "title": "RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation",
    "authors": [
      "Zheng Qin",
      "Le Wang",
      "Yabing Wang",
      "Sanping Zhou",
      "Gang Hua",
      "Wei Tang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.17991v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent image-goal navigation (ImageNav) methods learn a perception-action policy by separately capturing semantic features of the goal and egocentric images, then passing them to a policy network. However, challenges remain: (1) Semantic features often fail to provide accurate directional information, leading to superfluous actions, and (2) performance drops significantly when viewpoint inconsistencies arise between training and application. To address these challenges, we propose RSRNav, a simple yet effective method that reasons spatial relationships between the goal and current observations as navigation guidance. Specifically, we model the spatial relationship by constructing correlations between the goal and current observations, which are then passed to the policy network for action prediction. These correlations are progressively refined using fine-grained cross-correlation and direction-aware correlation for more precise navigation. Extensive evaluation of RSRNav on three benchmark datasets demonstrates superior navigation performance, particularly in the \"user-matched goal\" setting, highlighting its potential for real-world applications.",
    "score": 3,
    "reason": "该论文聚焦于图像目标导航（ImageNav）中的空间关系推理，虽涉及视觉理解与空间推理，但其应用场景为机器人导航而非文档图像理解。尽管引入了空间关系建模的思想，但与DIU任务在目标、输入模态（egocentric images vs. document images）、下游任务（路径规划 vs. 信息抽取）上均无直接关联。其方法无法直接迁移至DIU领域，且未涉及OCR、实体识别、关系抽取或多模态文档理解等核心环节。因此不值得阅读。",
    "summary": "RSRNav提出一种基于空间关系推理的图像目标导航方法，通过构建目标图像与当前观测之间的细粒度跨相关性来指导动作决策，提升导航精度，尤其在视角不一致场景下表现优越。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20579v1",
    "updated": "2025-08-28T09:17:57Z",
    "published": "2025-08-28T09:17:57Z",
    "title": "GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition",
    "authors": [
      "Debasis Maji",
      "Debaditya Barman"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20579v1.pdf",
    "comment": "11 pages, 6 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Facial expression recognition (FER) is a crucial task in computer vision with wide range of applications including human computer interaction, surveillance, and assistive technologies. However, challenges such as occlusion, expression variability, and lack of interpretability hinder the performance of traditional FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by modeling relational dependencies between facial landmarks, enabling structured and interpretable learning. In this paper, we propose GLaRE, a novel Graph-based Landmark Region Embedding network for emotion recognition. Facial landmarks are extracted using 3D facial alignment, and a quotient graph is constructed via hierarchical coarsening to preserve spatial structure while reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet and 94.24 percentage on FERG, outperforming several existing baselines. Additionally, ablation studies have demonstrated that region-level embeddings from quotient graphs have contributed to improved prediction performance.",
    "score": 2,
    "reason": "该论文聚焦于面部表情识别（FER），使用图神经网络建模面部关键点之间的关系，与文档图像理解（DIU）在任务目标、输入模态（人脸图像 vs. 文档图像）和应用场景上均无直接关联。尽管其采用图结构建模空间关系的技术思想可能对DIU中的布局分析有一定启发，但其核心方法面向的是情感识别而非信息抽取或文档语义理解，且未涉及OCR、实体识别、关系抽取等DIU核心环节。此外，论文未提及多模态模型、推理增强或agent相关技术，无法直接迁移至DIU领域。因此不值得阅读。",
    "summary": "本文提出GLaRE，一种基于图的地标区域嵌入网络用于情绪识别。通过3D面部对齐提取关键点，并构建分层粗化商图以保留空间结构并降低复杂度，实现对面部表情的高效建模。在AffectNet和FERG数据集上取得优于基线的表现，且区域级嵌入有助于提升预测性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19320v2",
    "updated": "2025-08-28T09:15:43Z",
    "published": "2025-08-26T14:00:16Z",
    "title": "MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation",
    "authors": [
      "Ming Chen",
      "Liyuan Cui",
      "Wenyuan Zhang",
      "Haoxian Zhang",
      "Yan Zhou",
      "Xiaohan Li",
      "Songlin Tang",
      "Jiwen Liu",
      "Borui Liao",
      "Hejia Chen",
      "Xiaoqiang Liu",
      "Pengfei Wan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19320v2.pdf",
    "comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with heavy computational cost and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.",
    "score": 3,
    "reason": "该论文聚焦于多模态数字人视频生成，虽涉及多模态输入与LLM结合，但核心任务是实时自回归视频生成，与文档图像理解（DIU）无直接关联。其技术如多模态条件编码和扩散模型虽具参考价值，但未针对文本布局、实体识别或关系抽取等DIU核心问题，且未开源或发表于CCF A类会议。虽然使用了LLM进行控制，但属于生成式内容创作领域，难以直接迁移至DIU任务中。",
    "summary": "MIDAS提出一种基于自回归扩散模型的多模态交互数字人视频生成框架，支持音频、姿态和文本输入，通过轻量级压缩编码器实现低延迟推理。构建了约20,000小时的多源对话数据集用于训练，适用于双工对话与跨语言合成，但主要面向虚拟人生成而非文档理解。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20570v1",
    "updated": "2025-08-28T09:08:30Z",
    "published": "2025-08-28T09:08:30Z",
    "title": "Towards Mechanistic Defenses Against Typographic Attacks in CLIP",
    "authors": [
      "Lorenz Hufe",
      "Constantin Venhoff",
      "Maximilian Dreyer",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20570v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, our method improves performance by up to 19.6% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.",
    "score": 4,
    "reason": "该论文研究CLIP模型在字体攻击下的防御机制，虽涉及视觉-语言模型（VLM）的安全性，但核心关注点是防御文本注入攻击，属于VLM安全性的子方向。其方法基于注意力头的消融，与DIU任务中的文档理解无直接关联。尽管CLIP与多模态模型相关，但本文未涉及文档图像理解、实体识别、关系抽取或agent架构等关键DIU环节。此外，论文未开源，也未发表于CCF A类会议，对DIU领域技术演进的直接贡献有限。",
    "summary": "本文分析CLIP视觉编码器在typographic攻击下的行为，发现后半层注意力头会提取并传递字体信息，进而提出一种无需微调的防御方法，通过选择性移除这些注意力头来提升模型鲁棒性。实验显示在ImageNet-100的变体上性能提升达19.6%，且标准准确率损失低于1%。作者发布了更抗攻击的dyslexic CLIP模型，适用于高安全性场景。"
  },
  {
    "id": "http://arxiv.org/abs/2503.06989v4",
    "updated": "2025-08-28T08:43:31Z",
    "published": "2025-03-10T07:10:38Z",
    "title": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application",
    "authors": [
      "Wenzhuo Xu",
      "Zhipeng Wei",
      "Xiongtao Sun",
      "Zonghao Ying",
      "Deyue Zhang",
      "Dongdong Yang",
      "Xiangzheng Zhang",
      "Quanchen Zou"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.06989v4.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their superior ability in understanding multimodal content. However, they remain vulnerable to jailbreak attacks, which exploit weaknesses in their safety alignment to generate harmful responses. Previous studies categorize jailbreaks as successful or failed based on whether responses contain malicious content. However, given the stochastic nature of MLLM responses, this binary classification of an input's ability to jailbreak MLLMs is inappropriate. Derived from this viewpoint, we introduce jailbreak probability to quantify the jailbreak potential of an input, which represents the likelihood that MLLMs generated a malicious response when prompted with this input. We approximate this probability through multiple queries to MLLMs. After modeling the relationship between input hidden states and their corresponding jailbreak probability using Jailbreak Probability Prediction Network (JPPN), we use continuous jailbreak probability for optimization. Specifically, we propose Jailbreak-Probability-based Attack (JPA) that optimizes adversarial perturbations on input image to maximize jailbreak probability, and further enhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To counteract attacks, we also propose Jailbreak-Probability-based Finetuning (JPF), which minimizes jailbreak probability through MLLM parameter updates. Extensive experiments show that (1) (M)JPA yields significant improvements when attacking a wide range of models under both white and black box settings. (2) JPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results demonstrate the significance of introducing jailbreak probability to make nuanced distinctions among input jailbreak abilities.",
    "score": 3,
    "reason": "该论文聚焦于多模态大模型（MLLM）的越狱攻击概率建模，虽涉及VLM领域且与安全相关，但其核心是攻击与防御机制研究，而非文档图像理解（DIU）任务本身。尽管提及了MLLM，但未直接解决OCR、实体识别或关系抽取等DIU关键环节，也未提出可迁移至DIU的推理增强或agent框架。此外，无开源信息，非CCF A类会议/期刊，且其技术路径为对抗性攻击，难以直接应用于DIU系统的性能提升或结构设计，因此关联度较低。",
    "summary": "本文提出一种基于概率的越狱攻击建模方法，通过定义输入的越狱概率来量化攻击潜力，并设计JPA和MJPA攻击方法及JPF防御策略。实验表明该方法能有效提升攻击成功率并大幅降低越狱率，但主要关注MLLM的安全性问题，与DIU任务无直接应用价值。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20551v1",
    "updated": "2025-08-28T08:43:03Z",
    "published": "2025-08-28T08:43:03Z",
    "title": "Contrastive Learning through Auxiliary Branch for Video Object Detection",
    "authors": [
      "Lucas Rakotoarivony"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20551v1.pdf",
    "comment": "Accepted paper for ACIVS 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Video object detection is a challenging task because videos often suffer from image deterioration such as motion blur, occlusion, and deformable shapes, making it significantly more difficult than detecting objects in still images. Prior approaches have improved video object detection performance by employing feature aggregation and complex post-processing techniques, though at the cost of increased computational demands. To improve robustness to image degradation without additional computational load during inference, we introduce a straightforward yet effective Contrastive Learning through Auxiliary Branch (CLAB) method. First, we implement a constrastive auxiliary branch using a contrastive loss to enhance the feature representation capability of the video object detector's backbone. Next, we propose a dynamic loss weighting strategy that emphasizes auxiliary feature learning early in training while gradually prioritizing the detection task as training converges. We validate our approach through comprehensive experiments and ablation studies, demonstrating consistent performance gains. Without bells and whistles, CLAB reaches a performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101, respectively, on the ImageNet VID dataset, thus achieving state-of-the-art performance for CNN-based models without requiring additional post-processing methods.",
    "score": 2,
    "reason": "该论文聚焦于视频目标检测中的对比学习方法，虽在计算机视觉领域具有创新性，但其核心任务与文档图像理解（DIU）无直接关联。研究内容涉及视频序列中的运动模糊、遮挡等退化问题，而DIU关注的是静态文档图像的文本识别、实体抽取与关系建模。此外，论文未涉及多模态模型、推理增强技术（如CoT、ToT）、agent系统或可迁移至DIU的架构设计，也未提及开源或发表于CCF A类会议。因此，不值得在DIU研究中优先阅读。",
    "summary": "本文提出一种基于辅助分支的对比学习方法（CLAB），用于提升视频目标检测在图像退化情况下的鲁棒性。通过引入对比损失优化骨干网络特征表示，并采用动态损失权重策略平衡辅助任务与检测任务的学习。在ImageNet VID数据集上取得SOTA性能，且无需额外后处理，但仅适用于视频场景，与文档图像理解无关。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20547v1",
    "updated": "2025-08-28T08:38:50Z",
    "published": "2025-08-28T08:38:50Z",
    "title": "SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes",
    "authors": [
      "Yunpeng Mei",
      "Hongjie Cao",
      "Yinqiu Xia",
      "Wei Xiao",
      "Zhaohan Feng",
      "Gang Wang",
      "Jie Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20547v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Real-time interactive grasp synthesis for dynamic objects remains challenging as existing methods fail to achieve low-latency inference while maintaining promptability. To bridge this gap, we propose SPGrasp (spatiotemporal prompt-driven dynamic grasp synthesis), a novel framework extending segment anything model v2 (SAMv2) for video stream grasp estimation. Our core innovation integrates user prompts with spatiotemporal context, enabling real-time interaction with end-to-end latency as low as 59 ms while ensuring temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on Jacquard. On the challenging GraspNet-1Billion dataset under continuous tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency, representing a 58.5% reduction compared to the prior state-of-the-art promptable method RoG-SAM while maintaining competitive accuracy. Real-world experiments involving 13 moving objects demonstrate a 94.8% success rate in interactive grasping scenarios. These results confirm SPGrasp effectively resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code is available at https://github.com/sejmoonwei/SPGrasp.",
    "score": 2,
    "reason": "该论文聚焦于动态场景下的抓取合成（grasp synthesis），属于机器人视觉与操作领域，虽然使用了SAMv2并涉及时空上下文建模，但其核心任务是物理抓取规划，与文档图像理解（DIU）在目标、数据模态和应用层面均无直接关联。尽管提及prompting和实时性，但其技术路径（如基于视频流的抓取估计）无法直接迁移至DIU任务中。此外，未涉及OCR、实体识别、关系抽取或多模态文档理解，也未使用VLM/LLM进行推理增强或agent架构设计，因此不适用于DIU研究。",
    "summary": "SPGrasp提出一种基于时空提示驱动的动态场景抓取合成框架，扩展SAMv2用于视频流中的实时抓取估计。通过融合用户提示与时空上下文，在保证时间一致性的同时实现低延迟（59ms）的端到端推理。在OCID、Jacquard和GraspNet-1Billion等数据集上表现优异，实测成功率达94.8%。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2507.20198v4",
    "updated": "2025-08-28T08:29:29Z",
    "published": "2025-07-27T09:33:56Z",
    "title": "When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios",
    "authors": [
      "Kele Shao",
      "Keda Tao",
      "Kejia Zhang",
      "Sicheng Feng",
      "Mu Cai",
      "Yuzhang Shang",
      "Haoxuan You",
      "Can Qin",
      "Yang Sui",
      "Huan Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.20198v4.pdf",
    "comment": "For ongoing updates and to track the latest advances in this promising area, we maintain a public repository: https://github.com/cokeshao/Awesome-Multimodal-Token-Compression",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Multimodal large language models (MLLMs) have made remarkable strides, largely driven by their ability to process increasingly long and complex contexts, such as high-resolution images, extended video sequences, and lengthy audio input. While this ability significantly enhances MLLM capabilities, it introduces substantial computational challenges, primarily due to the quadratic complexity of self-attention mechanisms with numerous input tokens. To mitigate these bottlenecks, token compression has emerged as an auspicious and critical approach, efficiently reducing the number of tokens during both training and inference. In this paper, we present the first systematic survey and synthesis of the burgeoning field of multimodal long context token compression. Recognizing that effective compression strategies are deeply tied to the unique characteristics and redundancies of each modality, we categorize existing approaches by their primary data focus, enabling researchers to quickly access and learn methods tailored to their specific area of interest: (1) image-centric compression, which addresses spatial redundancy in visual data; (2) video-centric compression, which tackles spatio-temporal redundancy in dynamic sequences; and (3) audio-centric compression, which handles temporal and spectral redundancy in acoustic signals. Beyond this modality-driven categorization, we further dissect methods based on their underlying mechanisms, including transformation-based, similarity-based, attention-based, and query-based approaches. By providing a comprehensive and structured overview, this survey aims to consolidate current progress, identify key challenges, and inspire future research directions in this rapidly evolving domain. We also maintain a public repository to continuously track and update the latest advances in this promising area.",
    "score": 8,
    "reason": "该论文是多模态长上下文令牌压缩的系统性综述，直接关联DIU中处理高分辨率文档图像时面临的计算瓶颈问题。文档图像通常包含大量冗余像素信息，而本文对图像中心压缩方法的深入分类（如基于变换、相似性、注意力等）可直接应用于优化DIU中OCR后文本与布局信息的融合效率。尤其在VLM用于DIU时，token压缩能显著降低推理成本并提升处理长文档的能力，具有高度可迁移性。此外，论文维护了开源仓库，持续更新最新进展，具备实用价值。虽非直接解决DIU任务，但其技术路径与DIU中的高效视觉-语言建模需求高度契合，值得重点关注。",
    "summary": "本文首次系统综述了跨图像、视频和音频的多模态长上下文令牌压缩技术，按数据类型分为图像、视频和音频三类，并进一步从变换、相似性、注意力和查询等机制角度剖析压缩方法。该工作为高效处理高分辨率文档图像提供了理论框架与技术参考，对提升DIU中VLM的推理效率具有直接指导意义。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20537v1",
    "updated": "2025-08-28T08:27:25Z",
    "published": "2025-08-28T08:27:25Z",
    "title": "Domain Adaptation Techniques for Natural and Medical Image Classification",
    "authors": [
      "Ahmad Chaddad",
      "Yihang Wu",
      "Reem Kateb",
      "Christian Desrosiers"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20537v1.pdf",
    "comment": "Accepted in Information Sciences",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Domain adaptation (DA) techniques have the potential in machine learning to alleviate distribution differences between training and test sets by leveraging information from source domains. In image classification, most advances in DA have been made using natural images rather than medical data, which are harder to work with. Moreover, even for natural images, the use of mainstream datasets can lead to performance bias. {With the aim of better understanding the benefits of DA for both natural and medical images, this study performs 557 simulation studies using seven widely-used DA techniques for image classification in five natural and eight medical datasets that cover various scenarios, such as out-of-distribution, dynamic data streams, and limited training samples.} Our experiments yield detailed results and insightful observations highlighting the performance and medical applicability of these techniques. Notably, our results have shown the outstanding performance of the Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved feasible classification accuracy (91.2\\%) in the COVID-19 dataset using Resnet50 and showed an important accuracy improvement in the dynamic data stream DA scenario (+6.7\\%) compared to the baseline. Our results also demonstrate that DSAN exhibits remarkable level of explainability when evaluated on COVID-19 and skin cancer datasets. These results contribute to the understanding of DA techniques and offer valuable insight into the effective adaptation of models to medical data.",
    "score": 3,
    "reason": "该论文研究的是自然图像与医学图像分类中的领域自适应（DA）技术，虽涉及图像分类和医学数据，但其核心任务是图像分类而非文档图像理解（DIU）。所用数据集为自然图像和医学图像（如COVID-19、皮肤癌），与文档图像无关。尽管DSAN算法表现良好，但其应用场景与DIU无直接关联，且未涉及OCR、实体识别、关系抽取或多模态模型在DIU中的应用。此外，论文未提及VLM、LLM推理增强或agent相关技术，因此无法直接迁移应用于DIU领域。",
    "summary": "本研究通过557次模拟实验评估了七种主流领域自适应方法在五种自然图像和八种医学图像数据集上的性能，重点分析了分布外、动态数据流和小样本场景下的表现。结果显示Deep Subdomain Adaptation Network（DSAN）在COVID-19和皮肤癌数据集上表现优异，准确率达91.2%，并具有较强可解释性。论文贡献在于对DA技术在医学图像中的适用性分析，但不涉及文档图像理解、视觉信息抽取或多模态模型在DIU中的应用。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20534v1",
    "updated": "2025-08-28T08:21:10Z",
    "published": "2025-08-28T08:21:10Z",
    "title": "Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset",
    "authors": [
      "Frederik Rajiv Manichand",
      "Robin Deuber",
      "Robert Jakob",
      "Steve Swerling",
      "Jamie Rosen",
      "Elgar Fleisch",
      "Patrick Langer"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20534v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Estimating Body Mass Index (BMI) from camera images with machine learning models enables rapid weight assessment when traditional methods are unavailable or impractical, such as in telehealth or emergency scenarios. Existing computer vision approaches have been limited to datasets of up to 14,500 images. In this study, we present a deep learning-based BMI estimation method trained on our WayBED dataset, a large proprietary collection of 84,963 smartphone images from 25,353 individuals. We introduce an automatic filtering method that uses posture clustering and person detection to curate the dataset by removing low-quality images, such as those with atypical postures or incomplete views. This process retained 71,322 high-quality images suitable for training. We achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test set (WayBED data) using full-body images, the lowest value in the published literature to the best of our knowledge. Further, we achieve a MAPE of 13% on the completely unseen~(during training) VisualBodyToBMI dataset, comparable with state-of-the-art approaches trained on it, demonstrating robust generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the full pipeline, including image filtering and BMI estimation, on Android devices using the CLAID framework. We release our complete code for model training, filtering, and the CLAID package for mobile deployment as open-source contributions.",
    "score": 2,
    "reason": "该论文聚焦于通过手机摄像头图像进行BMI估算，属于计算机视觉在医疗健康领域的应用，与文档图像理解（DIU）无直接关联。虽然涉及图像处理和深度学习模型部署，但其任务目标、数据类型（人体图像）和应用场景均与DIU无关。尽管模型在移动端部署并开源，但未涉及OCR、实体识别、关系抽取或多模态文档理解等核心DIU技术。此外，未使用VLM、LLM推理增强或agent架构，也未提出新数据集用于DIU领域。因此不相关。",
    "summary": "本文提出一种基于深度学习的BMI估算方法，利用包含84,963张真实世界智能手机图像的大规模数据集WayBED进行训练，并通过自动姿态聚类与人体检测过滤低质量图像，最终在自建测试集上实现7.9%的MAPE，优于现有文献。模型已通过CLAID框架部署至Android设备，并开源了完整代码。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20530v1",
    "updated": "2025-08-28T08:15:23Z",
    "published": "2025-08-28T08:15:23Z",
    "title": "Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection",
    "authors": [
      "Mingqian Ji",
      "Jian Yang",
      "Shanshan Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20530v1.pdf",
    "comment": "Accepted by ACM MM 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4$\\%$ mAP on the nuScenes validation benchmark.",
    "score": 2,
    "reason": "该论文聚焦于激光雷达与摄像头融合的无监督3D目标检测，属于自动驾驶感知领域，核心方法为数据级融合提升伪框质量。虽然涉及多模态融合思想，但应用场景（3D场景理解）与文档图像理解（DIU）差异巨大，且任务本质（几何结构检测）与文本/布局/实体关系抽取无直接关联。尽管采用视觉基础模型和动态自进化策略，但其技术路径（点云-图像投影、深度估计滤波）无法直接迁移至DIU任务中。此外，未涉及OCR、实体识别或关系抽取等DIU核心模块，也未使用VLM或LLM进行推理增强。因此，虽在多模态融合上具一定前沿性，但与DIU领域无直接应用价值。",
    "summary": "本文提出一种基于数据级融合的无监督3D目标检测框架，通过将RGB图像与LiDAR点云在早期阶段融合，利用视觉基础模型进行实例分割和深度估计，并设计双向融合机制与局部-全局滤波策略来提升伪框质量。进一步引入动态自进化策略迭代优化伪框，显著提升检测性能，在nuScenes数据集上mAP达到28.4%，优于现有SOTA方法。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20528v1",
    "updated": "2025-08-28T08:14:55Z",
    "published": "2025-08-28T08:14:55Z",
    "title": "Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation",
    "authors": [
      "Jingyun Yang",
      "Guoqing Zhang",
      "Jingge Wang",
      "Yang Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20528v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate gross tumor volume segmentation on multi-modal medical data is critical for radiotherapy planning in nasopharyngeal carcinoma and glioblastoma. Recent advances in deep neural networks have brought promising results in medical image segmentation, leading to an increasing demand for labeled data. Since labeling medical images is time-consuming and labor-intensive, active learning has emerged as a solution to reduce annotation costs by selecting the most informative samples to label and adapting high-performance models with as few labeled samples as possible. Previous active domain adaptation (ADA) methods seek to minimize sample redundancy by selecting samples that are farthest from the source domain. However, such one-off selection can easily cause negative transfer, and access to source medical data is often limited. Moreover, the query strategy for multi-modal medical data remains unexplored. In this work, we propose an active and sequential domain adaptation framework for dynamic multi-modal sample selection in ADA. We derive a query strategy to prioritize labeling and training on the most valuable samples based on their informativeness and representativeness. Empirical validation on diverse gross tumor volume segmentation tasks demonstrates that our method achieves favorable segmentation performance, significantly outperforming state-of-the-art ADA methods. Code is available at the git repository: \\href{https://github.com/Hiyoochan/mmActS}{mmActS}.",
    "score": 2,
    "reason": "该论文聚焦于多模态医学图像分割中的主动域适应，研究对象为肿瘤体积分割，属于医学影像分析领域。虽然涉及多模态数据和主动学习，但其核心任务与文档图像理解（DIU）无直接关联，应用场景、数据类型和目标均不一致。论文未涉及OCR、实体识别、关系抽取或视觉-语言模型在文档理解中的应用，也未提及推理增强、agent系统或inference scaling技术。尽管代码开源值得肯定，但内容与DIU、VLM、LLM推理优化及agent等目标领域无紧密可迁移性，故不推荐阅读。",
    "summary": "本文提出一种用于多模态医学图像中肿瘤体积分割的主动且序列化的域适应框架，通过设计基于信息量和代表性优先级的样本选择策略，减少标注成本并提升分割性能。实验表明该方法优于现有SOTA方法，代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20526v1",
    "updated": "2025-08-28T08:08:54Z",
    "published": "2025-08-28T08:08:54Z",
    "title": "Adam SLAM - the last mile of camera calibration with 3DGS",
    "authors": [
      "Matthieu Gendrin",
      "Stéphane Pateux",
      "Xiaoran Jiang",
      "Théo Ladune",
      "Luce Morin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20526v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The quality of the camera calibration is of major importance for evaluating progresses in novel view synthesis, as a 1-pixel error on the calibration has a significant impact on the reconstruction quality. While there is no ground truth for real scenes, the quality of the calibration is assessed by the quality of the novel view synthesis. This paper proposes to use a 3DGS model to fine tune calibration by backpropagation of novel view color loss with respect to the cameras parameters. The new calibration alone brings an average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine tuning may be long and its suitability depends on the criticity of training time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake of novel view quality is the most important.",
    "score": 2,
    "reason": "该论文聚焦于3DGS框架下的相机标定优化，通过反向传播颜色损失微调相机参数，属于计算机视觉中新型视图合成与几何重建方向。虽然涉及视觉建模和优化技术，但其核心任务是相机标定而非文档图像理解（DIU），与OCR、实体识别、关系抽取等DIU关键步骤无直接关联。同时，论文未涉及多模态模型（VLM）、大语言模型（LLM）推理增强（如CoT、ToT）、或智能体（agent）系统设计，也未提出新数据集或可迁移至DIU的通用方法。因此，尽管技术上具有创新性，但与DIU领域及所关注的VLM/LLM推理增强、agent等前沿方向无直接应用价值。",
    "summary": "本文提出一种基于3DGS的相机标定微调方法，利用新视角合成的颜色损失对相机参数进行反向传播优化，显著提升重建质量（平均PSNR提升0.4 dB）。该方法适用于高精度参考场景（如Mip-NeRF 360），但不涉及文档理解、多模态推理或智能体系统，与DIU领域无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2502.01401v4",
    "updated": "2025-08-28T07:57:55Z",
    "published": "2025-02-03T14:32:36Z",
    "title": "Language-to-Space Programming for Training-Free 3D Visual Grounding",
    "authors": [
      "Boyu Mi",
      "Hanqing Wang",
      "Tai Wang",
      "Yilun Chen",
      "Jiangmiao Pang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2502.01401v4.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "3D visual grounding (3DVG) is challenging due to the need to understand 3D spatial relations. While supervised approaches have achieved superior performance, they are constrained by the scarcity and high annotation costs of 3D vision-language datasets. Training-free approaches based on LLMs/VLMs eliminate the need for large-scale training data, but they either incur prohibitive grounding time and token costs or have unsatisfactory accuracy. To address the challenges, we introduce a novel method for training-free 3D visual grounding, namely Language-to-Space Programming (LaSP). LaSP introduces LLM-generated codes to analyze 3D spatial relations among objects, along with a pipeline that evaluates and optimizes the codes automatically. Experimental results demonstrate that LaSP achieves 52.9% accuracy on the Nr3D benchmark, ranking among the best training-free methods. Moreover, it substantially reduces the grounding time and token costs, offering a balanced trade-off between performance and efficiency.",
    "score": 3,
    "reason": "该论文聚焦于训练-free 3D视觉定位（3DVG），虽涉及语言与空间关系的理解，但研究对象为3D场景而非文档图像。其核心方法依赖LLM生成代码进行空间关系分析，虽与推理相关，但应用场景（3D空间理解）与DIU（2D文档结构与语义理解）差异显著，难以直接迁移至DIU任务。此外，未提及文档、布局、表格或视觉信息抽取等关键DIU要素，也无开源或顶会信息支持，故相关性较低。",
    "summary": "本文提出Language-to-Space Programming (LaSP)，一种无需训练的3D视觉定位方法，通过LLM生成代码来分析3D场景中物体之间的空间关系，并设计自动评估与优化流程。在Nr3D数据集上达到52.9%准确率，显著降低推理时间和token消耗，是当前训练-free 3DVG中的先进方法，但应用场景为3D环境而非文档图像理解。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20516v1",
    "updated": "2025-08-28T07:57:54Z",
    "published": "2025-08-28T07:57:54Z",
    "title": "DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample",
    "authors": [
      "Wenting Yin",
      "Han Sun",
      "Xinru Meng",
      "Ningzhong Liu",
      "Huiyu Zhou"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20516v1.pdf",
    "comment": "13 pages, accepted by PRCV2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Continual test-time adaptation aims to continuously adapt a pre-trained model to a stream of target domain data without accessing source data. Without access to source domain data, the model focuses solely on the feature characteristics of the target data. Relying exclusively on these features can lead to confusion and introduce learning biases. Currently, many existing methods generate pseudo-labels via model predictions. However, the quality of pseudo-labels cannot be guaranteed and the problem of error accumulation must be solved. To address these challenges, we propose DCFS, a novel CTTA framework that introduces dual-path feature consistency and confidence-aware sample learning. This framework disentangles the whole feature representation of the target data into semantic-related feature and domain-related feature using dual classifiers to learn distinct feature representations. By maintaining consistency between the sub-features and the whole feature, the model can comprehensively capture data features from multiple perspectives. Additionally, to ensure that the whole feature information of the target domain samples is not overlooked, we set a adaptive threshold and calculate a confidence score for each sample to carry out loss weighted self-supervised learning, effectively reducing the noise of pseudo-labels and alleviating the problem of error accumulation. The efficacy of our proposed method is validated through extensive experimentation across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C, demonstrating consistent performance in continual test-time adaptation scenarios.",
    "score": 3,
    "reason": "该论文提出一种持续测试时自适应方法（DCFS），聚焦于特征与样本的一致性，用于提升模型在目标域数据流中的泛化能力。虽然其技术思路涉及特征解耦和伪标签优化，对DIU中面对动态文档分布的鲁棒性有一定启发意义，但核心场景为通用图像分类任务（CIFAR、ImageNet-C等），未涉及文档理解、OCR、实体识别或关系抽取等关键环节。此外，论文未提及多模态、视觉语言模型、推理增强或Agent系统，也未开源。虽被PRCV2025接收，但非CCF A类会议，且与DIU领域无直接应用潜力，仅具间接参考价值。",
    "summary": "本文提出DCFS框架，通过双路径特征一致性与置信度感知的样本学习，在无源数据访问条件下实现持续测试时自适应。利用双分类器解耦语义与领域相关特征，并通过自监督损失加权减少伪标签噪声，有效缓解误差累积问题。在CIFAR10-C、CIFAR100-C和ImageNet-C上验证了方法的有效性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20505v1",
    "updated": "2025-08-28T07:45:08Z",
    "published": "2025-08-28T07:45:08Z",
    "title": "Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent",
    "authors": [
      "En Ci",
      "Shanyan Guan",
      "Yanhao Ge",
      "Yilin Zhang",
      "Wei Li",
      "Zhenyu Zhang",
      "Jian Yang",
      "Ying Tai"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20505v1.pdf",
    "comment": "Accepted by ICCV 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.",
    "score": 3,
    "reason": "该论文聚焦于文本到图像生成中的语义图像编辑，核心是通过描述性提示实现图像编辑，属于文本到图像生成（T2I）领域的创新。虽然涉及自然语言意图理解与视觉内容交互，但其目标是修改现有图像而非解析文档信息，与DIU任务在目标、输入输出形式和应用场景上均无直接关联。尽管使用了跨注意力机制等技术，但其方法论不适用于文档结构理解、实体识别或关系抽取。此外，未提及多模态模型在文档理解中的应用，也未涉及推理增强或agent系统设计。因此，虽为CCF A类会议接收，但与DIU、VLM推理强化及agent方向的直接关联性极弱，仅具间接启发价值。",
    "summary": "本文提出DescriptiveEdit框架，将指令式图像编辑重构为基于参考图像的文本到图像生成任务，通过引入交叉注意力UNet将参考图像特征注入生成过程，提升编辑准确性和一致性。该方法无需图像重建或架构修改，兼容ControlNet等扩展模块，实验表明在Emu Edit基准上性能优越。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20492v1",
    "updated": "2025-08-28T07:19:07Z",
    "published": "2025-08-28T07:19:07Z",
    "title": "IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection",
    "authors": [
      "Xuanming Cao",
      "Chengyu Tao",
      "Yifeng Cheng",
      "Juan Du"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20492v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce an novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet achieves a new state-of-the-art with a markedly lower false positive rate, underscoring its practical value for industrial deployment.",
    "score": 2,
    "reason": "该论文聚焦于3D点云的异常检测，属于工业视觉质检领域，与文档图像理解（DIU）无直接关联。尽管其提出的多模态融合思想有一定启发性，但应用场景（3D几何结构异常检测）与DIU（文本+布局+语义理解）差异显著，且未涉及OCR、实体识别、关系抽取或VLM/LLM推理技术。此外，未提及与多模态模型、inference scaling或agent系统的关联，无法直接迁移至DIU任务。因此不值得阅读。",
    "summary": "IAENet提出一种基于重要性感知的集成框架，结合2D预训练专家与3D专家模型进行3D点云异常检测。通过动态评估各模态贡献并重加权异常分数，提升检测精度，尤其在降低误报率方面表现优异。实验在MVTec 3D-AD数据集上验证了其SOTA性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20491v1",
    "updated": "2025-08-28T07:16:01Z",
    "published": "2025-08-28T07:16:01Z",
    "title": "CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information",
    "authors": [
      "Seunghyeon Jung",
      "Seoyoung Hong",
      "Jiwoo Jeong",
      "Seungwon Jeong",
      "Jaerim Choi",
      "Hoki Kim",
      "Woojin Lee"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20491v1.pdf",
    "comment": "12 pages with supplementary material",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in deep learning have led to more studies to enhance golfers' shot precision. However, these existing studies have not quantitatively established the relationship between swing posture and ball trajectory, limiting their ability to provide golfers with the necessary insights for swing improvement. In this paper, we propose a new dataset called CaddieSet, which includes joint information and various ball information from a single shot. CaddieSet extracts joint information from a single swing video by segmenting it into eight swing phases using a computer vision-based approach. Furthermore, based on expert golf domain knowledge, we define 15 key metrics that influence a golf swing, enabling the interpretation of swing outcomes through swing-related features. Through experiments, we demonstrated the feasibility of CaddieSet for predicting ball trajectories using various benchmarks. In particular, we focus on interpretable models among several benchmarks and verify that swing feedback using our joint features is quantitatively consistent with established domain knowledge. This work is expected to offer new insight into golf swing analysis for both academia and the sports industry.",
    "score": 2,
    "reason": "该论文聚焦于高尔夫挥杆动作分析，涉及人体关节特征与球轨迹的关系建模，属于计算机视觉在体育运动领域的应用。虽然使用了计算机视觉技术，但其研究对象（高尔夫挥杆）与文档图像理解（DIU）无直接关联，且未涉及OCR、实体识别、关系抽取、多模态模型、推理增强或智能体等核心DIU相关技术。尽管数据集构建有一定价值，但无法直接迁移至DIU任务中，因此不值得阅读。",
    "summary": "CaddieSet是一个用于高尔夫挥杆分析的新数据集，通过视频分割提取八阶段挥杆动作的人体关节信息，并结合专家定义的15个关键指标来预测球轨迹。实验验证了其在可解释性建模方面的有效性，为体育运动分析提供新工具，但与文档图像理解领域无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.13647v2",
    "updated": "2025-08-28T07:15:28Z",
    "published": "2025-08-19T08:57:32Z",
    "title": "Model-based Multi-object Visual Tracking: Identification and Standard Model Limitations",
    "authors": [
      "Jan Krejčí",
      "Oliver Kost",
      "Yuxuan Xia",
      "Lennart Svensson",
      "Ondřej Straka"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.13647v2.pdf",
    "comment": "Accepted for publication in 2025 28th International Conference on Information Fusion (FUSION)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This paper uses multi-object tracking methods known from the radar tracking community to address the problem of pedestrian tracking using 2D bounding box detections. The standard point-object (SPO) model is adopted, and the posterior density is computed using the Poisson multi-Bernoulli mixture (PMBM) filter. The selection of the model parameters rooted in continuous time is discussed, including the birth and survival probabilities. Some parameters are selected from the first principles, while others are identified from the data, which is, in this case, the publicly available MOT-17 dataset. Although the resulting PMBM algorithm yields promising results, a mismatch between the SPO model and the data is revealed. The model-based approach assumes that modifying the problematic components causing the SPO model-data mismatch will lead to better model-based algorithms in future developments.",
    "score": 2,
    "reason": "该论文研究的是基于模型的多目标视觉跟踪，聚焦于行人跟踪与雷达追踪中的点对象模型（SPO）与PMBM滤波器的应用。其核心方法和问题设定（如连续时间建模、出生/生存概率估计）与文档图像理解（DIU）无直接关联，且未涉及OCR、实体识别、关系抽取或布局理解等关键任务。虽然使用了2D边界框检测，但应用场景为动态视频中的行人跟踪，而非静态文档图像分析。此外，论文未提及多模态模型、LLM推理增强或agent架构，也未开源。尽管被FUSION会议接受（CCF B类），但技术路径与DIU领域无关，无法直接迁移应用。",
    "summary": "本文采用雷达跟踪中常用的点对象模型（SPO）与Poisson多伯努利混合（PMBM）滤波器，解决基于2D边界框检测的多行人跟踪问题。通过从第一性原理和MOT-17数据中联合识别模型参数，发现SPO模型与真实数据之间存在不匹配，并指出改进该模型是未来方向。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20488v1",
    "updated": "2025-08-28T07:09:21Z",
    "published": "2025-08-28T07:09:21Z",
    "title": "Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts",
    "authors": [
      "Zixuan Hu",
      "Dongxiao Li",
      "Xinzhu Ma",
      "Shixiang Tang",
      "Xiaotong Li",
      "Wenhan Yang",
      "Ling-Yu Duan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20488v1.pdf",
    "comment": "Accepted by ICCV 2025 (Highlight)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical applications like autonomous driving, yet its reliability deteriorates significantly under real-world domain shifts caused by environmental or sensor variations. To address these shifts, Test-Time Adaptation (TTA) methods have emerged, enabling models to adapt to target distributions during inference. While prior TTA approaches recognize the positive correlation between low uncertainty and high generalization ability, they fail to address the dual uncertainty inherent to M3OD: semantic uncertainty (ambiguous class predictions) and geometric uncertainty (unstable spatial localization). To bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA framework designed to jointly minimize both uncertainties for robust M3OD. Through a convex optimization lens, we introduce an innovative convex structure of the focal loss and further derive a novel unsupervised version, enabling label-agnostic uncertainty weighting and balanced learning for high-uncertainty objects. In parallel, we design a semantic-aware normal field constraint that preserves geometric coherence in regions with clear semantic cues, reducing uncertainty from the unstable 3D representation. This dual-branch mechanism forms a complementary loop: enhanced spatial perception improves semantic classification, and robust semantic predictions further refine spatial understanding. Extensive experiments demonstrate the superiority of DUO over existing methods across various datasets and domain shift types.",
    "score": 2,
    "reason": "该论文聚焦于单目3D目标检测中的测试时适应（TTA），提出一种双不确定性优化框架以应对环境变化带来的分布偏移。虽然涉及测试时推理优化（与inference scaling相关），但其应用场景为自动驾驶中的3D感知，与文档图像理解（DIU）无直接关联。所提方法针对几何与语义不确定性，属于三维视觉任务，无法直接迁移至文档场景下的文本、布局、关系抽取等核心问题。尽管被ICCV 2025接收（顶会加分），但领域不匹配，且未涉及多模态模型、LLM、agent或文档结构理解，因此不值得阅读。",
    "summary": "本文提出Dual Uncertainty Optimization (DUO)，一种用于单目3D目标检测的测试时自适应框架，通过联合优化语义不确定性和几何不确定性来提升模型在真实世界分布偏移下的鲁棒性。引入凸优化视角改进焦点损失，并设计语义感知的法向场约束以增强空间一致性。实验表明该方法在多种域偏移下优于现有技术。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19852v2",
    "updated": "2025-08-28T07:08:03Z",
    "published": "2025-08-27T13:09:55Z",
    "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
    "authors": [
      "Binjie Zhang",
      "Mike Zheng Shou"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19852v2.pdf",
    "comment": "Code: github.com/showlab/Ego-PM",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis.",
    "score": 3,
    "reason": "该论文聚焦于以手部轨迹为条件的自我中心场景下动作与视觉未来预测，虽涉及多模态（视觉、语言、动作）和未来视频生成，但核心任务是动作-视觉因果建模，主要用于机器人规划与人类行为理解。其方法基于扩散模型进行帧级视频生成，与文档图像理解（DIU）在任务目标、输入结构（非文档图像）、语义理解需求上无直接关联。尽管使用了多模态融合与推理思想，但未涉及OCR、实体识别、关系抽取或文档布局分析等DIU核心组件，且不适用于文本密集型文档理解。因此，无法直接应用于DIU领域，相关性较低。",
    "summary": "本文提出一种两阶段统一框架，基于手部轨迹条件预测自我中心场景中的未来动作及其视觉结果。第一阶段通过状态建模预测未来手部轨迹，第二阶段利用因果交叉注意力融合多模态信息，驱动潜空间扩散模型生成未来视频帧。在Ego4D、BridgeData和RLBench数据集上验证了其在动作预测与视频合成上的优越性能。"
  },
  {
    "id": "http://arxiv.org/abs/2507.10943v2",
    "updated": "2025-08-28T06:59:49Z",
    "published": "2025-07-15T03:16:12Z",
    "title": "Robust ID-Specific Face Restoration via Alignment Learning",
    "authors": [
      "Yushun Fang",
      "Lu Liu",
      "Xiang Gao",
      "Qiang Hu",
      "Ning Cao",
      "Jianghe Cui",
      "Gang Chen",
      "Xiaoyun Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.10943v2.pdf",
    "comment": "PRCV2025 Accepted",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness.",
    "score": 2,
    "reason": "该论文聚焦于人脸修复（Face Restoration），核心贡献在于通过扩散模型与身份对齐学习提升修复结果的身份保真度。尽管其技术路径涉及生成建模和多模态条件控制，但研究对象为单人面部图像的视觉恢复，与文档图像理解（DIU）在任务目标、输入模态（非结构化人脸 vs. 结构化文档）、信息类型（身份特征 vs. 文本语义与布局关系）上无直接关联。此外，其方法未涉及OCR、实体识别、关系抽取或文档布局理解等DIU关键环节，也未使用VLM或LLM进行推理增强，与inference scaling、reasoning技术或agent系统无明显联系。因此，该论文虽属计算机视觉前沿工作，但无法直接迁移至DIU领域，相关性极低。",
    "summary": "本文提出一种基于扩散模型的鲁棒ID特定人脸修复框架（RIDFR），通过内容注入与身份注入双模块，并引入对齐学习机制，有效抑制姿态、表情等无关语义干扰，实现高保真度的身份一致人脸重建。实验表明其在视觉质量和身份一致性上优于现有方法。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20478v1",
    "updated": "2025-08-28T06:55:08Z",
    "published": "2025-08-28T06:55:08Z",
    "title": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding",
    "authors": [
      "Yuan Xie",
      "Tianshui Chen",
      "Zheng Ge",
      "Lionel Ni"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20478v1.pdf",
    "comment": "15 pages, 9 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains a challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose Video-MTR, a reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in a single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for a more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce a novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding.",
    "score": 6,
    "reason": "该论文提出Video-MTR框架用于长视频理解，核心思想是通过多轮强化推理实现关键片段的渐进式选择和问题理解。虽然其多轮推理机制与DIU中可能需要的迭代分析有一定关联性（如文档结构化理解中的逐步聚焦），但研究对象为长视频而非文档图像，且未涉及OCR、实体识别或关系抽取等DIU核心任务。尽管引入了强化学习奖励机制，但该方法针对的是视觉时序建模而非文本-布局联合理解，难以直接迁移到DIU场景。因此虽在推理范式上有一定启发意义，但与DIU的直接关联较弱，仅具间接参考价值。",
    "summary": "Video-MTR提出一种基于强化学习的多轮推理框架，用于长视频理解。通过多轮迭代选择关键视频片段并动态优化问题理解，结合轨迹级和轮次级奖励机制实现端到端训练，避免依赖外部VLM，在多个基准上达到SOTA性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20476v1",
    "updated": "2025-08-28T06:51:42Z",
    "published": "2025-08-28T06:51:42Z",
    "title": "Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding",
    "authors": [
      "Jeong Hun Yeo",
      "Hyeongseop Rha",
      "Sungjune Park",
      "Junil Won",
      "Yong Man Ro"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20476v1.pdf",
    "comment": "Code available at: https://github.com/JeongHun0716/UniSLA",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance.",
    "score": 4,
    "reason": "该论文聚焦于手语、唇动和音频的统一建模，虽涉及多模态理解与LLM应用，但核心任务是面向听障人群的通信系统，与文档图像理解（DIU）无直接关联。尽管其使用了LLM-based框架并探索多模态融合，但输入模态（手语/唇动）与DIU中的视觉文档布局、文本结构差异显著，难以直接迁移至DIU场景。虽然代码开源且发表于CVPR（CCF A类），但技术路径与DIU的核心需求（如OCR-free识别、布局理解、实体关系抽取）不匹配，因此相关性较低。",
    "summary": "本文提出一个统一的LLM框架，用于处理手语、唇动和音频三种模态，实现口语文本生成。通过设计模态无关架构并探索唇动作为手语非手动线索的作用，实现了在SLT、VSR、ASR及AVSR任务上优于或相当现有方法的性能。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20475v1",
    "updated": "2025-08-28T06:51:31Z",
    "published": "2025-08-28T06:51:31Z",
    "title": "Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization",
    "authors": [
      "Marina Grifell i Plana",
      "Vladyslav Zalevskyi",
      "Léa Schmidt",
      "Yvan Gomez",
      "Thomas Sanchez",
      "Vincent Dunet",
      "Mériam Koob",
      "Vanessa Siffredi",
      "Meritxell Bach Cuadra"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20475v1.pdf",
    "comment": "Accepted at the PIPPI Workshop of MICCAI 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate fetal brain segmentation is crucial for extracting biomarkers and assessing neurodevelopment, especially in conditions such as corpus callosum dysgenesis (CCD), which can induce drastic anatomical changes. However, the rarity of CCD severely limits annotated data, hindering the generalization of deep learning models. To address this, we propose a pathology-informed domain randomization strategy that embeds prior knowledge of CCD manifestations into a synthetic data generation pipeline. By simulating diverse brain alterations from healthy data alone, our approach enables robust segmentation without requiring pathological annotations. We validate our method on a cohort comprising 248 healthy fetuses, 26 with CCD, and 47 with other brain pathologies, achieving substantial improvements on CCD cases while maintaining performance on both healthy fetuses and those with other pathologies. From the predicted segmentations, we derive clinically relevant biomarkers, such as corpus callosum length (LCC) and volume, and show their utility in distinguishing CCD subtypes. Our pathology-informed augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these quantitative gains, our approach yields segmentations with improved topological consistency relative to available ground truth, enabling more reliable shape-based analyses. Overall, this work demonstrates that incorporating domain-specific anatomical priors into synthetic data pipelines can effectively mitigate data scarcity and enhance analysis of rare but clinically significant malformations.",
    "score": 2,
    "reason": "该论文聚焦于胎儿MRI中胼胝体分割，属于医学图像分析领域，虽然使用了域随机化等技术，但其研究对象（胎儿脑部病理）与文档图像理解（DIU）无直接关联。所提出的方法针对的是罕见病数据稀缺问题，且应用场景为临床神经发育评估，无法直接迁移至文档理解任务。尽管在MICCAI Workshop接受，但与DIU、VLM、LLM推理增强或Agent系统均无紧密联系，因此不值得阅读。",
    "summary": "本文提出一种基于病理先验的域随机化方法，用于增强胎儿MRI中胼胝体的分割性能，通过合成数据模拟胼胝体发育不良（CCD）的形态变化，在缺乏病理标注的情况下提升模型对罕见病的泛化能力。在健康及多种脑部病理数据上验证，显著降低胼胝体长度估计误差，提升拓扑一致性，适用于临床生物标志物提取。"
  },
  {
    "id": "http://arxiv.org/abs/2406.14862v7",
    "updated": "2025-08-28T06:51:18Z",
    "published": "2024-06-21T04:39:03Z",
    "title": "LatentExplainer: Explaining Latent Representations in Deep Generative Models with Multimodal Large Language Models",
    "authors": [
      "Mengdan Zhu",
      "Raasikh Kanjiani",
      "Jiahui Lu",
      "Andrew Choi",
      "Qirui Ye",
      "Liang Zhao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.14862v7.pdf",
    "comment": "Accepted to CIKM 2025 Full Research Track",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Deep generative models like VAEs and diffusion models have advanced various generation tasks by leveraging latent variables to learn data distributions and generate high-quality samples. Despite the field of explainable AI making strides in interpreting machine learning models, understanding latent variables in generative models remains challenging. This paper introduces LatentExplainer, a framework for automatically generating semantically meaningful explanations of latent variables in deep generative models. LatentExplainer tackles three main challenges: inferring the meaning of latent variables, aligning explanations with inductive biases, and handling varying degrees of explainability. Our approach perturbs latent variables, interprets changes in generated data, and uses multimodal large language models (MLLMs) to produce human-understandable explanations. We evaluate our proposed method on several real-world and synthetic datasets, and the results demonstrate superior performance in generating high-quality explanations for latent variables. The results highlight the effectiveness of incorporating inductive biases and uncertainty quantification, significantly enhancing model interpretability.",
    "score": 6,
    "reason": "该论文提出使用多模态大语言模型（MLLM）解释生成模型中的潜在表示，虽涉及VLM和可解释性，但核心聚焦于生成模型的解释，与DIU任务中对文档结构、实体与关系的理解关联较弱。虽然MLLM的应用有一定相关性，但其方法主要用于解释潜在空间而非直接支持文档信息抽取或推理，无法直接迁移至DIU场景。CIKM为CCF B类会议，开源状态未提及，因此加分有限。",
    "summary": "LatentExplainer利用多模态大语言模型（MLLM）自动解释深度生成模型（如VAE、扩散模型）中的潜在变量。通过扰动潜在变量并分析生成结果的变化，结合MLLM生成语义化解释，解决了潜在变量语义模糊、归纳偏置对齐和可解释性程度不一的问题。在真实与合成数据集上验证了有效性，提升了生成模型的可解释性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.16201v2",
    "updated": "2025-08-28T06:44:28Z",
    "published": "2025-08-22T08:23:09Z",
    "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
    "authors": [
      "Yicheng Ji",
      "Jun Zhang",
      "Heming Xia",
      "Jinpeng Chen",
      "Lidan Shou",
      "Gang Chen",
      "Huan Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.16201v2.pdf",
    "comment": "Accepted at EMNLP 2025 Main",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens to enable efficient speculation without sacrificing accuracy. To achieve this, we performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for Qwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM.",
    "score": 6,
    "reason": "该论文提出SpecVLM，针对视频大模型的推测解码（speculative decoding）进行优化，通过验证器引导的分阶段token剪枝来加速推理，减少计算和内存开销。虽然其核心贡献在于提升视频理解模型的推理效率，且在技术上与inference scaling相关（如推测解码是推理阶段扩展的重要手段），但其应用场景聚焦于视频内容理解，而非文档图像理解（DIU）。尽管VLM和inference scaling是DIU的上游技术，但本文方法直接应用于视频token处理，缺乏与文档布局、文本结构、表格或视觉信息抽取等DIU关键特征的结合。因此虽具技术先进性，但迁移至DIU的直接潜力有限，仅值得泛读以了解推理加速技术进展。",
    "summary": "SpecVLM提出一种无需训练的推测解码框架，用于加速视频大语言模型的推理过程。通过两阶段token剪枝——第一阶段利用验证器注意力信号筛选关键帧，第二阶段均匀剪枝冗余空间信息——实现高达90%的视频token压缩，同时保持准确率。在多个视频理解基准上实现了最高2.68倍的解码加速，代码已开源。论文被EMNLP 2025接收，属于顶会。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20471v1",
    "updated": "2025-08-28T06:39:53Z",
    "published": "2025-08-28T06:39:53Z",
    "title": "Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation",
    "authors": [
      "Jiusi Li",
      "Jackson Jiang",
      "Jinyu Miao",
      "Miao Long",
      "Tuopu Wen",
      "Peijin Jia",
      "Shengxiang Liu",
      "Chunlei Yu",
      "Maolin Liu",
      "Yuzhan Cai",
      "Kun Jiang",
      "Mengmeng Yang",
      "Diange Yang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20471v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.",
    "score": 2,
    "reason": "该论文聚焦于自动驾驶视频中3D物体的编辑，使用3D高斯表示进行图像生成与姿态控制，属于计算机视觉中的图像生成与场景编辑方向。虽然涉及视觉建模和多模态信息融合，但其核心任务是驱动视频合成，与文档图像理解（DIU）在任务目标、数据类型（自然驾驶 vs. 结构化文档）、应用场景上均无直接关联。此外，论文未涉及OCR、实体识别、关系抽取或文档布局理解等DIU核心模块，也未利用VLM/LLM进行推理或构建agent系统。尽管技术上可能有部分共享（如3D表示），但无法直接迁移至DIU领域，因此不具相关性。",
    "summary": "G^2Editor提出一种基于3D高斯表示的驾驶视频对象编辑框架，通过引入密集先验和层级特征条件，在Waymo数据集上实现高保真、精确姿态控制的物体插入、删除与重定位，显著提升生成质量与空间一致性，适用于自动驾驶数据增强。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20470v1",
    "updated": "2025-08-28T06:39:41Z",
    "published": "2025-08-28T06:39:41Z",
    "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
    "authors": [
      "Xiaochuan Li",
      "Guoguang Du",
      "Runze Zhang",
      "Liang Jin",
      "Qi Jia",
      "Lihua Lu",
      "Zhenhua Guo",
      "Yaqian Zhao",
      "Haiyang Liu",
      "Tianqi Wang",
      "Changsheng Li",
      "Xiaoli Gong",
      "Rengang Li",
      "Baoyu Fan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20470v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/.",
    "score": 3,
    "reason": "该论文聚焦于利用视频中的常识先验来促进3D生成，虽然涉及多模态和生成模型，但其核心任务是3D内容生成，与文档图像理解（DIU）无直接关联。尽管使用了视频作为输入并引入了空间一致性先验，但其应用场景、数据类型（视频 vs. 文档图像）、目标（3D资产生成 vs. 文本/布局语义理解）均不匹配。此外，未提及任何与OCR、实体识别、关系抽取或视觉信息抽取相关的技术，也未涉及VLM/LLM推理增强或agent系统设计。因此，虽在多模态生成领域有创新，但无法直接应用于DIU任务，相关性较低。",
    "summary": "Droplet3D提出利用视频中蕴含的常识先验来解决3D生成中的数据稀缺问题。构建了首个大规模带多视角标注的视频数据集Droplet3D-4M，并训练了一个支持图像和密集文本输入的生成模型Droplet3D。实验表明该方法能生成空间一致且语义合理的3D内容，具备扩展至场景级应用的潜力。所有资源已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20469v1",
    "updated": "2025-08-28T06:39:38Z",
    "published": "2025-08-28T06:39:38Z",
    "title": "Prediction of Distant Metastasis for Head and Neck Cancer Patients Using Multi-Modal Tumor and Peritumoral Feature Fusion Network",
    "authors": [
      "Zizhao Tang",
      "Changhao Liu",
      "Nuo Tong",
      "Shuiping Gou",
      "Mei Shi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20469v1.pdf",
    "comment": "19 pages, 4 figures, 5 tables. Zizhao Tang, Changhao Liu, and Nuo Tong contributed equally. Corresponding Authors: Mei Shi (mshi82@fmmu.edu.cn), Shuiping Gou (shpgou@mail.xidian.edu.cn)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Metastasis remains the major challenge in the clinical management of head and neck squamous cell carcinoma (HNSCC). Reliable pre-treatment prediction of metastatic risk is crucial for optimizing treatment strategies and prognosis. This study develops a deep learning-based multimodal framework to predict metastasis risk in HNSCC patients by integrating computed tomography (CT) images, radiomics, and clinical data. 1497 HNSCC patients were included. Tumor and organ masks were derived from pretreatment CT images. A 3D Swin Transformer extracted deep features from tumor regions. Meanwhile, 1562 radiomics features were obtained using PyRadiomics, followed by correlation filtering and random forest selection, leaving 36 features. Clinical variables including age, sex, smoking, and alcohol status were encoded and fused with imaging-derived features. Multimodal features were fed into a fully connected network to predict metastasis risk. Performance was evaluated using five-fold cross-validation with area under the curve (AUC), accuracy (ACC), sensitivity (SEN), and specificity (SPE). The proposed fusion model outperformed single-modality models. The 3D deep learning module alone achieved an AUC of 0.715, and when combined with radiomics and clinical features, predictive performance improved (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758). Stratified analysis showed generalizability across tumor subtypes. Ablation studies indicated complementary information from different modalities. Evaluation showed the 3D Swin Transformer provided more robust representation learning than conventional networks. This multimodal fusion model demonstrated high accuracy and robustness in predicting metastasis risk in HNSCC, offering a comprehensive representation of tumor biology. The interpretable model has potential as a clinical decision-support tool for personalized treatment planning.",
    "score": 2,
    "reason": "该论文聚焦于头颈癌患者的远处转移预测，使用多模态数据（CT影像、放射组学特征和临床信息）进行融合建模，虽涉及多模态深度学习，但应用场景为医学影像分析，与文档图像理解（DIU）、视觉语言模型（VLM）、大语言模型推理增强（inference scaling）或智能体（agent）技术无直接关联。其方法不涉及文本识别、实体抽取、关系建模等DIU核心任务，且未使用任何视觉-语言或多模态预训练模型，无法迁移至DIU领域。因此，不值得阅读。",
    "summary": "本研究提出一种基于3D Swin Transformer的多模态融合网络，用于预测头颈鳞状细胞癌患者的远处转移风险。模型整合了CT影像、放射组学特征和临床变量，通过特征融合与全连接网络实现分类。实验表明，多模态融合显著提升预测性能（AUC=0.803），具有良好的泛化性和可解释性，适用于临床决策支持，但与文档图像理解无关。"
  },
  {
    "id": "http://arxiv.org/abs/2504.08419v2",
    "updated": "2025-08-28T06:36:37Z",
    "published": "2025-04-11T10:23:55Z",
    "title": "GeoTexBuild: 3D Building Model Generation from Map Footprints",
    "authors": [
      "Ruizhe Wang",
      "Junyan Yang",
      "Qiao Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.08419v2.pdf",
    "comment": "13 pages, 14 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce GeoTexBuild, a modular generative framework for creating 3D building models from footprints derived from site planning or map designs. The system is designed for architects and city planners, offering a seamless solution that directly converts map features into 3D buildings. The proposed framework employs a three-stage process comprising height map generation, geometry reconstruction, and appearance stylization, culminating in building models with detailed geometry and appearance attributes. By integrating customized ControlNet, Neural style field (NSF), and Multi-view diffusion model, we explore effective methods for controlling both geometric and visual attributes during the generation process. Our approach eliminates the problem of structural variations in a single facade image in existing 3D generation techniques for buildings. Experimental results at each stage validate the capability of GeoTexBuild to generate detailed and accurate building models from footprints.",
    "score": 2,
    "reason": "该论文聚焦于从地图轮廓生成3D建筑模型，属于计算机视觉中的3D生成领域，与文档图像理解（DIU）无直接关联。尽管涉及视觉建模和多模态生成，但其应用目标（建筑设计、城市规划）与文档信息抽取任务完全不同，且未涉及文本识别、实体识别或关系抽取等DIU核心环节。虽然使用了ControlNet和扩散模型等技术，但这些技术在本工作中用于几何与外观生成，而非处理文档布局、文本语义或结构化信息。因此，不适用于DIU研究，也不直接支持VLM/LLM推理增强或Agent系统构建。",
    "summary": "GeoTexBuild提出一个三阶段框架，从地图足迹生成具有详细几何和外观的3D建筑模型，结合ControlNet、神经风格场和多视角扩散模型实现对建筑形状与视觉风格的精确控制，旨在服务建筑师和城市规划者。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20466v1",
    "updated": "2025-08-28T06:36:10Z",
    "published": "2025-08-28T06:36:10Z",
    "title": "Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds",
    "authors": [
      "Pengpeng Yu",
      "Haoran Li",
      "Dingquan Li",
      "Runqing Jiang",
      "Jing Wang",
      "Liang Lin",
      "Yulan Guo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20466v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "LiDAR point clouds are fundamental to various applications, yet high-precision scans incur substantial storage and transmission overhead. Existing methods typically convert unordered points into hierarchical octree or voxel structures for dense-to-sparse predictive coding. However, the extreme sparsity of geometric details hinders efficient context modeling, thereby limiting their compression performance and speed. To address this challenge, we propose to generate compact features for efficient predictive coding. Our framework comprises two lightweight modules. First, the Geometry Re-Densification Module re-densifies encoded sparse geometry, extracts features at denser scale, and then re-sparsifies the features for predictive coding. This module avoids costly computation on highly sparse details while maintaining a lightweight prediction head. Second, the Cross-scale Feature Propagation Module leverages occupancy cues from multiple resolution levels to guide hierarchical feature propagation. This design facilitates information sharing across scales, thereby reducing redundant feature extraction and providing enriched features for the Geometry Re-Densification Module. By integrating these two modules, our method yields a compact feature representation that provides efficient context modeling and accelerates the coding process. Experiments on the KITTI dataset demonstrate state-of-the-art compression ratios and real-time performance, achieving 26 FPS for both encoding and decoding at 12-bit quantization. Code is available at https://github.com/pengpeng-yu/FastPCC.",
    "score": 2,
    "reason": "该论文聚焦于LiDAR点云的实时压缩，属于三维感知与数据压缩领域，与文档图像理解（DIU）、多模态模型（VLM）、大语言模型（LLM）推理增强或智能体（Agent）系统无直接关联。其技术路径（如几何重稠密化、跨尺度特征传播）无法直接迁移至文本-布局联合建模、视觉信息抽取或推理链构建等DIU核心任务。尽管代码开源且在顶会CVPR上发表（虽未明确标注，但category为CVPR类），但研究方向与目标领域无关，不值得阅读。",
    "summary": "本文提出一种用于LiDAR点云实时压缩的新方法，通过几何重稠密化模块和跨尺度特征传播模块，在保持高压缩率的同时实现26 FPS的编码解码速度。该方法利用多分辨率占用线索进行高效特征传播，避免对极稀疏细节的昂贵计算，适用于自动驾驶等场景中的点云传输与存储优化。"
  },
  {
    "id": "http://arxiv.org/abs/2504.20376v2",
    "updated": "2025-08-28T06:34:01Z",
    "published": "2025-04-29T02:40:36Z",
    "title": "When Memory Becomes a Vulnerability: Towards Multi-turn Jailbreak Attacks against Text-to-Image Generation Systems",
    "authors": [
      "Shiqian Zhao",
      "Jiayang Liu",
      "Yiming Li",
      "Runyi Hu",
      "Xiaojun Jia",
      "Wenshu Fan",
      "Xinfeng Li",
      "Jie Zhang",
      "Wei Dong",
      "Tianwei Zhang",
      "Luu Anh Tuan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.20376v2.pdf",
    "comment": "This work proposes a multi-turn jailbreak attack against real-world chat-based T2I generation systems that intergrate memory mechanism. It also constructed a simulation system, with considering three industrial-grade memory mechanisms, 7 kinds of safety filters (both input and output)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Modern text-to-image (T2I) generation systems (e.g., DALL$\\cdot$E 3) exploit the memory mechanism, which captures key information in multi-turn interactions for faithful generation. Despite its practicality, the security analyses of this mechanism have fallen far behind. In this paper, we reveal that it can exacerbate the risk of jailbreak attacks. Previous attacks fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or lead to the generation of non-unsafe images due to under- or over-detoxification. In contrast, we propose embedding the malice at the inception of the chat session in memory, addressing the above limitations. Specifically, we propose Inception, the first multi-turn jailbreak attack against real-world text-to-image generation systems that explicitly exploits their memory mechanisms. Inception is composed of two key modules: segmentation and recursion. We introduce Segmentation, a semantic-preserving method that generates multi-round prompts. By leveraging NLP analysis techniques, we design policies to decompose a prompt, together with its malicious intent, according to sentence structure, thereby evading safety filters. Recursion further addresses the challenge posed by unsafe sub-prompts that cannot be separated through simple segmentation. It firstly expands the sub-prompt, then invokes segmentation recursively. To facilitate multi-turn adversarial prompts crafting, we build VisionFlow, an emulation T2I system that integrates two-stage safety filters and industrial-grade memory mechanisms. The experiment results show that Inception successfully allures unsafe image generation, surpassing the SOTA by a 20.0\\% margin in attack success rate. We also conduct experiments on the real-world commercial T2I generation platforms, further validating the threats of Inception in practice.",
    "score": 2,
    "reason": "该论文研究的是文本到图像生成系统中的多轮越狱攻击，聚焦于安全漏洞和对抗性提示设计，与文档图像理解（DIU）、视觉语言模型（VLM）的推理增强、多模态信息抽取或智能体系统无直接关联。尽管其涉及多轮交互和记忆机制，但应用场景为生成式图像安全攻防，而非信息理解或结构化提取。虽提及记忆机制，但属于攻击面分析，无法直接迁移至DIU任务中。因此不值得阅读。",
    "summary": "本文提出一种针对集成记忆机制的多轮文本到图像生成系统的越狱攻击方法Inception，通过语义保持的分段与递归技术生成多轮恶意提示，有效规避安全过滤器。作者构建了模拟系统VisionFlow以复现工业级内存机制和双重过滤策略，并在真实平台验证了攻击有效性，攻击成功率较SOTA提升20%。"
  },
  {
    "id": "http://arxiv.org/abs/2505.10583v2",
    "updated": "2025-08-28T06:16:17Z",
    "published": "2025-05-14T09:41:38Z",
    "title": "Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models",
    "authors": [
      "Diogo Freitas",
      "Brigt Håvardstun",
      "Cèsar Ferri",
      "Darío Garigliotti",
      "Jan Arne Telle",
      "José Hernández-Orallo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.10583v2.pdf",
    "comment": "54 pages (42 pages of appendix). Accepted for publication at the ECAI 2025 conference",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Large language models have become multimodal, and many of them are said to integrate their modalities using common representations. If this were true, a drawing of a car as an image, for instance, should map to a similar area in the latent space as a textual description of the strokes that form the drawing. To explore this in a black-box access regime to these models, we propose the use of machine teaching, a theory that studies the minimal set of examples a teacher needs to choose so that the learner captures the concept. In this paper, we evaluate the complexity of teaching vision-language models a subset of objects in the Quick, Draw! dataset using two presentations: raw images as bitmaps and trace coordinates in TikZ format. The results indicate that image-based representations generally require fewer segments and achieve higher accuracy than coordinate-based representations. But, surprisingly, the teaching size usually ranks concepts similarly across both modalities, even when controlling for (a human proxy of) concept priors, suggesting that the simplicity of concepts may be an inherent property that transcends modality representations.",
    "score": 6,
    "reason": "该论文研究视觉-语言模型中模态无关的绘图识别复杂性，虽涉及VLM和多模态表示，但核心关注点是概念教学复杂度的模态不变性，与DIU任务中的文档理解、实体识别、关系抽取等具体需求关联较弱。其方法（机器教学）和数据集（Quick, Draw!）也偏离了真实文档场景。尽管发表于ECAI 2025（CCF B类会议），且有较长附录体现严谨性，但对DIU的直接应用价值有限，仅在理论层面提供关于多模态表征一致性的间接启发。",
    "summary": "本文通过机器教学框架评估VLM在图像和坐标轨迹两种模态下学习Quick, Draw!中特定对象的难度，发现尽管图像模态效率更高，但不同概念的教学复杂度排名在两种模态间高度一致，表明概念简单性可能是一种超越模态表征的本质属性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20461v1",
    "updated": "2025-08-28T06:15:06Z",
    "published": "2025-08-28T06:15:06Z",
    "title": "Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification",
    "authors": [
      "Ayaka Tsutsumi",
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Satoshi Kondo",
      "Miki Haseyama"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20461v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We propose a novel medical image classification method that integrates dual-model weight selection with self-knowledge distillation (SKD). In real-world medical settings, deploying large-scale models is often limited by computational resource constraints, which pose significant challenges for their practical implementation. Thus, developing lightweight models that achieve comparable performance to large-scale models while maintaining computational efficiency is crucial. To address this, we employ a dual-model weight selection strategy that initializes two lightweight models with weights derived from a large pretrained model, enabling effective knowledge transfer. Next, SKD is applied to these selected models, allowing the use of a broad range of initial weight configurations without imposing additional excessive computational cost, followed by fine-tuning for the target classification tasks. By combining dual-model weight selection with self-knowledge distillation, our method overcomes the limitations of conventional approaches, which often fail to retain critical information in compact models. Extensive experiments on publicly available datasets-chest X-ray images, lung computed tomography scans, and brain magnetic resonance imaging scans-demonstrate the superior performance and robustness of our approach compared to existing methods.",
    "score": 2,
    "reason": "该论文聚焦于医学图像分类，采用双模型权重选择与自知识蒸馏技术以提升轻量化模型性能。尽管方法在计算效率和模型压缩方面有创新，但其研究对象为医学影像（如X光、CT、MRI），与文档图像理解（DIU）无直接关联。此外，论文未涉及OCR、实体识别、关系抽取等DIU核心任务，也未使用或借鉴多模态模型（VLM）、LLM推理增强（如CoT、ToT）或智能体（Agent）技术。领域不匹配，无法直接迁移应用至DIU，因此不值得阅读。",
    "summary": "本文提出一种结合双模型权重选择与自知识蒸馏的轻量化医学图像分类方法，旨在提升小模型在资源受限场景下的性能。通过从大模型初始化两个轻量模型并利用SKD进行知识迁移，实现在胸部X光、肺部CT和脑部MRI数据集上的优越表现。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20449v1",
    "updated": "2025-08-28T05:55:28Z",
    "published": "2025-08-28T05:55:28Z",
    "title": "A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection",
    "authors": [
      "Libo Lv",
      "Tianyi Wang",
      "Mengxiao Huang",
      "Ruixia Liu",
      "Yinglong Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20449v1.pdf",
    "comment": "Accepted to PRCV 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "With the rapid advancement of real-time deepfake generation techniques, forged content is becoming increasingly realistic and widespread across applications like video conferencing and social media. Although state-of-the-art detectors achieve high accuracy on standard benchmarks, their heavy computational cost hinders real-time deployment in practical applications. To address this, we propose the Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture for real-time deepfake detection. We design a spatial-frequency hybrid aware module that jointly leverages spatial textures and frequency artifacts through a gated mechanism, enhancing sensitivity to subtle manipulations. A token-selective cross attention mechanism enables efficient multi-level feature interaction, while a residual-enhanced blur pooling structure helps retain key semantic cues during downsampling. Experiments on several benchmark datasets show that SFMFNet achieves a favorable balance between accuracy and efficiency, with strong generalization and practical value for real-time applications.",
    "score": 2,
    "reason": "该论文聚焦于实时深度伪造检测，属于计算机视觉中的图像取证方向，与文档图像理解（DIU）无直接关联。其核心方法（空间-频率感知多尺度融合、令牌选择性交叉注意力等）虽在技术上具有创新性，但应用场景为视频/图像伪造检测，无法直接迁移至DIU任务。尽管被PRCV 2025接收（CCF C类会议），且提出轻量化设计，但与DIU、VLM推理增强或Agent系统无紧密联系，故不具相关性。",
    "summary": "本文提出SFMFNet，一种用于实时深度伪造检测的轻量级多尺度融合网络。通过空间-频率联合感知模块、令牌选择性交叉注意力和残差增强模糊池化结构，在保持高精度的同时显著降低计算开销，适用于视频会议与社交媒体等实时场景。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20447v1",
    "updated": "2025-08-28T05:52:58Z",
    "published": "2025-08-28T05:52:58Z",
    "title": "MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection",
    "authors": [
      "Taiga Yamane",
      "Satoshi Suzuki",
      "Ryo Masumura",
      "Shota Orihashi",
      "Tomohiro Tanaka",
      "Mana Ihori",
      "Naoki Makishima",
      "Naotaka Kawata"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20447v1.pdf",
    "comment": "Accepted by BMVC 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end trainable deep learning methods have progressed greatly. However, they often struggle to detect pedestrians with consistently small or large scales in views or with vastly different scales between views. This is because they do not exploit multi-scale image features to generate the BEV feature and detect pedestrians. To overcome this problem, we propose a novel MVPD method, called Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV features by projecting multi-scale image features extracted from individual views into the BEV space, scale-by-scale. Each of these BEV features inherits the properties of its corresponding scale image features from multiple views. Therefore, these BEV features help the precise detection of pedestrians with consistently small or large scales in views. Then, MSMVD combines information at different scales of multiple views by processing the multi-scale BEV features using a feature pyramid network. This improves the detection of pedestrians with vastly different scales between views. Extensive experiments demonstrate that exploiting multi-scale image features via multi-scale BEV features greatly improves the detection performance, and MSMVD outperforms the previous highest MODA by $4.5$ points on the GMVD dataset.",
    "score": 2,
    "reason": "该论文聚焦于多视角行人检测（MVPD），核心方法是利用多尺度图像特征生成多尺度BEV特征，属于计算机视觉中的目标检测领域。尽管其技术思路涉及多尺度特征融合与BEV空间建模，但研究对象为行人检测，与文档图像理解（DIU）在任务目标、数据分布和语义理解层面无直接关联。此外，论文未涉及OCR、实体识别、关系抽取等DIU关键环节，也未使用或适配多模态模型（VLM）、大语言模型（LLM）或推理增强技术（如CoT、ToT）。因此，无法直接应用于DIU领域，相关性极低。",
    "summary": "MSMVD提出一种基于多尺度图像特征投影到BEV空间的多视角行人检测方法，通过分尺度生成BEV特征并结合特征金字塔网络提升对不同尺度行人的检测性能，在GMVD数据集上显著优于现有方法。"
  },
  {
    "id": "http://arxiv.org/abs/2501.14316v3",
    "updated": "2025-08-28T05:41:16Z",
    "published": "2025-01-24T08:21:35Z",
    "title": "T-Stars-Poster: A Framework for Product-Centric Advertising Image Design",
    "authors": [
      "Hongyu Chen",
      "Min Zhou",
      "Jing Jiang",
      "Jiale Chen",
      "Yang Lu",
      "Zihang Lin",
      "Bo Xiao",
      "Tiezheng Ge",
      "Bo Zheng"
    ],
    "pdf_url": "https://arxiv.org/pdf/2501.14316v3.pdf",
    "comment": "Accepted by CIKM 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Creating advertising images is often a labor-intensive and time-consuming process. Can we automatically generate such images using basic product information like a product foreground image, taglines, and a target size? Existing methods mainly focus on parts of the problem and lack a comprehensive solution. To bridge this gap, we propose a novel product-centric framework for advertising image design called T-Stars-Poster. It consists of four sequential stages to highlight product foregrounds and taglines while achieving overall image aesthetics: prompt generation, layout generation, background image generation, and graphics rendering. Different expert models are designed and trained for the first three stages: First, a visual language model (VLM) generates background prompts that match the products. Next, a VLM-based layout generation model arranges the placement of product foregrounds, graphic elements (taglines and decorative underlays), and various nongraphic elements (objects from the background prompt). Following this, an SDXL-based model can simultaneously accept prompts, layouts, and foreground controls to generate images. To support T-Stars-Poster, we create two corresponding datasets with over 50,000 labeled images. Extensive experiments and online A/B tests demonstrate that T-Stars-Poster can produce more visually appealing advertising images.",
    "score": 3,
    "reason": "该论文聚焦于广告图像生成，虽使用了VLM和SDXL等技术，但核心任务是图像合成而非文档理解。其内容与DIU无关，未涉及OCR、实体识别、关系抽取或文档结构分析。尽管采用了多模态模型，但应用场景为广告设计，无法直接迁移至文档信息抽取领域。因此不相关。",
    "summary": "T-Stars-Poster提出一个用于产品导向广告图像设计的四阶段框架，包括提示生成、布局生成、背景生成和图形渲染，利用VLM和SDXL实现自动化图像创作，并构建了两个大规模数据集。实验表明其能生成更具视觉吸引力的广告图，已被CIKM 2025接收。"
  },
  {
    "id": "http://arxiv.org/abs/2411.00626v2",
    "updated": "2025-08-28T04:44:11Z",
    "published": "2024-11-01T14:34:33Z",
    "title": "ZIM: Zero-Shot Image Matting for Anything",
    "authors": [
      "Beomyoung Kim",
      "Chanyong Shin",
      "Joonhyun Jeong",
      "Hyungsik Jung",
      "Se-Yun Lee",
      "Sewhan Chun",
      "Dong-Hyun Hwang",
      "Joonsang Yu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.00626v2.pdf",
    "comment": "ICCV 2025 (Highlight)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D NeRF. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at https://github.com/naver-ai/ZIM.",
    "score": 4,
    "reason": "该论文提出ZIM，一种零样本图像抠图模型，专注于生成精细的掩码。虽然其在图像分割与掩码生成方面具有技术先进性，且基于SAM构建并引入了新的数据集和结构设计，但其核心任务为图像抠图（image matting），与文档图像理解（DIU）的核心目标——文本识别、实体抽取与关系建模——无直接关联。尽管其精细掩码能力可能间接辅助OCR阶段的文本区域提取，但该论文未涉及文档布局分析、文本语义理解或信息抽取等关键DIU环节，也无法直接迁移至DIU中的实体识别或关系抽取任务。此外，论文未提及多模态模型、推理增强（如CoT/TOT）、agent架构或与LLM/VLM协同的系统设计，因此与inference scaling、reasoning或agent方向均无紧密联系。虽为ICCV 2025 Highlight且代码开源，但应用相关性较低，故评分中等偏下。",
    "summary": "ZIM提出一种零样本图像抠图模型，通过将分割标签转换为精细的抠图标签构建SA1B-Matte数据集，并引入分层像素解码器与提示感知注意力机制，提升细粒度掩码生成能力。在MicroMat-3K上表现优于现有方法，适用于图像修复与3D重建等下游任务，代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2412.10439v3",
    "updated": "2025-08-28T04:36:42Z",
    "published": "2024-12-11T09:50:35Z",
    "title": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs",
    "authors": [
      "Yihan Cao",
      "Jiazhao Zhang",
      "Zhinan Yu",
      "Shuzhen Liu",
      "Zheng Qin",
      "Qin Zou",
      "Bo Du",
      "Kai Xu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2412.10439v3.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Object goal navigation (ObjectNav) is a fundamental task in embodied AI, requiring an agent to locate a target object in previously unseen environments. This task is particularly challenging because it requires both perceptual and cognitive processes, including object recognition and decision-making. While substantial advancements in perception have been driven by the rapid development of visual foundation models, progress on the cognitive aspect remains constrained, primarily limited to either implicit learning through simulator rollouts or explicit reliance on predefined heuristic rules. Inspired by neuroscientific findings demonstrating that humans maintain and dynamically update fine-grained cognitive states during object search tasks in novel environments, we propose CogNav, a framework designed to mimic this cognitive process using large language models. Specifically, we model the cognitive process using a finite state machine comprising fine-grained cognitive states, ranging from exploration to identification. Transitions between states are determined by a large language model based on a dynamically constructed heterogeneous cognitive map, which contains spatial and semantic information about the scene being explored. Extensive evaluations on the HM3D, MP3D, and RoboTHOR benchmarks demonstrate that our cognitive process modeling significantly improves the success rate of ObjectNav at least by relative 14% over the state-of-the-arts.",
    "score": 6,
    "reason": "该论文提出基于LLM的认知过程建模用于目标导向导航（ObjectNav），虽涉及LLM在推理与决策中的应用，但其任务场景为具身智能中的环境探索与物体定位，与文档图像理解（DIU）的核心任务——文本识别、实体抽取与关系建模——无直接关联。尽管其使用LLM进行动态状态推理和认知地图构建的技术思路具有启发性，但该方法主要服务于空间感知与路径规划，无法直接迁移至文档结构解析或信息抽取任务。因此，虽在inference scaling和reasoning方面有一定相关性，但与DIU领域缺乏紧密技术耦合，仅具间接参考价值。",
    "summary": "CogNav提出一种基于大语言模型的框架，通过模拟人类在新环境中搜索目标物体时的认知状态演变（如探索、识别等），利用有限状态机与动态异构认知地图实现更高效的物体目标导航。实验表明，该方法在HM3D、MP3D和RoboTHOR数据集上显著提升导航成功率，较现有最优方法提升至少14%。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20415v1",
    "updated": "2025-08-28T04:31:48Z",
    "published": "2025-08-28T04:31:48Z",
    "title": "Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection",
    "authors": [
      "Yuqi Xiong",
      "Wuzhen Shi",
      "Yang Wen",
      "Ruhan Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20415v1.pdf",
    "comment": "ICONIP 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In view of the problems that existing salient object detection (SOD) methods are prone to losing details, blurring edges, and insufficient fusion of single-modal information in complex scenes, this paper proposes a dynamic uncertainty propagation and multimodal collaborative reasoning network (DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is designed to propagate uncertainty between layers through a sparse graph constructed based on spatial semantic distance, and combined with channel adaptive interaction, it effectively improves the detection accuracy of small structures and edge regions. Secondly, a multimodal collaborative fusion strategy (MCF) is proposed, which uses learnable modality gating weights to weightedly fuse the attention maps of RGB, depth, and edge features. It can dynamically adjust the importance of each modality according to different scenes, effectively suppress redundant or interfering information, and strengthen the semantic complementarity and consistency between cross-modalities, thereby improving the ability to identify salient regions under occlusion, weak texture or background interference. Finally, the detection performance at the pixel level and region level is optimized through multi-scale BCE and IoU loss, cross-scale consistency constraints, and uncertainty-guided supervision mechanisms. Extensive experiments show that DUP-MCRNet outperforms various SOD methods on most common benchmark datasets, especially in terms of edge clarity and robustness to complex backgrounds. Our code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.",
    "score": 2,
    "reason": "该论文聚焦于显著性物体检测（SOD），属于计算机视觉中的基础任务，与文档图像理解（DIU）无直接关联。尽管其提出的动态不确定性传播和多模态融合机制在理论上可能对多模态模型的鲁棒性有启发，但SOD的目标是定位图像中视觉上突出的区域，而非结构化信息抽取、实体识别或关系建模，无法直接应用于DIU的核心任务。此外，论文未涉及OCR、布局分析、表格理解或文档语义理解等关键环节，也未使用或适配VLM/LLM进行推理或agent架构设计，因此不具可迁移性。虽代码开源且发表于ICONIP 2025，但该会议非CCF A类，影响力有限。",
    "summary": "本文提出DUP-MCRNet，一种用于显著性物体检测的动态不确定性传播与多模态协同推理网络。通过构建基于空间语义距离的稀疏图进行不确定性传播，并结合通道自适应交互提升小结构和边缘区域的检测精度；同时设计可学习模态门控权重实现RGB、深度和边缘特征的动态融合，增强跨模态一致性与抗干扰能力。实验表明该方法在多个基准数据集上优于现有SOD方法，尤其在复杂背景下的边缘清晰度表现优异。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20414v1",
    "updated": "2025-08-28T04:31:41Z",
    "published": "2025-08-28T04:31:41Z",
    "title": "Federated Learning for Large Models in Medical Imaging: A Comprehensive Review",
    "authors": [
      "Mengyu Sun",
      "Ziyuan Yang",
      "Yongqiang Huang",
      "Hui Yu",
      "Yingyu Chen",
      "Shuren Qi",
      "Andrew Beng Jin Teoh",
      "Yi Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20414v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Artificial intelligence (AI) has demonstrated considerable potential in the realm of medical imaging. However, the development of high-performance AI models typically necessitates training on large-scale, centralized datasets. This approach is confronted with significant challenges due to strict patient privacy regulations and legal restrictions on data sharing and utilization. These limitations hinder the development of large-scale models in medical domains and impede continuous updates and training with new data. Federated Learning (FL), a privacy-preserving distributed training framework, offers a new solution by enabling collaborative model development across fragmented medical datasets. In this survey, we review FL's contributions at two stages of the full-stack medical analysis pipeline. First, in upstream tasks such as CT or MRI reconstruction, FL enables joint training of robust reconstruction networks on diverse, multi-institutional datasets, alleviating data scarcity while preserving confidentiality. Second, in downstream clinical tasks like tumor diagnosis and segmentation, FL supports continuous model updating by allowing local fine-tuning on new data without centralizing sensitive images. We comprehensively analyze FL implementations across the medical imaging pipeline, from physics-informed reconstruction networks to diagnostic AI systems, highlighting innovations that improve communication efficiency, align heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this paper provides an outlook on future research directions, aiming to serve as a valuable reference for the field's development.",
    "score": 2,
    "reason": "该论文聚焦于医学影像中的联邦学习，虽涉及大规模模型训练与隐私保护，但研究场景为医疗影像分析，与文档图像理解（DIU）无直接关联。其技术路径（联邦学习）虽可能间接启发DIU中的数据隐私问题，但无法直接应用于DIU任务中。同时，未涉及OCR、实体识别、关系抽取或多模态模型、推理增强、agent系统等关键方向。因此，不值得在DIU研究中优先阅读。",
    "summary": "本文综述了联邦学习在医学影像领域的应用，涵盖从图像重建到临床诊断的全流程，强调其在跨机构数据协作中的隐私保护优势，提出提升通信效率与参数聚合安全性的方法，并展望未来研究方向。"
  },
  {
    "id": "http://arxiv.org/abs/2505.24389v2",
    "updated": "2025-08-28T04:31:31Z",
    "published": "2025-05-30T09:19:33Z",
    "title": "Leadership Assessment in Pediatric Intensive Care Unit Team Training",
    "authors": [
      "Liangyang Ouyang",
      "Yuki Sakai",
      "Ryosuke Furuta",
      "Hisataka Nozawa",
      "Hikoro Matsui",
      "Yoichi Sato"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.24389v2.pdf",
    "comment": "This paper is accepted by EgoVis Workshop at CVPR 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This paper addresses the task of assessing PICU team's leadership skills by developing an automated analysis framework based on egocentric vision. We identify key behavioral cues, including fixation object, eye contact, and conversation patterns, as essential indicators of leadership assessment. In order to capture these multimodal signals, we employ Aria Glasses to record egocentric video, audio, gaze, and head movement data. We collect one-hour videos of four simulated sessions involving doctors with different roles and levels. To automate data processing, we propose a method leveraging REMoDNaV, SAM, YOLO, and ChatGPT for fixation object detection, eye contact detection, and conversation classification. In the experiments, significant correlations are observed between leadership skills and behavioral metrics, i.e., the output of our proposed methods, such as fixation time, transition patterns, and direct orders in speech. These results indicate that our proposed data collection and analysis framework can effectively solve skill assessment for training PICU teams.",
    "score": 2,
    "reason": "该论文聚焦于儿科重症监护团队训练中的领导力评估，使用第一人称视觉数据（如Aria眼镜）结合多模态分析，虽涉及视觉与语言模型（ChatGPT）的应用，但其核心任务是医疗团队行为分析，与文档图像理解（DIU）无直接关联。尽管使用了SAM、YOLO等视觉模型，但应用场景为人体行为识别而非文档内容解析。此外，未涉及OCR、实体识别、关系抽取或布局理解等DIU关键技术，且研究目标为医疗培训评估，无法直接迁移至DIU领域。虽然在CVPR Workshop发表，但相关性较低。",
    "summary": "本研究通过第一人称视觉设备采集医生在模拟PICU团队训练中的视频、音频、注视和头部运动数据，提出一个融合REM oDNaV、SAM、YOLO和ChatGPT的框架，用于检测注视对象、眼神接触和对话模式，并将其与领导力评分相关联。实验表明这些行为指标与领导力表现显著相关，验证了该框架在医疗团队培训评估中的有效性。"
  },
  {
    "id": "http://arxiv.org/abs/2406.04680v3",
    "updated": "2025-08-28T04:29:29Z",
    "published": "2024-06-07T06:51:09Z",
    "title": "MTS-Net: Dual-Enhanced Positional Multi-Head Self-Attention for 3D CT Diagnosis of May-Thurner Syndrome",
    "authors": [
      "Yixin Huang",
      "Yiqi Jin",
      "Ke Tao",
      "Kaijian Xia",
      "Jianfeng Gu",
      "Lei Yu",
      "Haojie Li",
      "Lan Du",
      "Cunjian Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.04680v3.pdf",
    "comment": "Accepted by Biomedical Signal Processing and Control",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "May-Thurner Syndrome (MTS) is a vascular condition that affects over 20\\% of the population and significantly increases the risk of iliofemoral deep venous thrombosis. Accurate and early diagnosis of MTS using computed tomography (CT) remains a clinical challenge due to the subtle anatomical compression and variability across patients. In this paper, we propose MTS-Net, an end-to-end 3D deep learning framework designed to capture spatial-temporal patterns from CT volumes for reliable MTS diagnosis. MTS-Net builds upon 3D ResNet-18 by embedding a novel dual-enhanced positional multi-head self-attention (DEP-MHSA) module into the Transformer encoder of the network's final stages. The proposed DEP-MHSA employs multi-scale convolution and integrates positional embeddings into both attention weights and residual paths, enhancing spatial context preservation, which is crucial for identifying venous compression. To validate our approach, we curate the first publicly available dataset for MTS, MTS-CT, containing over 747 gender-balanced subjects with standard and enhanced CT scans. Experimental results demonstrate that MTS-Net achieves average 0.79 accuracy, 0.84 AUC, and 0.78 F1-score, outperforming baseline models including 3D ResNet, DenseNet-BC, and BabyNet. Our work not only introduces a new diagnostic architecture for MTS but also provides a high-quality benchmark dataset to facilitate future research in automated vascular syndrome detection. We make our code and dataset publicly available at:https://github.com/Nutingnon/MTS_dep_mhsa.",
    "score": 2,
    "reason": "该论文聚焦于3D CT图像中May-Thurner综合征的诊断，属于医学影像分析领域，与文档图像理解（DIU）无直接关联。尽管其提出的DEP-MHSA模块在结构上具有创新性，但应用于解剖结构识别而非文本/布局信息提取，无法直接迁移至DIU任务。虽有开源代码和数据集，且被生物医学信号处理类期刊接收，但与DIU、VLM、LLM推理增强或agent技术无关，不具可应用性。",
    "summary": "MTS-Net提出一种基于3D ResNet-18的深度学习框架，通过引入双增强位置多头自注意力模块（DEP-MHSA）提升对CT体积中静脉压缩特征的捕捉能力，用于May-Thurner综合征的自动诊断。研究构建了首个公开的MTS-CT数据集，并在747例患者数据上验证模型性能，达到0.79准确率、0.84 AUC和0.78 F1-score，优于多个基线模型。代码与数据已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2506.23484v2",
    "updated": "2025-08-28T04:19:48Z",
    "published": "2025-06-30T03:14:07Z",
    "title": "TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity",
    "authors": [
      "Yuzhuo Chen",
      "Zehua Ma",
      "Han Fang",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2506.23484v2.pdf",
    "comment": "Camera-ready version for ICCV 2025. Adds GitHub link; acknowledgments; appendix. Abstract and Figure 1 updated for clarity",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. However, the widespread adoption and advancing capabilities of generative image editing tools have amplified malicious tampering risks, while simultaneously posing new challenges to passive tampering detection and watermark robustness. To address these challenges, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results demonstrate that TAG-WM achieves state-of-the-art performance in both tampering robustness and localization capability even under distortion, while preserving lossless generation quality and maintaining a watermark capacity of 256 bits. The code is available at: https://github.com/Suchenl/TAG-WM.",
    "score": 3,
    "reason": "该论文聚焦于生成图像的防篡改水印技术，虽然涉及视觉内容完整性验证，但其核心目标是版权保护与篡改检测，而非文档信息理解。尽管使用了扩散模型和逆向推理，但应用场景为通用图像生成内容的安全性，与DIU任务中对文档结构、文本语义、字段关系的理解无直接关联。此外，未提及文档图像、OCR、实体识别或关系抽取等关键环节，也未涉及多模态模型在DIU中的应用。因此，虽属计算机视觉前沿，但与DIU领域技术路径不直接相关，仅具备间接参考价值。",
    "summary": "TAG-WM提出一种基于扩散模型逆向敏感性的生成图像水印方法，通过联合嵌入版权与定位水印，并利用反向采样和统计偏差分析实现高鲁棒性篡改检测与定位。实验表明其在失真下仍保持256比特容量和无损生成质量。代码已开源，发表于ICCV 2025（CCF A类会议）"
  },
  {
    "id": "http://arxiv.org/abs/2505.15576v2",
    "updated": "2025-08-28T04:15:35Z",
    "published": "2025-05-21T14:28:43Z",
    "title": "Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models",
    "authors": [
      "Xin Huang",
      "Ruibin Li",
      "Tong Jia",
      "Wei Zheng",
      "Ya Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.15576v2.pdf",
    "comment": "Accepted at the International Joint Conference on Artificial Intelligence (IJCAI 2025)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the model's discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMs' performance on complex CR tasks. The source code is available at https://github.com/nynu-BDAI/AHNPL.",
    "score": 9,
    "reason": "该论文针对视觉语言模型在组合推理任务中的关键瓶颈——视觉负样本利用不足与难样本对齐问题，提出创新的AHPNL方法，通过将文本硬负样本转化为语义扰动的图像负样本，并引入动态边距对比学习，显著提升模型对细粒度语义差异的区分能力。该工作直接服务于VLM在DIU中所需的复杂语义理解与跨模态对齐能力，尤其适用于文档中结构化信息的精确识别与关系推断。IJCAI 2025为CCF A类会议，且代码已开源，具备高可复现性与应用潜力。",
    "summary": "本文提出自适应硬负样本扰动学习（AHNPL），通过生成语义扰动的图像负样本并结合多模态硬负损失与动态边距损失，增强视觉语言模型在组合推理任务中的性能。实验表明该方法有效提升模型对细粒度语义差异的辨别能力，适用于需要精细语义理解的文档图像理解场景。代码已公开。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20392v1",
    "updated": "2025-08-28T03:32:45Z",
    "published": "2025-08-28T03:32:45Z",
    "title": "Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection",
    "authors": [
      "Chengjun Zhang",
      "Yuhao Zhang",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20392v1.pdf",
    "comment": "12 pages, 8 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Spiking Neural Networks (SNNs), inspired by the brain, are characterized by minimal power consumption and swift inference capabilities on neuromorphic hardware, and have been widely applied to various visual perception tasks. Current ANN-SNN conversion methods have achieved excellent results in classification tasks with ultra-low time-steps, but their performance in visual detection tasks remains suboptimal. In this paper, we propose a delay-spike approach to mitigate the issue of residual membrane potential caused by heterogeneous spiking patterns. Furthermore, we propose a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This enables Integrate-and-fire (IF) neurons to dynamically adjust their accumulation and firing behaviors based on the temporal order of time-steps. Our method enables spikes to exhibit distinct temporal properties, rather than relying solely on frequency-based representations. Moreover, the tdIF neuron maintains energy consumption on par with traditional IF neuron. We demonstrate that our method achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. In this study, we conduct extensive evaluation of the tdIF method across two critical vision tasks: object detection and lane line detection. The results demonstrate that the proposed method surpasses current ANN-SNN conversion approaches, achieving state-of-the-art performance with ultra-low latency (within 5 time-steps).",
    "score": 2,
    "reason": "该论文研究的是脉冲神经网络（SNN）在目标检测中的应用，聚焦于低延迟和能效优化。虽然其技术方向与高效推理相关，但属于神经形态计算领域，与文档图像理解（DIU）、多模态模型（VLM）、大语言模型（LLM）的推理增强（如CoT、ToT）或智能体（Agent）系统无直接关联。其方法基于脉冲时序建模，无法直接迁移至文本-布局联合理解任务，也不涉及视觉信息抽取、实体识别或关系抽取等DIU核心环节。因此，不值得阅读。",
    "summary": "本文提出一种基于时间依赖积分-发放（tdIF）神经元的脉冲神经网络架构，用于实现超低延迟的目标检测与车道线检测。通过引入延迟脉冲机制和动态调整神经元行为，提升了SNN在极短时间步内的特征表示能力，实现了优于现有ANN-SNN转换方法的性能，同时保持低功耗。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20381v1",
    "updated": "2025-08-28T03:07:57Z",
    "published": "2025-08-28T03:07:57Z",
    "title": "More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning",
    "authors": [
      "Luong Tran",
      "Thieu Vo",
      "Anh Nguyen",
      "Sang Dinh",
      "Van Nguyen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20381v1.pdf",
    "comment": "ICCV 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Multi-label learning is a challenging computer vision task that requires assigning multiple categories to each image. However, fully annotating large-scale datasets is often impractical due to high costs and effort, motivating the study of learning from partially annotated data. In the extreme case of Single Positive Multi-Label Learning (SPML), each image is provided with only one positive label, while all other labels remain unannotated. Traditional SPML methods that treat missing labels as unknown or negative tend to yield inaccuracies and false negatives, and integrating various pseudo-labeling strategies can introduce additional noise. To address these challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a novel loss function that effectively learns from diverse pseudo-labels while mitigating noise. Complementing this, we introduce a simple yet effective Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling (AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate that our framework significantly advances multi-label classification, achieving state-of-the-art results.",
    "score": 4,
    "reason": "该论文聚焦于单正例多标签学习（SPML）中的伪标签优化问题，提出GPR Loss与DAMP框架以提升多标签分类性能。虽然其方法在视觉分类任务中表现优异，且发表于ICCV 2025（CCF A类会议），具备一定学术价值，但其核心贡献属于通用计算机视觉中的标签学习范式改进，与文档图像理解（DIU）、视觉-语言模型（VLM）推理增强或智能体系统无直接关联。文中未涉及文档结构、实体识别、关系抽取或可迁移至DIU的推理机制，也未提及与LLM/VLM协同应用。因此，虽具技术深度，但无法直接应用于DIU领域，仅作为背景参考可选阅读。",
    "summary": "本文提出AEVLP框架，通过新型GPR损失函数与动态多焦点伪标签技术，在单正例多标签学习场景下有效缓解伪标签噪声问题，显著提升多标签分类性能，在多个基准数据集上达到SOTA水平。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20379v1",
    "updated": "2025-08-28T03:00:30Z",
    "published": "2025-08-28T03:00:30Z",
    "title": "Audio-Guided Visual Editing with Complex Multi-Modal Prompts",
    "authors": [
      "Hyeonyu Kim",
      "Seokhoon Jeong",
      "Seonghee Han",
      "Chanhyuk Choi",
      "Taehwan Kim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20379v1.pdf",
    "comment": "Accepted to BMVC 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.",
    "score": 3,
    "reason": "该论文聚焦于音频引导的视觉编辑，虽涉及多模态融合与复杂提示处理，但核心任务为图像编辑而非文档理解。其方法基于扩散模型和音频-文本联合引导，与DIU中OCR、实体识别、关系抽取等关键环节无直接关联。尽管使用了多模态提示和零样本能力，但应用场景为通用视觉编辑，无法直接迁移至文档结构理解或信息抽取任务。未涉及VLM在DIU中的端到端应用，也未触及inference scaling或agent框架。因此不具直接相关性。",
    "summary": "本文提出一种无需额外训练的音频引导视觉编辑框架，利用预训练多模态编码器整合文本与音频提示，通过分离噪声分支和自适应补丁选择实现复杂多模态编辑。实验表明其在多种复杂编辑任务中优于纯文本引导方法，适用于通用图像修改场景。"
  },
  {
    "id": "http://arxiv.org/abs/2507.04671v2",
    "updated": "2025-08-28T02:55:48Z",
    "published": "2025-07-07T05:22:55Z",
    "title": "DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and Continuous Adaptation",
    "authors": [
      "Maolin Wang",
      "Tianshuo Wei",
      "Sheng Zhang",
      "Ruocheng Guo",
      "Wanyu Wang",
      "Shanshan Ye",
      "Lixin Zou",
      "Xuetao Wei",
      "Xiangyu Zhao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.04671v2.pdf",
    "comment": "Accepted by IJCAI 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Neural Architecture Search (NAS) has emerged as a powerful approach for automating neural network design. However, existing NAS methods face critical limitations in real-world deployments: architectures lack adaptability across scenarios, each deployment context requires costly separate searches, and performance consistency across diverse platforms remains challenging. We propose DANCE (Dynamic Architectures with Neural Continuous Evolution), which reformulates architecture search as a continuous evolution problem through learning distributions over architectural components. DANCE introduces three key innovations: a continuous architecture distribution enabling smooth adaptation, a unified architecture space with learned selection gates for efficient sampling, and a multi-stage training strategy for effective deployment optimization. Extensive experiments across five datasets demonstrate DANCE's effectiveness. Our method consistently outperforms state-of-the-art NAS approaches in terms of accuracy while significantly reducing search costs. Under varying computational constraints, DANCE maintains robust performance while smoothly adapting architectures to different hardware requirements. The code and appendix can be found at https://github.com/Applied-Machine-Learning-Lab/DANCE.",
    "score": 3,
    "reason": "该论文提出DANCE，一种面向资源高效的神经架构搜索方法，专注于在不同硬件环境下动态适应模型结构。虽然其在NAS领域具有创新性，且被IJCAI 2025接收（CCF-B类会议），但其与文档图像理解（DIU）、视觉多模态模型（VLM）推理增强（inference scaling）、或智能体（agent）系统无直接关联。其核心贡献是架构搜索机制的改进，无法直接应用于DIU任务中的OCR、实体识别、关系抽取或基于VLM/LLM的端到端理解。因此，尽管技术上先进，但与目标领域关联度低，不值得优先阅读。",
    "summary": "DANCE提出一种基于连续演化的神经架构搜索框架，通过学习架构组件的概率分布实现跨场景自适应。引入连续架构分布、统一架构空间与选择门机制，并采用多阶段训练策略，在多种硬件约束下实现高性能与低搜索成本。实验表明其在多个数据集上优于现有NAS方法。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20376v1",
    "updated": "2025-08-28T02:50:19Z",
    "published": "2025-08-28T02:50:19Z",
    "title": "Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction",
    "authors": [
      "Mang Cao",
      "Sanping Zhou",
      "Yizhe Li",
      "Ye Deng",
      "Wenli Huang",
      "Le Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20376v1.pdf",
    "comment": "Codes are available online: \\url{https://github.com/mmm-cc/BIM\\_for\\_MTL}",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Sufficient cross-task interaction is crucial for success in multi-task dense prediction. However, sufficient interaction often results in high computational complexity, forcing existing methods to face the trade-off between interaction completeness and computational efficiency. To address this limitation, this work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel scanning mechanisms to adapt the Mamba modeling approach for multi-task dense prediction. On the one hand, we introduce a novel Bidirectional Interaction Scan (BI-Scan) mechanism, which constructs task-specific representations as bidirectional sequences during interaction. By integrating task-first and position-first scanning modes within a unified linear complexity architecture, BI-Scan efficiently preserves critical cross-task information. On the other hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve multi-granularity scene modeling. This design not only meets the diverse granularity requirements of various tasks but also enhances nuanced cross-task feature interactions. Extensive experiments on two challenging benchmarks, \\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its state-of-the-art competitors.",
    "score": 4,
    "reason": "该论文提出一种用于多任务密集预测的双向交互Mamba架构（BIM），虽然在计算效率与跨任务交互之间取得平衡，但其应用场景集中在NYUD-V2和PASCAL-Context等通用场景图像理解任务上，与文档图像理解（DIU）无直接关联。尽管Mamba结构具有潜在优势，但未涉及OCR、实体识别、关系抽取或布局建模等DIU核心模块，也未使用视觉语言模型或推理增强技术。虽有开源代码且发表于CVPR类别会议，但技术路径与DIU领域关键需求（如文档结构理解、多模态对齐、生成式信息抽取）匹配度低，无法直接迁移应用。",
    "summary": "本文提出Bidirectional Interaction Mamba（BIM），通过引入双向扫描机制（BI-Scan）和多尺度扫描机制（MS-Scan），在保持线性复杂度的前提下提升多任务密集预测中的跨任务交互能力。在NYUD-V2和PASCAL-Context数据集上验证了其有效性，但研究目标为通用场景理解，不适用于文档图像理解任务。"
  },
  {
    "id": "http://arxiv.org/abs/2503.22677v2",
    "updated": "2025-08-28T01:47:51Z",
    "published": "2025-03-28T17:59:53Z",
    "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.22677v2.pdf",
    "comment": "Accepted at ICCV 2025 (Highlight). Project page: https://ruiningli.com/dso",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Most 3D object generators prioritize aesthetic quality, often neglecting the physical constraints necessary for practical applications. One such constraint is that a 3D object should be self-supporting, i.e., remain balanced under gravity. Previous approaches to generating stable 3D objects relied on differentiable physics simulators to optimize geometry at test time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models with external feedback, we propose Direct Simulation Optimization (DSO). This framework leverages feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator directly outputs stable 3D objects. We construct a dataset of 3D objects labeled with stability scores obtained from the physics simulator. This dataset enables fine-tuning of the 3D generator using the stability score as an alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO) - a novel objective we introduce to align diffusion models without requiring pairwise preferences. Our experiments demonstrate that the fine-tuned feed-forward generator, using either the DPO or DRO objective, is significantly faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework functions even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.",
    "score": 2,
    "reason": "该论文聚焦于3D生成器与物理仿真反馈对齐，以提升3D物体的物理稳定性。核心方法涉及利用非可微物理模拟器进行反馈优化，虽在生成模型与外部环境交互方面有创新，但其研究对象为3D几何生成，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型（LLM）推理增强或智能体系统无直接关联。尽管提出的方法可能间接启发未来跨模态生成中的稳定性设计，但无法直接应用于DIU任务中，因此不具直接应用价值。",
    "summary": "本文提出Direct Simulation Optimization (DSO)框架，通过物理仿真反馈引导3D生成器输出自支撑物体，避免依赖测试时优化。利用仿真获得的稳定性评分，采用DPO或新提出的DRO目标对扩散模型进行微调，实现高效且稳定的3D生成。该方法无需真实3D训练数据，支持生成器自我迭代改进。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20345v1",
    "updated": "2025-08-28T01:39:16Z",
    "published": "2025-08-28T01:39:16Z",
    "title": "MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models",
    "authors": [
      "Xiao Li",
      "Yanfan Zhu",
      "Ruining Deng",
      "Wei-Qi Wei",
      "Yu Wang",
      "Shilin Zhao",
      "Yaohong Wang",
      "Haichun Yang",
      "Yuankai Huo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20345v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in medical vision-language models (VLMs) open up remarkable opportunities for clinical applications such as automated report generation, copilots for physicians, and uncertainty quantification. However, despite their promise, medical VLMs introduce serious security concerns, most notably risks of Protected Health Information (PHI) exposure, data leakage, and vulnerability to cyberthreats - which are especially critical in hospital environments. Even when adopted for research or non-clinical purposes, healthcare organizations must exercise caution and implement safeguards. To address these challenges, we present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1) enables physicians to manually select and use different models without programming expertise, (2) supports engineers in efficiently deploying medical VLMs in a plug-and-play fashion, with seamless integration of Hugging Face open-source models, and (3) ensures privacy-preserving inference through Docker-orchestrated, operating system agnostic deployment. MedFoundationHub requires only an offline local workstation equipped with a single NVIDIA A6000 GPU, making it both secure and accessible within the typical resources of academic research labs. To evaluate current capabilities, we engaged board-certified pathologists to deploy and assess five state-of-the-art VLMs (Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases, yielding 1015 clinician-model scoring events. These assessments revealed recurring limitations, including off-target answers, vague reasoning, and inconsistent pathology terminology.",
    "score": 6,
    "reason": "该论文聚焦于医疗视觉语言模型的部署工具包，虽涉及VLM在医学文档理解中的应用潜力，但核心贡献是安全部署框架而非DIU任务本身。其应用场景局限于医疗病理图像，与通用文档图像理解（如账单、简历）关联较弱；且未直接解决OCR、实体识别或关系抽取等DIU关键问题。尽管使用了Hugging Face模型并支持本地部署，但缺乏对DIU流程中关键环节（如布局分析、结构化信息抽取）的技术创新。虽然具备一定的实践价值，但可迁移性有限，仅对特定领域（医疗）的VLM部署有参考意义。",
    "summary": "MedFoundationHub是一个面向医疗视觉语言模型的轻量级、安全的部署工具包，提供图形化界面支持非编程人员使用多种VLM，并通过Docker实现隐私保护的本地化部署。实验评估了五种先进VLM在结肠和肾脏病理图像上的表现，发现存在答非所问、推理模糊和术语不一致等问题。"
  },
  {
    "id": "http://arxiv.org/abs/2408.04631v2",
    "updated": "2025-08-28T01:30:18Z",
    "published": "2024-08-08T17:59:38Z",
    "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2408.04631v2.pdf",
    "comment": "Accepted at ICCV 2025. Project page: https://vgg-puppetmaster.github.io/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce Puppet-Master, an interactive video generator that captures the internal, part-level motion of objects, serving as a proxy for modeling object dynamics universally. Given an image of an object and a set of \"drags\" specifying the trajectory of a few points on the object, the model synthesizes a video where the object's parts move accordingly. To build Puppet-Master, we extend a pre-trained image-to-video generator to encode the input drags. We also propose all-to-first attention, an alternative to conventional spatial attention that mitigates artifacts caused by fine-tuning a video generator on out-of-domain data. The model is fine-tuned on Objaverse-Animation-HQ, a new dataset of curated part-level motion clips obtained by rendering synthetic 3D animations. Unlike real videos, these synthetic clips avoid confounding part-level motion with overall object and camera motion. We extensively filter sub-optimal animations and augment the synthetic renderings with meaningful drags that emphasize the internal dynamics of objects. We demonstrate that Puppet-Master learns to generate part-level motions, unlike other motion-conditioned video generators that primarily move the object as a whole. Moreover, Puppet-Master generalizes well to out-of-domain real images, outperforming existing methods on real-world benchmarks in a zero-shot manner.",
    "score": 2,
    "reason": "该论文聚焦于基于交互式拖拽的视频生成，重点在于物体局部运动建模与合成，属于视频生成与动态建模领域。尽管其使用了多模态输入（图像+拖拽点）并涉及视觉理解，但其核心目标是生成具有物理合理性的动态视频，而非文档信息理解。与DIU任务无直接关联，未涉及OCR、实体识别、关系抽取或文档布局分析。虽然引入了创新的注意力机制和高质量合成数据集，但其技术栈与DIU无关，无法直接迁移至文档图像理解任务。此外，不涉及VLM推理增强、inference scaling或agent系统设计。因此，相关性极低。",
    "summary": "Puppet-Master提出一种交互式视频生成模型，通过用户拖拽指定物体上的关键点轨迹，生成体现物体内部局部运动的视频。模型基于预训练图像到视频生成器，引入all-to-first注意力机制以缓解微调时的伪影问题，并在自研的Objaverse-Animation-HQ数据集上训练，该数据集包含由3D动画渲染的高保真部分级运动片段。实验表明，该模型能有效捕捉物体内部动态，且在真实图像上具备零样本泛化能力，优于现有方法。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19575v2",
    "updated": "2025-08-28T01:28:08Z",
    "published": "2025-08-27T05:15:16Z",
    "title": "Interact-Custom: Customized Human Object Interaction Image Generation",
    "authors": [
      "Zhu Xu",
      "Zhaowen Wang",
      "Yuxin Peng",
      "Yang Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19575v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild application. Existing approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target entities. To enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between them. Two primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction semantics. To tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive poses. Then we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities features. Furthermore, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach.",
    "score": 2,
    "reason": "该论文聚焦于人-物交互图像生成，属于计算机视觉中的图像合成方向，与文档图像理解（DIU）无直接关联。其核心任务是控制人物与物体之间的交互姿态和身份保留，涉及的HOI数据集、空间配置建模等技术无法直接迁移至DIU场景。尽管模型设计中提到两阶段生成与掩码引导，但其目标是生成逼真的人物交互图像，而非文本识别、实体抽取或关系建模。此外，未提及多模态模型、推理增强或agent架构，也未开源或发表于CCF A类会议。因此不值得阅读。",
    "summary": "本文提出Customized Human Object Interaction Image Generation (CHOI)任务，旨在同时保持目标人物与物体的身份特征并控制其交互语义。为此，作者构建了一个包含多种交互姿态的大规模数据集，并设计了两阶段模型Interact-Custom：第一阶段生成描述交互行为的前景掩码，第二阶段在掩码引导下生成符合交互语义且身份保留的图像。支持用户指定背景图和物体位置，提升内容可控性。实验表明该方法在自定义指标上优于基线。"
  },
  {
    "id": "http://arxiv.org/abs/2407.14209v2",
    "updated": "2025-08-28T01:17:16Z",
    "published": "2024-07-19T11:15:02Z",
    "title": "Unlearning Concepts from Text-to-Video Diffusion Models",
    "authors": [
      "Shiqi Liu",
      "Yihua Tan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2407.14209v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "With the advancement of computer vision and natural language processing, text-to-video generation, enabled by text-to-video diffusion models, has become more prevalent. These models are trained using a large amount of data from the internet. However, the training data often contain copyrighted content, including cartoon character icons and artist styles, private portraits, and unsafe videos. Since filtering the data and retraining the model is challenging, methods for unlearning specific concepts from text-to-video diffusion models have been investigated. However, due to the high computational complexity and relative large optimization scale, there is little work on unlearning methods for text-to-video diffusion models. We propose a novel concept-unlearning method by transferring the unlearning capability of the text encoder of text-to-image diffusion models to text-to-video diffusion models. Specifically, the method optimizes the text encoder using few-shot unlearning, where several generated images are used. We then use the optimized text encoder in text-to-video diffusion models to generate videos. Our method costs low computation resources and has small optimization scale. We discuss the generated videos after unlearning a concept. The experiments demonstrates that our method can unlearn copyrighted cartoon characters, artist styles, objects and people's facial characteristics. Our method can unlearn a concept within about 100 seconds on an RTX 3070. Since there was no concept unlearning method for text-to-video diffusion models before, we make concept unlearning feasible and more accessible in the text-to-video domain.",
    "score": 2,
    "reason": "该论文研究的是文本到视频扩散模型中的概念遗忘（unlearning），主要针对版权内容（如卡通角色、艺术家风格）的移除。虽然涉及多模态模型和内容安全，但其核心任务与文档图像理解（DIU）无直接关联。技术路径聚焦于文本编码器的微调以实现特定概念的删除，与DIU中OCR、实体识别、关系抽取或视觉-语言建模的任务目标不一致。此外，未涉及推理增强（inference scaling）、agent架构或视觉信息抽取等与DIU高度相关的方向。尽管在计算效率上表现良好，但应用场景与DIU无关，无法直接迁移应用。",
    "summary": "本文提出一种低成本的文本到视频扩散模型中的概念遗忘方法，通过将文本到图像模型的文本编码器遗忘能力迁移至视频生成模型，利用少量样本优化文本编码器，可在RTX 3070上100秒内完成对卡通角色、艺术家风格等特定概念的移除。该方法具有低计算开销和小优化规模的优势，但仅适用于视频生成领域，与文档图像理解（DIU）无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20325v1",
    "updated": "2025-08-28T00:07:10Z",
    "published": "2025-08-28T00:07:10Z",
    "title": "GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs",
    "authors": [
      "Haibo Jin",
      "Ruoxi Chen",
      "Peiyan Zhang",
      "Andy Zhou",
      "Yang Zhang",
      "Haohan Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20325v1.pdf",
    "comment": "54 pages",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance. To address this challenge, we introduce GUARD (\\textbf{G}uideline \\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and Jailbreak \\textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations. We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.",
    "score": 6,
    "reason": "该论文提出GUARD方法用于测试LLM对伦理指南的遵守情况，核心在于通过自适应角色扮演和越狱诊断生成违反指南的问题。虽然其在LLM安全评估方面具有创新性，并且可扩展至视觉语言模型（VLM），但其主要目标是安全测试而非提升推理能力或直接支持文档图像理解任务。尽管提到可迁移至VLM，但未展示在DIU场景中的具体应用或性能增益，与DIU任务的直接关联较弱。因此仅部分相关，适合关注LLM安全方向的研究者，对DIU领域贡献有限。",
    "summary": "GUARD是一种用于评估大型语言模型是否遵守政府伦理指南的测试框架，通过自动化生成违反指南的问题并结合越狱诊断技术检测模型潜在的安全漏洞。实验覆盖多个主流LLM，结果可用于生成合规报告。该方法还可扩展至视觉语言模型，但未在文档图像理解任务中验证其有效性。"
  },
  {
    "id": "http://arxiv.org/abs/2405.14862v2",
    "updated": "2025-08-28T17:59:31Z",
    "published": "2024-05-23T17:59:22Z",
    "title": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs",
    "authors": [
      "Dawid J. Kopiczko",
      "Tijmen Blankevoort",
      "Yuki M. Asano"
    ],
    "pdf_url": "https://arxiv.org/pdf/2405.14862v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning.",
    "score": 6,
    "reason": "该论文提出Bitune方法，在解码器-only LLM中引入双向注意力以提升信息流表达能力，对推理任务（如常识推理、算术）有显著提升。虽然其核心思想与DIU中的多步推理和上下文理解相关，但主要聚焦于LLM架构改进，未直接涉及文档图像理解、视觉模态或agent系统。其在DIU中的可迁移性有限，仅能作为底层语言模型增强技术间接支持，因此评分中等。",
    "summary": "Bitune通过在提示处理阶段引入双向注意力机制，增强decoder-only LLM的表达能力，显著提升在常识推理、算术和语言理解任务上的性能，且兼容多种微调策略。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21051v1",
    "updated": "2025-08-28T17:55:07Z",
    "published": "2025-08-28T17:55:07Z",
    "title": "Enabling Equitable Access to Trustworthy Financial Reasoning",
    "authors": [
      "William Jurayj",
      "Nils Holzenberger",
      "Benjamin Van Durme"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21051v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "According to the United States Internal Revenue Service, ''the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.",
    "score": 7,
    "reason": "该论文提出将LLM与符号求解器结合的神经符号架构，用于税务推理任务，涉及复杂规则应用与数值计算，具有强逻辑推理需求。虽然研究场景为税务，但其核心思想——通过符号系统增强LLM的可审计性、准确性与可解释性——对DIU中需要高可靠性的文档理解（如财务票据、合同）极具借鉴意义。尤其在关系抽取和实体识别需保证准确性的场景下，该方法可直接迁移至DIU中的‘可信推理’模块。尽管未明确提及DIU或视觉信息，但其inference scaling与reasoning增强机制（如形式化规则+检索示例）与你关注的LLM推理技术路线高度契合。未开源且非顶会，故不加分。",
    "summary": "本文提出一种神经符号框架，将大语言模型与符号求解器结合，用于复杂税务规则推理。通过将自然语言税法转化为形式化逻辑程序，并引入智能检索的案例示例，显著提升推理准确率并降低部署成本。实验基于SARA数据集，验证了该方法在高可靠性要求场景下的可行性与经济性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.10175v2",
    "updated": "2025-08-28T17:54:36Z",
    "published": "2025-08-13T20:22:58Z",
    "title": "Estimating Machine Translation Difficulty",
    "authors": [
      "Lorenzo Proietti",
      "Stefano Perrella",
      "Vilém Zouhar",
      "Roberto Navigli",
      "Tom Kocmi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.10175v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Machine translation quality has steadily improved over the years, achieving near-perfect translations in recent benchmarks. These high-quality outputs make it difficult to distinguish between state-of-the-art models and to identify areas for future improvement. In this context, automatically identifying texts where machine translation systems struggle holds promise for developing more discriminative evaluations and guiding future research. In this work, we address this gap by formalizing the task of translation difficulty estimation, defining a text's difficulty based on the expected quality of its translations. We introduce a new metric to evaluate difficulty estimators and use it to assess both baselines and novel approaches. Finally, we demonstrate the practical utility of difficulty estimators by using them to construct more challenging benchmarks for machine translation. Our results show that dedicated models outperform both heuristic-based methods and LLM-as-a-judge approaches, with Sentinel-src achieving the best performance. Thus, we release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which can be used to scan large collections of texts and select those most likely to challenge contemporary machine translation systems.",
    "score": 2,
    "reason": "该论文聚焦于机器翻译难度估计，属于自然语言处理中的翻译质量评估方向，与文档图像理解（DIU）无直接关联。尽管其提出的难度评估思想可能间接启发模型鲁棒性分析，但无法直接应用于DIU任务中的OCR、实体识别或关系抽取。同时，论文未涉及多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）等关键方向，也未使用LLM进行视觉信息抽取或跨模态推理。因此，不值得阅读。",
    "summary": "本文提出机器翻译难度估计任务，旨在自动识别对当前翻译系统具有挑战性的文本。作者定义了基于翻译质量预期的难度指标，设计新评估度量，并比较了多种方法，包括基于规则和大语言模型的判断方式。实验表明，专用模型Sentinel-src-24和Sentinel-src-25表现最佳，并发布两个改进模型用于构建更具挑战性的翻译基准。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21049v1",
    "updated": "2025-08-28T17:54:35Z",
    "published": "2025-08-28T17:54:35Z",
    "title": "Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm",
    "authors": [
      "Ramazan Ali Bahrami",
      "Ramin Yahyapour"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21049v1.pdf",
    "comment": "Presented in 8th International Conference on Natural Language and Speech Processing (ICNLSP), 25-27 August 2025, SDU, Odense, Denmark",
    "category": "Computation and Language",
    "abstract": "Sentential relation extraction (RE) is an important task in natural language processing (NLP). In this paper we propose to do sentential RE with dynamic routing in capsules. We first show that the proposed approach outperform state of the art on common sentential relation extraction datasets Tacred, Tacredrev, Retacred, and Conll04. We then investigate potential reasons for its good performance on the mentioned datasets, and yet low performance on another similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise in Wikidata labels as one of the reasons that can hinder performance. Additionally, we show associativity of better performance with better re-representation, a term from neuroscience referred to change of representation in human brain to improve the match at comparison time. As example, in the given analogous terms King:Queen::Man:Woman, at comparison time, and as a result of re-representation, the similarity between related head terms (King,Man), and tail terms (Queen,Woman) increases. As such, our observation show that our proposed model can do re-representation better than the vanilla model compared with. To that end, beside noise in the labels of the distantly supervised RE datasets, we propose re-representation as a challenge in sentential RE.",
    "score": 3,
    "reason": "该论文聚焦于句法级关系抽取中的动态胶囊路由与重表示机制，虽在NLP领域有一定创新性，但其研究对象为纯文本关系抽取，未涉及文档图像理解（DIU）中关键的视觉模态、布局信息或OCR后处理任务。此外，论文发表于ICNLSP（非CCF A类会议），且无开源声明。虽然‘重表示’概念与DIU中对语义表征优化的需求存在间接启发，但无法直接应用于DIU系统中的多模态理解、实体对齐或结构推理环节，因此相关性较弱。",
    "summary": "本文提出基于胶囊网络动态路由的句法级关系抽取方法，通过引入神经科学中的‘重表示’机制提升模型对语义关系的匹配能力，在Tacred等数据集上表现优于现有方法，但对Wikidata数据集性能下降归因于标签噪声。研究集中于纯文本关系抽取，未涉及视觉信息或文档结构建模。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21038v1",
    "updated": "2025-08-28T17:43:53Z",
    "published": "2025-08-28T17:43:53Z",
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "authors": [
      "Orion Weller",
      "Michael Boratko",
      "Iftekhar Naim",
      "Jinhyuk Lee"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21038v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.",
    "score": 3,
    "reason": "该论文探讨嵌入检索的理论局限性，虽在理论上对向量表示有深刻洞察，但其核心关注点是通用检索任务中的嵌入维度限制，与DIU、VLM推理增强或agent系统无直接关联。尽管涉及‘推理’相关概念，但其问题设定为抽象的文档检索，而非视觉-文本联合理解或结构化信息抽取。未提及多模态、布局感知、表格解析或DIU特定任务，也未开源或发表于顶会。因此，虽具理论价值，但无法直接迁移至DIU领域，相关性较低。",
    "summary": "本文揭示了基于嵌入的检索方法在理论上的根本限制：即使在简单查询场景下，高维嵌入空间也无法支持所有可能的top-k文档子集返回。作者构建了名为LIMIT的数据集，验证了当前SOTA模型在此类任务中仍失败。研究指出，现有单向量嵌入范式存在不可逾越的瓶颈，呼吁发展新方法。"
  },
  {
    "id": "http://arxiv.org/abs/2203.13722v3",
    "updated": "2025-08-28T17:30:05Z",
    "published": "2022-03-25T15:45:49Z",
    "title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in Values",
    "authors": [
      "Arnav Arora",
      "Lucie-Aimée Kaffee",
      "Isabelle Augenstein"
    ],
    "pdf_url": "https://arxiv.org/pdf/2203.13722v3.pdf",
    "comment": "Accepted to C3NLP, EACL 2023: https://aclanthology.org/2023.c3nlp-1.12/",
    "category": "Computation and Language",
    "abstract": "Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.",
    "score": 3,
    "reason": "该论文研究预训练语言模型中跨文化价值观的表征，虽涉及语言模型与社会文化的关系，但核心关注点为价值观偏见分析，与文档图像理解（DIU）、视觉多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）技术无直接关联。其方法和目标不适用于DIU任务中的文本识别、实体关系抽取或系统级架构设计。尽管发表于EACL 2023（CCF B类），但内容不具可迁移性，故仅作基础相关性评分。",
    "summary": "本文通过设计探针分析预训练语言模型中跨文化价值观的编码情况，发现模型虽能捕捉文化差异，但与主流价值理论对齐度较低，强调了在跨文化应用中使用此类模型的风险。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19200v2",
    "updated": "2025-08-28T17:29:36Z",
    "published": "2025-08-26T17:03:43Z",
    "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
    "authors": [
      "Xinran Zhao",
      "Boyuan Zheng",
      "Chenglei Si",
      "Haofei Yu",
      "Ken Liu",
      "Runlong Zhou",
      "Ruochen Li",
      "Tong Chen",
      "Xiang Li",
      "Yiming Zhang",
      "Tongshuang Wu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19200v2.pdf",
    "comment": "21 pages, 3 figures",
    "category": "Computation and Language",
    "abstract": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.",
    "score": 3,
    "reason": "该论文探讨的是基于拉蒙·柳利的组合逻辑框架构建AI辅助科研创意生成系统，虽涉及LLM与思维链式推理的结合，但核心聚焦于科研创新的启发机制，而非文档图像理解（DIU）、多模态模型推理优化或智能体系统构建。其提出的‘三轴组合’方法与DIU任务无直接关联，也未涉及视觉信息抽取、布局理解或实体关系建模等关键环节。尽管使用了LLM进行创意生成，但应用场景为科研选题建议，无法直接迁移至DIU领域。因此不具直接应用价值。",
    "summary": "本文受中世纪哲学家拉蒙·柳利的组合艺术启发，提出一种用于自动化科研创意生成的‘柳利思维机器’，通过主题、领域和方法三个维度的符号组合来激发新颖研究想法。作者利用LLM对已有论文进行元素提取并生成组合提示，验证了其在提升创意多样性与文献相关性方面的有效性。该系统旨在增强人类科研创造力，属于AI辅助创新范畴，与DIU、VLM推理优化及agent系统构建无直接技术关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21024v1",
    "updated": "2025-08-28T17:27:09Z",
    "published": "2025-08-28T17:27:09Z",
    "title": "An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs",
    "authors": [
      "Mathieu Bourdin",
      "Anas Neumann",
      "Thomas Paviot",
      "Robert Pellerin",
      "Samir Lamouri"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21024v1.pdf",
    "comment": "20 pages, 3 figures",
    "category": "Computation and Language",
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to mitigate the limitations of Large Language Models (LLMs), such as hallucinations and outdated knowledge. However, deploying RAG-based tools in Small and Medium Enterprises (SMEs) remains a challenge due to their limited resources and lack of expertise in natural language processing (NLP). This paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a structured, agile method designed to facilitate the deployment of RAG systems in industrial SME contexts. EASI-RAG is based on method engineering principles and comprises well-defined roles, activities, and techniques. The method was validated through a real-world case study in an environmental testing laboratory, where a RAG tool was implemented to answer operators queries using data extracted from operational procedures. The system was deployed in under a month by a team with no prior RAG experience and was later iteratively improved based on user feedback. Results demonstrate that EASI-RAG supports fast implementation, high user adoption, delivers accurate answers, and enhances the reliability of underlying data. This work highlights the potential of RAG deployment in industrial SMEs. Future works include the need for generalization across diverse use cases and further integration with fine-tuned models.",
    "score": 3,
    "reason": "该论文聚焦于RAG在工业中小企业中的部署方法，虽涉及RAG技术，但核心是流程方法论与工程落地，而非DIU、VLM或LLM推理增强/agent等前沿方向。其内容与文档图像理解无直接关联，未涉及OCR、实体识别、关系抽取或多模态模型应用。尽管RAG是相关技术，但本文不提供可迁移至DIU领域的技术创新或模型架构突破，且未开源、非顶会，故不具高价值。",
    "summary": "本文提出EASI-RAG方法，旨在帮助工业中小企业快速部署RAG系统。基于敏捷开发原则，定义了角色、活动与技术流程，在环境检测实验室成功实现RAG工具用于回答操作员问题。系统在一个月内完成部署，用户反馈良好，但研究重点为组织方法而非模型技术，与DIU、VLM推理增强或agent系统无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2301.06375v2",
    "updated": "2025-08-28T17:12:29Z",
    "published": "2023-01-16T11:40:50Z",
    "title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset",
    "authors": [
      "Jeongkyun Park",
      "Jung-Wook Hwang",
      "Kwanghee Choi",
      "Seung-Hyun Lee",
      "Jun Hwan Ahn",
      "Rae-Hong Park",
      "Hyung-Min Park"
    ],
    "pdf_url": "https://arxiv.org/pdf/2301.06375v2.pdf",
    "comment": "Accepted to ICASSP 2024",
    "category": "Computation and Language",
    "abstract": "Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the OLKAVS dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis.",
    "score": 2,
    "reason": "该论文聚焦于韩语语音-视觉语音数据集的构建，属于多模态音频-视觉语音识别领域，与文档图像理解（DIU）无直接关联。尽管涉及多模态数据，但其任务为语音识别与唇读，应用场景和输入模态（音频+视频）均与DIU无关。虽然被ICASSP 2024接收（CCF B类），且数据规模较大并开源，但无法直接应用于DIU、VLM推理增强或agent系统中，因此相关性极低。",
    "summary": "OLKAVS是一个大规模韩语音视频语音数据集，包含1150小时来自1107位说话者的多视角录音与视频，涵盖多种噪声环境。研究者还提供了用于语音识别和唇读的预训练基线模型，并验证了多视角多模态训练的优势。该数据集旨在推动韩语语音识别、说话人识别等方向的研究。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21010v1",
    "updated": "2025-08-28T17:10:53Z",
    "published": "2025-08-28T17:10:53Z",
    "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Basura Fernando"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21010v1.pdf",
    "comment": "Project page: https://paritoshparmar.github.io/chainreaction/",
    "category": "Computation and Language",
    "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/",
    "score": 4,
    "reason": "该论文聚焦于因果视频问答中的推理链构建，虽引入了可解释的中间表示（因果链），但研究对象为视频理解而非文档图像理解。尽管其提出的‘结构化推理链’思想与DIU中提升逻辑推理能力有潜在关联，但应用场景（视频）与DIU（文档）差异显著，且未涉及OCR、实体识别、布局分析等DIU核心组件。虽然使用LLM生成推理链的方法有一定借鉴意义，但无法直接迁移至DIU任务，因此相关性较弱。",
    "summary": "本文提出一种基于自然语言因果链的两阶段视频问答框架，通过分离因果推理与答案生成，利用大语言模型从现有数据集中自动构建高质量因果链作为中间表示，以提升模型的可解释性和泛化能力。引入新评估指标CauCo用于衡量因果描述质量，在多个基准上优于现有方法。"
  },
  {
    "id": "http://arxiv.org/abs/2508.21004v1",
    "updated": "2025-08-28T17:05:18Z",
    "published": "2025-08-28T17:05:18Z",
    "title": "Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution",
    "authors": [
      "Chen Chen",
      "Yuchen Sun",
      "Jiaxin Gao",
      "Xueluan Gong",
      "Qian Wang",
      "Ziyao Wang",
      "Yongsen Zheng",
      "Kwok-Yan Lam"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21004v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) have seen significant advancements, achieving superior performance in various Natural Language Processing (NLP) tasks. However, they remain vulnerable to backdoor attacks, where models behave normally for standard queries but generate harmful responses or unintended output when specific triggers are activated. Existing backdoor defenses either lack comprehensiveness, focusing on narrow trigger settings, detection-only mechanisms, and limited domains, or fail to withstand advanced scenarios like model-editing-based, multi-trigger, and triggerless attacks. In this paper, we present LETHE, a novel method to eliminate backdoor behaviors from LLMs through knowledge dilution using both internal and external mechanisms. Internally, LETHE leverages a lightweight dataset to train a clean model, which is then merged with the backdoored model to neutralize malicious behaviors by diluting the backdoor impact within the model's parametric memory. Externally, LETHE incorporates benign and semantically relevant evidence into the prompt to distract LLM's attention from backdoor features. Experimental results on classification and generation domains across 5 widely used LLMs demonstrate that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor attacks. LETHE reduces the attack success rate of advanced backdoor attacks by up to 98% while maintaining model utility. Furthermore, LETHE has proven to be cost-efficient and robust against adaptive backdoor attacks.",
    "score": 3,
    "reason": "该论文聚焦于大语言模型的后门防御，提出通过知识稀释（knowledge dilution）来净化被攻击的LLM。虽然其技术对LLM的安全性有重要意义，但与文档图像理解（DIU）、视觉多模态模型（VLM）的推理增强、或智能体系统在DIU中的应用无直接关联。其方法主要面向NLP任务中的安全问题，无法直接迁移至DIU场景下的视觉-文本联合理解、布局分析或结构化信息抽取等核心需求。因此，尽管在LLM安全领域具有价值，但对本研究方向的直接贡献有限。",
    "summary": "LETHE提出一种新型后门防御方法，通过内部轻量数据训练纯净模型并融合到受污染模型中，同时在外部使用良性提示干扰模型对恶意触发器的关注，从而有效消除多种高级后门攻击的影响。实验表明其在分类和生成任务上显著降低攻击成功率，且保持模型性能，具备成本效益和抗适应攻击能力。"
  },
  {
    "id": "http://arxiv.org/abs/2405.07764v4",
    "updated": "2025-08-28T16:44:35Z",
    "published": "2024-05-13T14:07:15Z",
    "title": "LGDE: Local Graph-based Dictionary Expansion",
    "authors": [
      "Juni Schindler",
      "Sneha Jha",
      "Xixuan Zhang",
      "Kilian Buehling",
      "Annett Heft",
      "Mauricio Barahona"
    ],
    "pdf_url": "https://arxiv.org/pdf/2405.07764v4.pdf",
    "comment": "Python code available at: https://github.com/barahona-research-group/LGDE",
    "category": "Computation and Language",
    "abstract": "We present Local Graph-based Dictionary Expansion (LGDE), a method for data-driven discovery of the semantic neighbourhood of words using tools from manifold learning and network science. At the heart of LGDE lies the creation of a word similarity graph from the geometry of word embeddings followed by local community detection based on graph diffusion. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings to capture word similarities based on paths of semantic association, over and above direct pairwise similarities. Exploiting such semantic neighbourhoods enables the expansion of dictionaries of pre-selected keywords, an important step for tasks in information retrieval, such as database queries and online data collection. We validate LGDE on two user-generated English-language corpora and show that LGDE enriches the list of keywords with improved performance relative to methods based on direct word similarities or co-occurrences. We further demonstrate our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on the expansion of a conspiracy-related dictionary from online data collected and analysed by domain experts. Our empirical results and expert user assessment indicate that LGDE expands the seed dictionary with more useful keywords due to the manifold-learning-based similarity network.",
    "score": 6,
    "reason": "LGDE 提出了一种基于局部图的词典扩展方法，利用流形学习和网络科学挖掘词语语义邻域，对 DIU 中的实体识别（如开放域实体识别）有一定潜在价值，尤其在关键词扩展和语义关联建模方面。其核心思想可辅助提升 OCR 后文本的语义理解能力，但该方法本质上是针对通用语义邻域发现的，未直接面向文档图像理解任务，也未结合视觉信息或布局结构。虽然代码开源且发表于计算语言学领域（非CCF A类），但与 DIU 的直接关联较弱，仅具间接启发性。",
    "summary": "LGDE 是一种基于词嵌入几何结构构建词相似性图并利用图扩散进行局部社区检测的方法，用于数据驱动地扩展关键词词典。该方法通过捕捉词语间的路径式语义关联，优于传统基于共现或直接相似度的方法，在通信科学中的阴谋论词典扩展中表现良好，代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2507.22931v2",
    "updated": "2025-08-28T16:42:39Z",
    "published": "2025-07-24T13:46:51Z",
    "title": "Dynamic Context Compression for Efficient RAG",
    "authors": [
      "Shuyu Guo",
      "Zhaochun Ren"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.22931v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.",
    "score": 8,
    "reason": "该论文提出动态上下文压缩框架ACC-RAG，针对RAG中因检索上下文过长导致的推理开销问题，通过根据输入复杂度自适应调整压缩率，实现高效推理。其核心思想——动态信息保留与人类‘略读’行为类比，具有很强的可迁移性。在DIU任务中，当需要从文档图像中提取大量文本并进行语义问答时，RAG常被用于增强LLM理解能力，而此方法可显著降低推理延迟，提升效率。虽然未直接面向DIU，但其技术可直接应用于基于VLM+LLM的DIU系统中的知识检索与生成模块，尤其适合处理多文档、长篇幅（如合同、报告）的理解场景。具备较强的应用潜力，值得重点关注。",
    "summary": "本文提出ACC-RAG，一种用于检索增强生成（RAG）的自适应上下文压缩框架。它根据输入查询的复杂度动态调整压缩率，结合分层压缩器与上下文选择机制，在保证准确性的前提下显著提升推理效率。在多个基准数据集上，相比固定压缩率方法表现更优，且推理速度提升超过4倍。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20973v1",
    "updated": "2025-08-28T16:26:44Z",
    "published": "2025-08-28T16:26:44Z",
    "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents",
    "authors": [
      "Tianjian Liu",
      "Fanqi Wan",
      "Jiajian Guo",
      "Xiaojun Quan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20973v1.pdf",
    "comment": "21 pages, 6 Figures",
    "category": "Computation and Language",
    "abstract": "Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.",
    "score": 6,
    "reason": "该论文聚焦于主动对话代理的评估框架，虽涉及LLM推理与规划能力，但核心是对话系统评价，与DIU任务中的文档理解、信息抽取、布局建模等无直接关联。尽管其提出的‘目标规划’和‘对话引导’机制可能间接启发DIU agent的规划模块，但缺乏对视觉信息、文档结构或实体关系处理的直接支持，迁移应用难度大。未提及开源或顶会发表，故不加分。",
    "summary": "本文提出ProactiveEval，一个统一的主动对话代理评估框架，将主动对话分解为目标规划与对话引导两个维度，并构建了328个跨领域评估环境。通过实验对比22种LLM，发现DeepSeek-R1在目标规划上表现优异，Claude-3.7-Sonnet在对话引导上更优。研究还探讨了推理能力对主动行为的影响，为未来模型设计提供参考。"
  },
  {
    "id": "http://arxiv.org/abs/2504.07612v2",
    "updated": "2025-08-28T16:22:13Z",
    "published": "2025-04-10T10:03:29Z",
    "title": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline Dataset",
    "authors": [
      "Mihnea-Alexandru Vîrlan",
      "Răzvan-Alexandru Smădu",
      "Dumitru-Clementin Cercel",
      "Florin Pop",
      "Mihaela-Claudia Cercel"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.07612v2.pdf",
    "comment": "13 pages, 2 figures",
    "category": "Computation and Language",
    "abstract": "The primary goal of a news headline is to summarize an event in as few words as possible. Depending on the media outlet, a headline can serve as a means to objectively deliver a summary or improve its visibility. For the latter, specific publications may employ stylistic approaches that incorporate the use of sarcasm, irony, and exaggeration, key elements of a satirical approach. As such, even the headline must reflect the tone of the satirical main content. Current approaches for the Romanian language tend to detect the non-conventional tone (i.e., satire and clickbait) of the news content by combining both the main article and the headline. Because we consider a headline to be merely a brief summary of the main article, we investigate in this paper the presence of satirical tone in headlines alone, testing multiple baselines ranging from standard machine learning algorithms to deep learning models. Our experiments show that Bidirectional Transformer models outperform both standard machine-learning approaches and Large Language Models (LLMs), particularly when the meta-learning Reptile approach is employed.",
    "score": 2,
    "reason": "该论文研究罗马尼亚语新闻标题中的讽刺检测，属于自然语言处理中情感与风格分析的子任务，与文档图像理解（DIU）无直接关联。其关注点为文本语义中的讽刺识别，不涉及文档布局、OCR、实体识别或关系抽取等DIU核心环节。尽管使用了Transformer和LLM，但应用场景是新闻内容分析而非视觉文档理解。此外，未提及多模态、视觉信息抽取或可迁移至DIU的技术路径，因此不值得在DIU研究中优先阅读。",
    "summary": "本文研究在罗马尼亚新闻标题中独立检测讽刺语气的任务，对比了多种机器学习与深度学习模型，发现双向Transformer结合元学习Reptile方法在该任务上表现最优。研究聚焦于纯文本层面的讽刺识别，未涉及视觉信息或文档结构分析。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20944v1",
    "updated": "2025-08-28T16:04:39Z",
    "published": "2025-08-28T16:04:39Z",
    "title": "STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment",
    "authors": [
      "Jiaqian Li",
      "Qisheng Hu",
      "Jing Li",
      "Wenya Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20944v1.pdf",
    "comment": "EMNLP 2025 Main",
    "category": "Computation and Language",
    "abstract": "In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to perform a wide range of tasks without task-specific fine-tuning. However, the effectiveness of ICL heavily depends on the quality of exemplar selection. In particular, for structured prediction tasks such as semantic parsing, existing ICL selection strategies often overlook structural alignment, leading to suboptimal performance and poor generalization. To address this issue, we propose a novel two-stage exemplar selection strategy that achieves a strong balance between efficiency, generalizability, and performance. First, we fine-tune a BERT-based retriever using structure-aware supervision, guiding it to select exemplars that are both semantically relevant and structurally aligned. Then, we enhance the retriever with a plug-in module, which amplifies syntactically meaningful information in the hidden representations. This plug-in is model-agnostic, requires minimal overhead, and can be seamlessly integrated into existing pipelines. Experiments on four benchmarks spanning three semantic parsing tasks demonstrate that our method consistently outperforms existing baselines with multiple recent LLMs as inference-time models.",
    "score": 9,
    "reason": "该论文聚焦于In-Context Learning（ICL）中示例选择的关键问题，提出基于结构对齐的两阶段策略，显著提升结构化预测任务（如语义解析）的性能。其核心思想——通过结构感知监督和可插拔模块增强模型对语法结构信息的捕捉能力——与DIU中的布局理解、字段关系建模高度相关。尤其在DIU中，文档结构（如表单布局、层级关系）是关键线索，该方法可直接迁移用于优化OCR后实体识别与关系抽取中的上下文示例选择。EMNLP 2025主会为CCF A类会议，且论文具有开源潜力（虽未明说但方法具普适性），对DIU领域有较强启发性和可应用性。",
    "summary": "本文提出一种基于结构对齐的两阶段示例选择方法，用于提升LLM在结构化预测任务中的In-Context Learning性能。首先使用结构感知监督微调BERT检索器以选择语义与结构均匹配的示例；其次引入一个轻量级、模型无关的插件模块，增强隐藏表示中的句法信息。在多个语义解析基准上验证了方法的有效性，显著优于现有基线，适用于DIU中复杂文档结构的理解任务。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20931v1",
    "updated": "2025-08-28T15:57:33Z",
    "published": "2025-08-28T15:57:33Z",
    "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench",
    "authors": [
      "Venkatesh Mishra",
      "Amir Saeidi",
      "Satyam Raj",
      "Mutsumi Nakamura",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Ali Payani",
      "Chitta Baral"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20931v1.pdf",
    "comment": "Accepted to EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like $\\tau$-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.",
    "score": 9,
    "reason": "该论文聚焦于复杂动态环境中大语言模型的工具调用准确率问题，提出Input-Reformulation Multi-Agent (IRMA)框架，通过重构输入以融入领域规则和工具建议，显著提升代理在多轮对话中的推理一致性与准确性。其核心思想——通过输入重写增强LLM在复杂任务中的可控性与可靠性——可直接迁移至DIU Agent系统中，用于优化文档理解过程中的工具选择、信息提取与决策路径。EMNLP 2025 Findings为CCF B类会议，具有较高学术价值；且论文强调多轮交互与长期依赖处理，与DIU中跨字段关系推理、多步骤信息抽取高度契合。虽未明确提及视觉或文档，但其方法论对构建高鲁棒性的DIU智能体极具启发性。",
    "summary": "本文针对大语言模型在动态多轮交互环境（如τ-bench）中工具调用不一致的问题，提出IRMA框架，通过自动重构用户查询并注入领域规则与工具建议来提升代理决策质量。实验表明，IRMA在pass^5指标上优于ReAct、Function Calling和Self-Reflection分别16.1%、12.7%和19.1%，显著增强了长期推理的可靠性。该方法对构建具备稳定推理能力的DIU智能体具有直接借鉴意义。"
  },
  {
    "id": "http://arxiv.org/abs/2508.11017v2",
    "updated": "2025-08-28T15:51:55Z",
    "published": "2025-08-14T18:44:13Z",
    "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
    "authors": [
      "Carter Blum",
      "Katja Filippova",
      "Ann Yuan",
      "Asma Ghandeharioun",
      "Julian Zimmert",
      "Fred Zhang",
      "Jessica Hoffmann",
      "Tal Linzen",
      "Martin Wattenberg",
      "Lucas Dixon",
      "Mor Geva"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11017v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.",
    "score": 6,
    "reason": "该论文研究的是多语言模型中跨语言知识迁移的统一表征机制，核心贡献在于揭示了统一表示对跨语言推理的重要性，并提出调控方法。虽然其研究对象是LLM的预训练动态，但其发现对DIU中多语言文档理解（如跨国账单、多语种简历）具有潜在启发意义。然而，该工作基于小规模合成数据训练，且未直接涉及视觉模态或文档结构，与DIU的直接应用关联较弱；虽属LLM基础研究，但距离可迁移至DIU系统仍有较大距离。因此仅具间接价值。",
    "summary": "本文通过在合成多语言数据上从头训练小型Transformer模型，研究了跨语言知识表示的统一性如何影响模型的跨语言迁移能力。发现统一表征是实现有效跨语言推理的关键，且其程度受语言信息易提取性和事实-语言互信息的影响。作者提出调控策略和可视化工具以量化统一性，为改进多语言LLM提供了新思路。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20916v1",
    "updated": "2025-08-28T15:47:37Z",
    "published": "2025-08-28T15:47:37Z",
    "title": "SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement",
    "authors": [
      "Yuan Ge",
      "Junxiang Zhang",
      "Xiaoqian Liu",
      "Bei Li",
      "Xiangnan Ma",
      "Chenglong Wang",
      "Kaiyang Ye",
      "Yangfan Du",
      "Linfeng Zhang",
      "Yuxin Huang",
      "Tong Xiao",
      "Zhengtao Yu",
      "JingBo Zhu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20916v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling end-to-end spoken dialogue systems. However, evaluating these models remains a fundamental challenge. We propose \\texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches that disregard acoustic features, SageLM jointly assesses both semantic and acoustic dimensions. Second, it leverages rationale-based supervision to enhance explainability and guide model learning, achieving superior alignment with evaluation outcomes compared to rule-based reinforcement learning methods. Third, we introduce \\textit{SpeechFeedback}, a synthetic preference dataset, and employ a two-stage training paradigm to mitigate the scarcity of speech preference data. Trained on both semantic and acoustic dimensions, SageLM achieves an 82.79\\% agreement rate with human evaluators, outperforming cascaded and SLM-based baselines by at least 7.42\\% and 26.20\\%, respectively.",
    "score": 3,
    "reason": "该论文聚焦于语音评估任务，提出SageLM用于语音到语音大模型的多维度评价，虽涉及LLM与可解释性设计，但核心应用场景为语音对话系统评估，与文档图像理解（DIU）无直接关联。尽管其采用的‘理由引导监督’和‘两阶段训练’可能对DIU中的推理链构建有间接启发，但整体技术栈不适用于视觉-文本联合理解任务。未提及多模态、布局分析或文档结构处理，且未开源，因此相关性较低。",
    "summary": "SageLM是一种用于语音到语音大语言模型综合评估的端到端多维度可解释模型，同时考虑语义与声学维度，通过理性引导监督和合成偏好数据集提升评估一致性，达成82.79%的人类评估一致率。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20893v1",
    "updated": "2025-08-28T15:22:31Z",
    "published": "2025-08-28T15:22:31Z",
    "title": "The Uneven Impact of Post-Training Quantization in Machine Translation",
    "authors": [
      "Benjamin Marie",
      "Atsushi Fujita"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20893v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Quantization is essential for deploying large language models (LLMs) on resource-constrained hardware, but its implications for multilingual tasks remain underexplored. We conduct the first large-scale evaluation of post-training quantization (PTQ) on machine translation across 55 languages using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that while 4-bit quantization often preserves translation quality for high-resource languages and large models, significant degradation occurs for low-resource and typologically diverse languages, particularly in 2-bit settings. We compare four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing that algorithm choice and model size jointly determine robustness. GGUF variants provide the most consistent performance, even at 2-bit precision. Additionally, we quantify the interactions between quantization, decoding hyperparameters, and calibration languages, finding that language-matched calibration offers benefits primarily in low-bit scenarios. Our findings offer actionable insights for deploying multilingual LLMs for machine translation under quantization constraints, especially in low-resource settings.",
    "score": 3,
    "reason": "该论文聚焦于机器翻译中的后训练量化（PTQ）影响，虽涉及大语言模型部署与量化技术，但研究主题为多语言机器翻译，与文档图像理解（DIU）、视觉多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）无直接关联。其内容无法直接迁移至DIU任务，尤其未涉及视觉信息、文档布局理解、实体关系抽取或基于多模态的端到端推理。尽管量化是LLM部署的关键技术，但本文未触及DIU场景下的核心挑战，如图文对齐、结构化信息提取或跨模态推理，因此相关性较低。",
    "summary": "本文系统评估了后训练量化（PTQ）在55种语言上的机器翻译性能，涵盖1.7B至70B参数的五种LLM，发现低资源和语言类型多样性的语言在2-bit量化下显著退化，GGUF表现最稳定。同时分析了量化算法、解码超参与校准语言的影响，强调语言匹配校准在低比特下的重要性。"
  },
  {
    "id": "http://arxiv.org/abs/2507.17232v2",
    "updated": "2025-08-28T15:15:18Z",
    "published": "2025-07-23T05:56:20Z",
    "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task",
    "authors": [
      "Mashiro Toyooka",
      "Kiyoharu Aizawa",
      "Yoko Yamakata"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.17232v2.pdf",
    "comment": "Accepted to ACM Multimedia 2025. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
    "category": "Computation and Language",
    "abstract": "Large Language Models (LLMs) are trained on a vast amount of procedural texts, but they do not directly observe real-world phenomena. In the context of cooking recipes, this poses a challenge, as intermediate states of ingredients are often omitted, making it difficult for models to track ingredient states and understand recipes accurately. In this paper, we apply state probing, a method for evaluating a language model's understanding of the world, to the domain of cooking. We propose a new task and dataset for evaluating how well LLMs can recognize intermediate ingredient states during cooking procedures. We first construct a new Japanese recipe dataset with clear and accurate annotations of ingredient state changes, collected from well-structured and controlled recipe texts. Using this dataset, we design three novel tasks to evaluate whether LLMs can track ingredient state transitions and identify ingredients present at intermediate steps. Our experiments with widely used LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state knowledge improves their understanding of cooking processes, achieving performance comparable to commercial LLMs. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
    "score": 4,
    "reason": "该论文聚焦于烹饪菜谱中的食材状态追踪任务，虽涉及语言模型对现实世界状态的理解（state probing），但其应用场景为通用烹饪领域，与文档图像理解（DIU）的核心目标（如账单、简历等结构化文档的信息抽取）关联较弱。尽管数据集开源且被ACM MM接收（CCF A类会议），具备一定价值，但其任务设计和模态输入（纯文本菜谱）不适用于视觉文档图像处理。虽然LLM推理能力的提升有一定参考意义，但该工作未涉及视觉信息、布局分析或多模态理解，无法直接迁移至DIU任务中。因此，仅作为LLM世界知识研究的补充材料，相关性较低。",
    "summary": "本文构建了一个高质量的日语菜谱数据集，标注了烹饪过程中食材的状态变化，提出三项新任务用于评估大语言模型对中间状态的追踪能力。实验表明，引入状态知识可提升模型对烹饪过程的理解。数据集已在Hugging Face公开。"
  },
  {
    "id": "http://arxiv.org/abs/2409.15912v3",
    "updated": "2025-08-28T15:08:16Z",
    "published": "2024-09-24T09:28:24Z",
    "title": "Explaining word embeddings with perfect fidelity: Case study in research impact prediction",
    "authors": [
      "Lucie Dvorackova",
      "Marcin P. Joachimiak",
      "Michal Cerny",
      "Adriana Kubecova",
      "Vilem Sklenak",
      "Tomas Kliegr"
    ],
    "pdf_url": "https://arxiv.org/pdf/2409.15912v3.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "The best-performing approaches for scholarly document quality prediction are based on embedding models. In addition to their performance when used in classifiers, embedding models can also provide predictions even for words that were not contained in the labelled training data for the classification model, which is important in the context of the ever-evolving research terminology. Although model-agnostic explanation methods, such as Local interpretable model-agnostic explanations, can be applied to explain machine learning classifiers trained on embedding models, these produce results with questionable correspondence to the model. We introduce a new feature importance method, Self-model Rated Entities (SMER), for logistic regression-based classification models trained on word embeddings. We show that SMER has theoretically perfect fidelity with the explained model, as the average of logits of SMER scores for individual words (SMER explanation) exactly corresponds to the logit of the prediction of the explained model. Quantitative and qualitative evaluation is performed through five diverse experiments conducted on 50,000 research articles (papers) from the CORD-19 corpus. Through an AOPC curve analysis, we experimentally demonstrate that SMER produces better explanations than LIME, SHAP and global tree surrogates.",
    "score": 3,
    "reason": "该论文聚焦于词嵌入的可解释性方法（SMER），用于学术文档质量预测，虽涉及文本分析与嵌入模型，但核心目标是提升解释 fidelity，而非文档理解任务本身。其方法针对 logistic regression 模型设计，与 DIU 中的 OCR、实体识别、关系抽取等核心环节无直接关联；且未涉及多模态、布局理解、表格处理或视觉信息抽取等 DIU 关键技术。尽管使用了 CORD-19 数据集，但研究重点不在文档结构或视觉内容理解。因此，虽在 NLP 领域有一定价值，但对 DIU 的直接应用潜力极低。",
    "summary": "本文提出一种名为 Self-model Rated Entities (SMER) 的新特征重要性方法，用于解释基于词嵌入的逻辑回归分类器，声称具有理论上的完美保真度。通过在 CORD-19 数据集上进行五项实验，证明 SMER 在解释质量上优于 LIME、SHAP 和全局树代理。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20869v1",
    "updated": "2025-08-28T15:00:51Z",
    "published": "2025-08-28T15:00:51Z",
    "title": "OLMoASR: Open Models and Data for Training Robust Speech Recognition Models",
    "authors": [
      "Huong Ngo",
      "Matt Deitke",
      "Martijn Bartelds",
      "Sarah Pratt",
      "Josh Gardner",
      "Matt Jordan",
      "Ludwig Schmidt"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20869v1.pdf",
    "comment": "17 pages, 7 figures",
    "category": "Computation and Language",
    "abstract": "Improvements in training data scale and quality have led to significant advances, yet its influence in speech recognition remains underexplored. In this paper, we present a large-scale dataset, OLMoASR-Pool, and series of models, OLMoASR, to study and develop robust zero-shot speech recognition models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio and 17M transcripts, we design text heuristic filters to remove low-quality or mistranscribed data. Our curation pipeline produces a new dataset containing 1M hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M (tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR achieves comparable average performance to OpenAI's Whisper on short and long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a 12.8\\% and 11.0\\% word error rate (WER) that is on par with Whisper's largest English-only model Whisper-medium.en's 12.4\\% and 10.5\\% WER for short and long-form recognition respectively (at equivalent parameter count). OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will be made publicly available to further research on robust speech processing.",
    "score": 2,
    "reason": "该论文聚焦于语音识别（ASR），与文档图像理解（DIU）无直接关联。尽管其数据集和模型具有开放性，但研究内容属于音频模态，不涉及视觉文本理解、布局分析或多模态文档处理。虽然VLM/LLM推理增强技术可能间接启发跨模态设计，但本文未涉及任何可迁移至DIU的架构、任务或方法。因此，不值得在DIU研究中重点关注。",
    "summary": "OLMoASR提出一个大规模高质量英语语音-文本数据集OLMoASR-Mix（1M小时音频-转录对），并训练了一系列从39M到1.5B参数的语音识别模型。通过文本启发式过滤去除低质量数据，其性能在短时和长时语音识别上达到与Whisper相当的水平。数据集与代码将公开，推动鲁棒语音识别研究。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20867v1",
    "updated": "2025-08-28T14:59:55Z",
    "published": "2025-08-28T14:59:55Z",
    "title": "MSRS: Evaluating Multi-Source Retrieval-Augmented Generation",
    "authors": [
      "Rohan Phanse",
      "Yijie Zhou",
      "Kejian Shi",
      "Wencai Zhang",
      "Yixin Liu",
      "Yilun Zhao",
      "Arman Cohan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20867v1.pdf",
    "comment": "COLM 2025; this article supersedes the preprint: arXiv:2309.08960",
    "category": "Computation and Language",
    "abstract": "Retrieval-augmented systems are typically evaluated in settings where information required to answer the query can be found within a single source or the answer is short-form or factoid-based. However, many real-world applications demand the ability to integrate and summarize information scattered across multiple sources, where no single source is sufficient to respond to the user's question. In such settings, the retrieval component of a RAG pipeline must recognize a variety of relevance signals, and the generation component must connect and synthesize information across multiple sources. We present a scalable framework for constructing evaluation benchmarks that challenge RAG systems to integrate information across distinct sources and generate long-form responses. Using our framework, we build two new benchmarks on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing narrative synthesis and summarization tasks, respectively, that require retrieval from large collections. Our extensive experiments with various RAG pipelines -- including sparse and dense retrievers combined with frontier LLMs -- reveal that generation quality is highly dependent on retrieval effectiveness, which varies greatly by task. While multi-source synthesis proves challenging even in an oracle retrieval setting, we find that reasoning models significantly outperform standard LLMs at this distinct step.",
    "score": 8,
    "reason": "该论文聚焦于多源检索增强生成（MSRS），核心挑战是跨多个来源的信息整合与长文本生成，这与DIU中需要从复杂文档布局中提取并关联分散信息的任务高度相关。其提出的MSRS-Story和MSRS-Meet基准对多源信息融合能力的评估极具参考价值，尤其适用于模拟账单、简历等包含多段落、跨区域信息的文档理解场景。论文发现推理模型在合成任务上显著优于普通LLM，这直接支持将inference scaling中的高级推理技术（如CoT、ToT）应用于DIU系统中以提升关系抽取与语义整合能力。虽非直接针对DIU，但其方法论可直接迁移至DIU中的多文档/多区域信息融合任务。此外，该工作已被COLM 2025接收（CCF B类会议），具备一定学术认可度。",
    "summary": "本文提出MSRS框架，用于构建评估多源检索与合成能力的基准，涵盖叙事合成（MSRS-Story）与会议纪要摘要（MSRS-Meet）两类任务。实验表明，生成质量严重依赖检索效果，且推理型模型在跨源信息整合任务中显著优于标准LLM，为DIU中复杂信息融合提供了关键启示。"
  },
  {
    "id": "http://arxiv.org/abs/2503.11519v3",
    "updated": "2025-08-28T14:55:38Z",
    "published": "2025-03-14T15:42:42Z",
    "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models",
    "authors": [
      "Hao Cheng",
      "Erjia Xiao",
      "Yichi Wang",
      "Lingfeng Zhang",
      "Qiang Zhang",
      "Jiahang Cao",
      "Kaidi Xu",
      "Mengshu Sun",
      "Xiaoshuai Hao",
      "Jindong Gu",
      "Renjing Xu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.11519v3.pdf",
    "comment": "This paper is accepted by IJCAI2025 Workshop on Deepfake Detection, Localization, and Interpretability",
    "category": "Computation and Language",
    "abstract": "Current Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-Vision tasks, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have attracted significant attention. Large Vision Language Models (LVLMs) and I2I Generation Models (GMs) are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to produce disruptive outputs that are semantically aligned with those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of cross-vision tasks. However, the specific characteristics of the threats posed by visual prompts remain underexplored. In this paper, to comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we propose the Typographic Visual Prompts Injection Dataset and thoroughly evaluate the TVPI security risks on various open-source and closed-source LVLMs and I2I GMs under visual prompts with different target semantics, deepening the understanding of TVPI threats.",
    "score": 4,
    "reason": "该论文研究视觉提示注入攻击（TVPI）对跨模态生成模型的影响，属于安全与对抗性攻击方向。虽然涉及多模态模型（LVLMs），但其核心关注点是模型的脆弱性与防御机制，而非文档图像理解（DIU）任务本身。尽管VLM是DIU的重要技术基础，但本文并未提出可用于DIU的新方法或改进，也未在DIU数据集上验证效果，且无开源代码，与DIU直接应用关联较弱。虽为IJCAI Workshop论文，但非CCF A类会议，且内容偏向安全威胁分析，不具可迁移性。因此仅具间接参考价值。",
    "summary": "本文构建了Typographic Visual Prompts Injection Dataset，系统评估了在不同语义目标下，印刷型视觉提示对大型视觉语言模型（LVLMs）和图像到图像生成模型（I2I GMs）的干扰能力，揭示了视觉提示注入在跨视觉任务中的安全风险，但未涉及文档理解、信息抽取或推理增强等DIU关键环节。"
  },
  {
    "id": "http://arxiv.org/abs/2504.12140v2",
    "updated": "2025-08-28T14:32:15Z",
    "published": "2025-04-16T14:52:22Z",
    "title": "Multilingual Contextualization of Large Language Models for Document-Level Machine Translation",
    "authors": [
      "Miguel Moura Ramos",
      "Patrick Fernandes",
      "Sweta Agrawal",
      "André F. T. Martins"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.12140v2.pdf",
    "comment": "COLM 2025",
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.",
    "score": 6,
    "reason": "该论文聚焦于文档级机器翻译，虽涉及文档理解与长距离依赖建模，但核心任务是翻译而非信息抽取或结构理解。尽管引入了DocBlocks数据集并支持多范式推理，其方法主要服务于语言生成而非DIU中的实体识别、关系抽取或布局理解。虽然LLM的上下文建模能力对DIU有间接启发，但论文未直接解决DIU关键问题，且无开源或顶会加持（COLM非CCF A类），因此相关性较弱，仅在长文档处理方面有有限参考价值。",
    "summary": "本文提出通过在高质量文档级数据集DocBlocks上进行针对性微调，提升大语言模型在文档级机器翻译中的表现。方法支持直接文档到文档及分块翻译，并融合上下文指令以增强跨句依赖建模。实验表明该方法优于提示工程和基于代理的方法，在翻译质量与推理速度上均有提升。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20828v1",
    "updated": "2025-08-28T14:23:39Z",
    "published": "2025-08-28T14:23:39Z",
    "title": "GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction",
    "authors": [
      "Jie Zhao",
      "Wanting Ning",
      "Yuxiao Fei",
      "Yubo Feng",
      "Lishuang Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20828v1.pdf",
    "comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP Findings)",
    "category": "Computation and Language",
    "abstract": "In Natural Language Processing(NLP), Event Temporal Relation Extraction (ETRE) is to recognize the temporal relations of two events. Prior studies have noted the importance of language models for ETRE. However, the restricted pre-trained knowledge of Small Language Models(SLMs) limits their capability to handle minority class relations in imbalanced classification datasets. For Large Language Models(LLMs), researchers adopt manually designed prompts or instructions, which may introduce extra noise, leading to interference with the model's judgment of the long-distance dependencies between events. To address these issues, we propose GDLLM, a Global Distance-aware modeling approach based on LLMs. We first present a distance-aware graph structure utilizing Graph Attention Network(GAT) to assist the LLMs in capturing long-distance dependency features. Additionally, we design a temporal feature learning paradigm based on soft inference to augment the identification of relations with a short-distance proximity band, which supplements the probabilistic information generated by LLMs into the multi-head attention mechanism. Since the global feature can be captured effectively, our framework substantially enhances the performance of minority relation classes and improves the overall learning ability. Experiments on two publicly available datasets, TB-Dense and MATRES, demonstrate that our approach achieves state-of-the-art (SOTA) performance.",
    "score": 6,
    "reason": "该论文聚焦于事件时序关系抽取（ETRE），虽使用LLM并引入图结构与距离感知机制，属于推理增强方向，但其研究对象为纯文本事件关系，未涉及文档图像理解中的视觉、布局或结构信息。尽管其提出的全局距离建模和软推理机制对DIU中长距离实体关系建模有潜在启发意义，但缺乏与视觉模态的结合，无法直接迁移至DIU任务。EMNLP Findings为顶会，略有加分，但相关性不足，故评分中等。",
    "summary": "GDLLM提出一种基于大语言模型的全局距离感知方法，通过图注意力网络捕捉事件间的远距离依赖，并设计软推理机制增强短距关系识别，显著提升少数类时序关系的性能，在TB-Dense和MATRES数据集上达到SOTA。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20810v1",
    "updated": "2025-08-28T14:10:59Z",
    "published": "2025-08-28T14:10:59Z",
    "title": "A Graph-Based Test-Harness for LLM Evaluation",
    "authors": [
      "Jessica Lundin",
      "Guillaume Chabot-Couture"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20810v1.pdf",
    "comment": "4 pages, 2 figures, dataset",
    "category": "Computation and Language",
    "abstract": "We present a first known prototype of a dynamic, systematic benchmark of medical guidelines for 400+ questions, with 3.3+ trillion possible combinations, covering 100\\% of guideline relationships. We transformed the WHO IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms, treatments, follow-ups, severities) and 300+ edges, then used graph traversal to generate questions that incorporated age-specific scenarios and contextual distractors to ensure clinical relevance. Our graph-based approach enables systematic evaluation across clinical tasks (45-67\\% accuracy), and we find models excel at symptom recognition but struggle with triaging severity, treatment protocols and follow-up care, demonstrating how customized benchmarks can identify specific capability gaps that general-domain evaluations miss. Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training (supervised finetuning, GRPO, DPO), where correct answers provide high-reward samples without expensive human annotation. The graph-based approach successfully addresses the coverage limitations of manually curated benchmarks. This methodology is a step toward scalable, contamination-resistant solution for creating comprehensive benchmarks that can be dynamically generated, including when the guidelines are updated. Code and datasets are available at https://github.com/jessicalundin/graph_testing_harness",
    "score": 4,
    "reason": "该论文提出了一种基于图的动态测试框架用于LLM评估，虽在方法上具有创新性，尤其适用于医疗指南场景，但其应用场景为医学临床决策，与文档图像理解（DIU）无直接关联。尽管其图结构生成和动态问答机制对DIU中的关系抽取或推理链构建有潜在启发意义，但论文未涉及视觉信息、文档布局、OCR或实体关系建模，且未提及可迁移至DIU任务的具体方法。因此，虽具一定参考价值，但无法直接应用于DIU领域，故评分较低。",
    "summary": "本文提出一种基于图的动态基准测试框架，将WHO IMCI医疗手册转化为包含200+节点和300+边的有向图，通过图遍历生成涵盖100%关系的超大规模多选题（3.3万亿组合），用于系统化评估LLM在临床决策中的表现。实验发现模型在症状识别上表现较好，但在严重程度判断、治疗方案和随访方面存在明显短板。该方法可用于LLM后训练优化，提升奖励信号质量，减少人工标注依赖。代码与数据集已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.08846v2",
    "updated": "2025-08-28T14:07:41Z",
    "published": "2025-08-12T11:09:03Z",
    "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs",
    "authors": [
      "Afrozah Nadeem",
      "Mark Dras",
      "Usman Naseem"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.08846v2.pdf",
    "comment": "Accepted at CASE@RANLP2025",
    "category": "Computation and Language",
    "abstract": "Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases along political and economic dimensions. In this paper, we employ a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), this method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions.",
    "score": 3,
    "reason": "该论文聚焦于大语言模型中的政治偏见问题，虽然涉及LLM的内部表示与推理机制分析，但其核心目标是模型公平性与偏见缓解，而非提升推理能力或支持DIU任务。尽管方法中提及激活层分析和对比对，与inference scaling或reasoning技术有一定关联，但未直接应用于文档理解、视觉信息抽取或agent系统构建。此外，论文未提及开源或在CCF A类会议发表，且研究方向与DIU领域无直接迁移价值，故不推荐。",
    "summary": "本文提出一种基于政治光谱测试（PCT）的框架，通过对比对分析解码器型LLM的隐藏层激活，揭示模型在政治与经济维度上的表征偏差，并利用定向向量实现偏见缓解。研究展示了多层激活差异与政治表述之间的联系，为模型公平性提供了一种基于内部表示的干预路径。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20805v1",
    "updated": "2025-08-28T14:07:07Z",
    "published": "2025-08-28T14:07:07Z",
    "title": "Exploring Machine Learning and Language Models for Multimodal Depression Detection",
    "authors": [
      "Javier Si Zhao Hong",
      "Timothy Zoe Delaya",
      "Sherwyn Chan Yin Kit",
      "Pai Chet Ng",
      "Xiaoxiao Miao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20805v1.pdf",
    "comment": "This paper has been accepted by APCIPA ASC 2025",
    "category": "Computation and Language",
    "abstract": "This paper presents our approach to the first Multimodal Personality-Aware Depression Detection Challenge, focusing on multimodal depression detection using machine learning and deep learning models. We explore and compare the performance of XGBoost, transformer-based architectures, and large language models (LLMs) on audio, video, and text features. Our results highlight the strengths and limitations of each type of model in capturing depression-related signals across modalities, offering insights into effective multimodal representation strategies for mental health prediction.",
    "score": 3,
    "reason": "该论文聚焦于多模态抑郁症检测，研究对象为心理健康预测，虽涉及多模态模型（如LLM）和跨模态特征融合，但应用场景与文档图像理解（DIU）无直接关联。其核心任务是情感与心理状态识别，而非文档结构解析、实体识别或关系抽取。尽管使用了LLM，但未涉及推理增强技术（如CoT、ToT）或Agent架构，也未提出可用于DIU的新数据集或方法。因此，相关性较低，仅在技术层面有轻微参考价值。",
    "summary": "本文参与了首届多模态人格感知抑郁症检测挑战赛，比较了XGBoost、基于Transformer的架构及大型语言模型在音频、视频和文本特征上的表现，旨在识别抑郁相关信号。结果揭示了不同模型在多模态表示中的优劣，为心理健康预测提供策略参考。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20771v1",
    "updated": "2025-08-28T13:28:07Z",
    "published": "2025-08-28T13:28:07Z",
    "title": "Signs of Struggle: Spotting Cognitive Distortions across Language and Register",
    "authors": [
      "Abhishek Kuber",
      "Enrico Liscio",
      "Ruixuan Zhang",
      "Caroline Figueroa",
      "Pradeep K. Murukannaiah"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20771v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Rising mental health issues among youth have increased interest in automated approaches for detecting early signs of psychological distress in digital text. One key focus is the identification of cognitive distortions, irrational thought patterns that have a role in aggravating mental distress. Early detection of these distortions may enable timely, low-cost interventions. While prior work has focused on English clinical data, we present the first in-depth study of cross-lingual and cross-register generalization of cognitive distortion detection, analyzing forum posts written by Dutch adolescents. Our findings show that while changes in language and writing style can significantly affect model performance, domain adaptation methods show the most promise.",
    "score": 2,
    "reason": "该论文研究认知扭曲在多语言和不同语域下的检测，属于心理健康与自然语言处理交叉领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）技术无直接关联。其核心任务为文本心理状态分析，不涉及文档结构、视觉信息抽取、OCR或布局理解，也无法直接迁移至DIU系统中。因此，相关性极低。",
    "summary": "本文首次深入研究了认知扭曲检测在跨语言（英语与荷兰语）和跨写作风格（如论坛帖子）场景下的泛化能力，基于荷兰青少年的在线讨论数据进行实验，发现语言风格变化显著影响模型性能，并指出领域自适应方法最具潜力。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20766v1",
    "updated": "2025-08-28T13:22:33Z",
    "published": "2025-08-28T13:22:33Z",
    "title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection",
    "authors": [
      "Harethah Abu Shairah",
      "Hasan Abed Al Kader Hammoud",
      "George Turkiyyah",
      "Bernard Ghanem"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20766v1.pdf",
    "comment": "Under Review",
    "category": "Computation and Language",
    "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.",
    "score": 5,
    "reason": "该论文聚焦于大语言模型的安全对齐，提出通过秩一权重注入增强安全拒绝能力。虽然其方法在提升LLM安全性方面具有创新性，但核心内容与DIU任务无直接关联。其技术路径（安全对齐、权重微调）虽可间接用于DIU中的安全约束设计，但不具备可直接迁移至文档理解、实体识别或关系抽取的能力。且当前处于评审阶段，未开源，也未发表于CCF A类会议。因此，仅对DIU中可能涉及的‘安全可控’场景有弱相关性，不建议优先阅读。",
    "summary": "本文提出Rank-One Safety Injection (ROSI)，一种无需微调的白盒方法，通过在残差流写矩阵上施加秩一权重修改，永久性地将模型激活推向拒绝有害请求的子空间。该方法基于少量有害/无害指令对计算安全方向，能有效提升Llama Guard 3评估下的拒绝率，同时保持模型在MMLU、HellaSwag等基准上的性能。还可用于重新对齐‘去审查’模型，作为最后一道安全防线。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20764v1",
    "updated": "2025-08-28T13:19:31Z",
    "published": "2025-08-28T13:19:31Z",
    "title": "Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions",
    "authors": [
      "Xiaoyi Wang",
      "Jiwei Zhang",
      "Guangtao Zhang",
      "Honglei Guo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20764v1.pdf",
    "comment": "Accepted at EMNLP 2025,14 page,3 figures",
    "category": "Computation and Language",
    "abstract": "Synthetic therapy dialogues generated by large language models (LLMs) are increasingly used in mental health NLP to simulate counseling scenarios, train models, and supplement limited real-world data. However, it remains unclear whether these synthetic conversations capture the nuanced emotional dynamics of real therapy. In this work, we conduct the first comparative analysis of emotional arcs between real and LLM-generated Cognitive Behavioral Therapy dialogues. We adapt the Utterance Emotion Dynamics framework to analyze fine-grained affective trajectories across valence, arousal, and dominance dimensions. Our analysis spans both full dialogues and individual speaker roles (counselor and client), using real sessions transcribed from public videos and synthetic dialogues from the CACTUS dataset. We find that while synthetic dialogues are fluent and structurally coherent, they diverge from real conversations in key emotional properties: real sessions exhibit greater emotional variability,more emotion-laden language, and more authentic patterns of reactivity and regulation. Moreover, emotional arc similarity between real and synthetic speakers is low, especially for clients. These findings underscore the limitations of current LLM-generated therapy data and highlight the importance of emotional fidelity in mental health applications. We introduce RealCBT, a curated dataset of real CBT sessions, to support future research in this space.",
    "score": 2,
    "reason": "该论文研究LLM生成的心理咨询对话中的情感弧线，属于心理健康NLP领域，与文档图像理解（DIU）无直接关联。尽管涉及LLM和情感分析，但其核心任务是情感动态建模而非文档信息抽取或视觉语义理解。虽然提及LLM生成数据，但未涉及多模态、OCR、实体识别或关系抽取等DIU关键技术。此外，未涉及inference scaling、reasoning技术或agent系统设计。因此不值得在DIU研究中优先阅读。",
    "summary": "本文对比了真实认知行为疗法（CBT）会话与LLM生成会话的情感演化轨迹，使用情绪动力学框架分析效价、唤醒度和支配度三个维度。研究发现，尽管合成对话在结构上流畅，但在情感多样性、语言情感密度和反应调节模式上不如真实会话，尤其在来访者角色表现更差。作者构建了RealCBT数据集以支持未来研究。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20757v1",
    "updated": "2025-08-28T13:14:20Z",
    "published": "2025-08-28T13:14:20Z",
    "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation",
    "authors": [
      "Yuanhao Ding",
      "Esteban Garces Arias",
      "Meimingwei Li",
      "Julian Rodemann",
      "Matthias Aßenmacher",
      "Danlu Chen",
      "Gaojuan Fan",
      "Christian Heumann",
      "Chongsheng Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20757v1.pdf",
    "comment": "Accepted at Findings of the Association for Computational Linguistics: EMNLP (Findings) 2025",
    "category": "Computation and Language",
    "abstract": "Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. While contrastive search-based decoding strategies have emerged to address this trade-off, their practical utility is often limited by hyperparameter dependence and high computational costs. We introduce GUARD, a self-adaptive decoding method that effectively balances these competing objectives through a novel \"Glocal\" uncertainty-driven framework. GUARD combines global entropy estimates with local entropy deviations to integrate both long-term and short-term uncertainty signals. We demonstrate that our proposed global entropy formulation effectively mitigates abrupt variations in uncertainty, such as sudden overconfidence or high entropy spikes, and provides theoretical guarantees of unbiasedness and consistency. To reduce computational overhead, we incorporate a simple yet effective token-count-based penalty into GUARD. Experimental results demonstrate that GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. In a more nuanced comparison study across different dimensions of text quality, both human and LLM evaluators validated its remarkable performance. Our code is available at https://github.com/YecanLee/GUARD.",
    "score": 7,
    "reason": "GUARD 提出了一种自适应的解码方法，通过结合全局与局部不确定性来平衡生成文本的连贯性与多样性，这在推理阶段提升LLM性能方面具有重要意义。虽然其核心目标是开放域文本生成，但其在降低计算开销、提高推理效率方面的设计（如基于token计数的惩罚机制）对DIU任务中涉及的长序列生成（如文档结构解析或问答生成）有潜在直接应用价值。此外，该方法可用于增强VLM在生成式DIU中的输出质量，尤其适用于需要高稳定性和高效性的场景。论文被EMNLP Findings接收，属于ACL系列会议，具有一定影响力；代码已开源，加分。虽非直接面向DIU，但其inference scaling与reasoning优化策略可直接迁移至DIU中的生成环节。",
    "summary": "GUARD是一种自适应的解码方法，利用‘Glocal’不确定性感知框架，融合全局熵与局部熵偏差，有效缓解生成过程中的突发过自信或熵突变问题，同时引入轻量级token计数惩罚以降低计算成本。实验表明其在保持文本多样性和连贯性的同时显著提升生成速度，人评和模型评估均表现优异。代码已开源，适用于需高效推理的生成任务，对DIU中基于VLM的生成式理解有借鉴意义。"
  },
  {
    "id": "http://arxiv.org/abs/2411.19770v2",
    "updated": "2025-08-28T13:12:13Z",
    "published": "2024-11-29T15:18:01Z",
    "title": "Noro: Noise-Robust One-shot Voice Conversion with Hidden Speaker Representation Learning",
    "authors": [
      "Haorui He",
      "Yuchen Song",
      "Yuancheng Wang",
      "Haoyang Li",
      "Xueyao Zhang",
      "Li Wang",
      "Gongping Huang",
      "Eng Siong Chng",
      "Zhizheng Wu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.19770v2.pdf",
    "comment": "Accepted by APSIPA ASC 2025",
    "category": "Computation and Language",
    "abstract": "The effectiveness of one-shot voice conversion (VC) decreases in real-world scenarios where reference speeches, which are often sourced from the internet, contain various disturbances like background noise. To address this issue, we introduce Noro, a noise-robust one-shot VC system. Noro features innovative components tailored for VC using noisy reference speeches, including a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss. Experimental results demonstrate that Noro outperforms our baseline system in both clean and noisy scenarios, highlighting its efficacy for real-world applications. Additionally, we investigate the hidden speaker representation capabilities of our baseline system by repurposing its reference encoder as a speaker encoder. The results show that it is competitive with several advanced self-supervised learning models for speaker representation under the SUPERB settings, highlighting the potential for advancing speaker representation learning through one-shot VC tasks.",
    "score": 2,
    "reason": "该论文聚焦于噪声鲁棒的一次性语音转换（one-shot voice conversion），属于语音处理领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型（LLM）推理增强（inference scaling/reasoning）或智能体（agent）系统无直接关联。尽管其提出的隐式说话人表征学习可能在理论上具有泛化潜力，但无法直接迁移至DIU任务中。论文未提及任何与视觉、布局、文本理解或多模态信息抽取相关的内容，且发表于非CCF A类会议（APSIPA ASC 2025）。",
    "summary": "Noro提出一种针对噪声参考语音的一次性语音转换系统，通过双分支编码器和抗噪对比损失提升鲁棒性，在真实场景下表现优于基线。同时发现基线参考编码器可作为高效说话人表征提取器，性能接近先进自监督模型。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20750v1",
    "updated": "2025-08-28T13:08:57Z",
    "published": "2025-08-28T13:08:57Z",
    "title": "Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets",
    "authors": [
      "Vassiliy Cheremetiev",
      "Quang Long Ho Ngo",
      "Chau Ying Kot",
      "Alina Elena Baia",
      "Andrea Cavallaro"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20750v1.pdf",
    "comment": "Paper accepted at the DHOW Workshop at ACM Multimedia 2025. Code available at https://github.com/idiap/implicit-hsd",
    "category": "Computation and Language",
    "abstract": "Implicit hate speech (IHS) is indirect language that conveys prejudice or hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to detect as it does not include explicit derogatory or inflammatory words. To address this challenge, task-specific pipelines can be complemented with external knowledge or additional information such as context, emotions and sentiment data. In this paper, we show that, by solely fine-tuning recent general-purpose embedding models based on large language models (LLMs), such as Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance. Experiments on multiple IHS datasets show up to 1.10 percentage points improvements for in-dataset, and up to 20.35 percentage points improvements in cross-dataset evaluation, in terms of F1-macro score.",
    "score": 2,
    "reason": "该论文聚焦于隐性仇恨言论检测，属于自然语言处理中的社会敏感内容识别任务，与文档图像理解（DIU）无直接关联。尽管其使用了LLM嵌入模型并进行了微调，但应用场景、数据类型（纯文本）和目标（情感/偏见检测）均不匹配DIU的核心需求。虽有代码开源和会议接受信息，但技术可迁移性极低，无法直接应用于OCR后实体识别、关系抽取或视觉-语言联合建模等DIU关键环节。",
    "summary": "本文通过微调通用LLM嵌入模型（如Stella、Jasper等）提升隐性仇恨言论检测性能，在跨数据集评估中取得显著提升。研究强调仅依赖嵌入层微调即可实现SOTA效果，适用于社交媒体文本分析场景，但与文档图像理解无关。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20736v1",
    "updated": "2025-08-28T12:59:01Z",
    "published": "2025-08-28T12:59:01Z",
    "title": "Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees",
    "authors": [
      "Stephen Meisenbacher",
      "Maulik Chevli",
      "Florian Matthes"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20736v1.pdf",
    "comment": "17 pages, 2 figures, 11 tables. Accepted to EMNLP 2025 (Main)",
    "category": "Computation and Language",
    "abstract": "Many works at the intersection of Differential Privacy (DP) in Natural Language Processing aim to protect privacy by transforming texts under DP guarantees. This can be performed in a variety of ways, from word perturbations to full document rewriting, and most often under local DP. Here, an input text must be made indistinguishable from any other potential text, within some bound governed by the privacy parameter $\\varepsilon$. Such a guarantee is quite demanding, and recent works show that privatizing texts under local DP can only be done reasonably under very high $\\varepsilon$ values. Addressing this challenge, we introduce DP-ST, which leverages semantic triples for neighborhood-aware private document generation under local DP guarantees. Through the evaluation of our method, we demonstrate the effectiveness of the divide-and-conquer paradigm, particularly when limiting the DP notion (and privacy guarantees) to that of a privatization neighborhood. When combined with LLM post-processing, our method allows for coherent text generation even at lower $\\varepsilon$ values, while still balancing privacy and utility. These findings highlight the importance of coherence in achieving balanced privatization outputs at reasonable $\\varepsilon$ levels.",
    "score": 3,
    "reason": "该论文聚焦于在局部差分隐私（Local DP）下生成私有化文档，使用语义三元组进行邻域感知的文本生成。虽然涉及隐私保护与文本生成，但其核心是文本生成而非文档图像理解（DIU），且未涉及视觉信息、布局理解或OCR/实体识别等DIU关键环节。尽管EMNLP 2025为CCF A类会议，且使用LLM后处理有一定相关性，但整体研究目标与DIU无直接关联，无法直接应用于DIU任务中的信息抽取、结构理解或多模态建模。因此不具直接应用价值。",
    "summary": "本文提出DP-ST方法，利用语义三元组实现局部差分隐私下的文档生成，在较低ε值下仍保持文本连贯性。通过将隐私保护限制在邻域层面并结合LLM后处理，平衡了隐私与文本质量。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20722v1",
    "updated": "2025-08-28T12:45:25Z",
    "published": "2025-08-28T12:45:25Z",
    "title": "rStar2-Agent: Agentic Reasoning Technical Report",
    "authors": [
      "Ning Shang",
      "Yifei Liu",
      "Yi Zhu",
      "Li Lyna Zhang",
      "Weijiang Xu",
      "Xinyu Guan",
      "Buze Zhang",
      "Bingcheng Dong",
      "Xudong Zhou",
      "Bowen Zhang",
      "Ying Xin",
      "Ziming Miao",
      "Scarlett Li",
      "Fan Yang",
      "Mao Yang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20722v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.",
    "score": 9,
    "reason": "该论文提出rStar2-Agent，一个基于代理强化学习的14B数学推理模型，在复杂问题求解中展现出高级认知行为，如思考后调用Python工具、根据执行反馈自我反思与修正。其关键技术（高效RL基础设施、GRPO-RoC算法、分阶段训练配方）对DIU领域极具启发性，尤其在提升视觉多模态模型（VLM）的推理能力方面有直接迁移潜力。虽然主要面向数学推理，但其agentic reasoning框架可直接用于构建DIU智能体，实现文档理解中的多步推理与工具调用。代码开源，且在有限GPU资源下达成SOTA性能，具备高实用价值。虽未明确提及DIU，但技术路径高度契合未来DIU agent的发展方向。",
    "summary": "rStar2-Agent是一个14B规模的数学推理模型，通过代理强化学习实现前沿性能。它具备在使用Python工具前仔细思考、根据执行反馈自我验证与优化的能力，得益于高效的RL基础设施、Resample-on-Correct策略及分阶段训练方法。仅用510步RL训练即在AIME24/25上超越DeepSeek-R1（671B），响应更短。代码已开源，具备强泛化能力，适用于科学推理与工具使用任务，为构建DIU智能体提供了关键范式。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20718v1",
    "updated": "2025-08-28T12:43:21Z",
    "published": "2025-08-28T12:43:21Z",
    "title": "Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models",
    "authors": [
      "Ruiyi Yan",
      "Yugo Murawaki"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20718v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models have significantly enhanced the capacities and efficiency of text generation. On the one hand, they have improved the quality of text-based steganography. On the other hand, they have also underscored the importance of watermarking as a safeguard against malicious misuse. In this study, we focus on tokenization inconsistency (TI) between Alice and Bob in steganography and watermarking, where TI can undermine robustness. Our investigation reveals that the problematic tokens responsible for TI exhibit two key characteristics: infrequency and temporariness. Based on these findings, we propose two tailored solutions for TI elimination: a stepwise verification method for steganography and a post-hoc rollback method for watermarking. Experiments show that (1) compared to traditional disambiguation methods in steganography, directly addressing TI leads to improvements in fluency, imperceptibility, and anti-steganalysis capacity; (2) for watermarking, addressing TI enhances detectability and robustness against attacks.",
    "score": 2,
    "reason": "该论文聚焦于大语言模型在隐写术和水印中的分词不一致问题，属于信息安全与文本生成领域的应用研究。虽然涉及LLM，但其核心目标是提升隐写/水印的鲁棒性与隐蔽性，与文档图像理解（DIU）、视觉多模态建模、推理增强或智能体系统无直接关联。所提方法无法直接迁移至DIU任务中，且未涉及视觉信息、布局理解、实体识别或关系抽取等关键环节。因此，与DIU领域关联度极低，不具直接应用价值。",
    "summary": "本文研究大语言模型在隐写术与水印中因分词不一致（TI）导致的鲁棒性下降问题，发现异常token具有稀有性和临时性特征，并提出分步验证（用于隐写）与事后回滚（用于水印）两种解决方案。实验表明，该方法可提升隐写内容的流畅性、不可察觉性及抗分析能力，以及水印的可检测性与抗攻击性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20712v1",
    "updated": "2025-08-28T12:30:32Z",
    "published": "2025-08-28T12:30:32Z",
    "title": "Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning",
    "authors": [
      "Nelson Filipe Costa",
      "Leila Kosseim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20712v1.pdf",
    "comment": "Published at SIGDIAL 2025. Best paper award",
    "category": "Computation and Language",
    "abstract": "This paper introduces the first multi-lingual and multi-label classification model for implicit discourse relation recognition (IDRR). Our model, HArch, is evaluated on the recently released DiscoGeM 2.0 corpus and leverages hierarchical dependencies between discourse senses to predict probability distributions across all three sense levels in the PDTB 3.0 framework. We compare several pre-trained encoder backbones and find that RoBERTa-HArch achieves the best performance in English, while XLM-RoBERTa-HArch performs best in the multi-lingual setting. In addition, we compare our fine-tuned models against GPT-4o and Llama-4-Maverick using few-shot prompting across all language configurations. Our results show that our fine-tuned models consistently outperform these LLMs, highlighting the advantages of task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our hierarchical approach.",
    "score": 4,
    "reason": "该论文聚焦于隐式话语关系识别（IDRR），属于自然语言处理中的话语分析范畴，与文档图像理解（DIU）的直接关联较弱。尽管其多语言和层次化建模方法有一定参考价值，但IDRR主要处理文本间的语义连接，不涉及文档布局、视觉内容理解或OCR后信息抽取等DIU核心任务。虽然使用了LLM进行对比实验，但其研究目标并非提升视觉-语言联合理解能力，也无法直接迁移至DIU场景。SIGDIAL是领域内会议且获最佳论文奖，属高质量工作，但与DIU技术栈关联度低，仅具间接启发意义。",
    "summary": "本文提出HArch模型，用于多语言、多标签的隐式话语关系识别，利用PDTB 3.0框架中的层级语义结构，在DiscoGeM 2.0上实现SOTA性能。通过对比RoBERTa与XLM-RoBERTa等编码器及GPT-4o、Llama-4-Maverick的少样本提示效果，验证了任务特定微调优于提示工程。"
  },
  {
    "id": "http://arxiv.org/abs/2212.07126v2",
    "updated": "2025-08-28T12:21:45Z",
    "published": "2022-12-14T09:25:49Z",
    "title": "Explainability of Text Processing and Retrieval Methods: A Survey",
    "authors": [
      "Sourav Saha",
      "Debapriyo Majumdar",
      "Mandar Mitra"
    ],
    "pdf_url": "https://arxiv.org/pdf/2212.07126v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Deep Learning and Machine Learning based models have become extremely popular in text processing and information retrieval. However, the non-linear structures present inside the networks make these models largely inscrutable. A significant body of research has focused on increasing the transparency of these models. This article provides a broad overview of research on the explainability and interpretability of natural language processing and information retrieval methods. More specifically, we survey approaches that have been applied to explain word embeddings, sequence modeling, attention modules, transformers, BERT, and document ranking. The concluding section suggests some possible directions for future research on this topic.",
    "score": 2,
    "reason": "该论文为文本处理与检索方法可解释性的综述，主题聚焦于NLP模型的透明性分析，如注意力机制、BERT等。虽然涉及部分DIU相关技术（如Transformer、文档排序），但其核心目标并非文档图像理解，且未涉及视觉模态、OCR、布局理解或多模态推理。与VLM、LLM推理增强（inference scaling）、agent系统等前沿方向无直接关联，无法直接迁移应用于DIU任务。因此不具直接应用价值。",
    "summary": "本文系统回顾了自然语言处理与信息检索中模型可解释性的研究进展，涵盖词嵌入、序列建模、注意力机制、Transformer、BERT及文档排序等方法的解释技术，提出未来研究方向，但未涉及视觉信息或文档图像理解场景。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20701v1",
    "updated": "2025-08-28T12:19:34Z",
    "published": "2025-08-28T12:19:34Z",
    "title": "Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings",
    "authors": [
      "Ares Fabregat-Hernández",
      "Javier Palanca",
      "Vicent Botti"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20701v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "The paper introduces a novel framework based on category theory to enhance the explainability of artificial intelligence systems, particularly focusing on word embeddings. Key topics include the construction of categories $\\mathcal{L}_T$ and $\\mathcal{P}_T$, providing schematic representations of the semantics of a text $ T $, and reframing the selection of the element with maximum probability as a categorical notion. Additionally, the monoidal category $\\mathcal{P}_T$ is constructed to visualize various methods of extracting semantic information from $T$, offering a dimension-agnostic definition of semantic spaces reliant solely on information within the text. Furthermore, the paper defines the categories of configurations Conf and word embeddings $\\mathcal{Emb}$, accompanied by the concept of divergence as a decoration on $\\mathcal{Emb}$. It establishes a mathematically precise method for comparing word embeddings, demonstrating the equivalence between the GloVe and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural network algorithms (black box) to a transparent framework. Finally, the paper presents a mathematical approach to computing biases before embedding and offers insights on mitigating biases at the semantic space level, advancing the field of explainable artificial intelligence."
  },
  {
    "id": "http://arxiv.org/abs/2508.20700v1",
    "updated": "2025-08-28T12:18:35Z",
    "published": "2025-08-28T12:18:35Z",
    "title": "Generative Annotation for ASR Named Entity Correction",
    "authors": [
      "Yuanchang Luo",
      "Daimeng Wei",
      "Shaojun Li",
      "Hengchao Shang",
      "Jiaxin Guo",
      "Zongyao Li",
      "Zhanglin Wu",
      "Xiaoyu Chen",
      "Zhiqiang Rao",
      "Jinlong Yang",
      "Hao Yang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20700v1.pdf",
    "comment": "12 pages, 7 figures, 7 tables, EMNLP 2025",
    "category": "Computation and Language",
    "abstract": "End-to-end automatic speech recognition systems often fail to transcribe domain-specific named entities, causing catastrophic failures in downstream tasks. Numerous fast and lightweight named entity correction (NEC) models have been proposed in recent years. These models, mainly leveraging phonetic-level edit distance algorithms, have shown impressive performances. However, when the forms of the wrongly-transcribed words(s) and the ground-truth entity are significantly different, these methods often fail to locate the wrongly transcribed words in hypothesis, thus limiting their usage. We propose a novel NEC method that utilizes speech sound features to retrieve candidate entities. With speech sound features and candidate entities, we inovatively design a generative method to annotate entity errors in ASR transcripts and replace the text with correct entities. This method is effective in scenarios of word form difference. We test our method using open-source and self-constructed test sets. The results demonstrate that our NEC method can bring significant improvement to entity accuracy. We will open source our self-constructed test set and training data.",
    "score": 4,
    "reason": "该论文聚焦于语音识别中的命名实体纠错（NEC），虽涉及实体识别与纠错，但应用场景为语音转写而非文档图像理解。其核心方法基于声学特征生成标注，与DIU中基于视觉的文本识别与结构理解无直接关联。尽管提及实体纠错，但任务本质是ASR后处理，无法直接迁移至DIU领域。此外，未涉及多模态、推理增强或agent相关技术，也未使用VLM/LLM进行端到端理解。虽在EMNLP 2025发表属顶会，且开源数据集值得肯定，但与DIU核心任务关联度低，故仅给予中等偏低评分。",
    "summary": "本文提出一种基于语音声学特征的生成式命名实体纠错方法，通过检索候选实体并生成错误标注来修正ASR输出中的实体错误。实验表明该方法在词形差异大的场景下优于传统编辑距离方法，所用数据集和训练数据将开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.18321v2",
    "updated": "2025-08-28T12:18:04Z",
    "published": "2025-08-24T09:58:10Z",
    "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions",
    "authors": [
      "Maojia Song",
      "Tej Deep Pala",
      "Weisheng Jin",
      "Amir Zadeh",
      "Chuan Li",
      "Dorien Herremans",
      "Soujanya Poria"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.18321v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: https://github.com/declare-lab/KAIROS.",
    "score": 8,
    "reason": "该论文聚焦于多智能体系统中LLM的社交互动行为，特别是信任形成、信息融合与抗干扰能力，与DIU领域中构建智能体进行文档理解与决策有高度相关性。其提出的KAIROS基准可直接用于评估DIU Agent在多源信息输入下的推理鲁棒性，尤其适用于模拟多个OCR或实体识别模块间的信息交互与冲突解决场景。虽然未直接针对文档图像，但其对多Agent协作下推理一致性与可信度的分析，对构建高可靠性的DIU Agent具有重要借鉴意义。此外，GRPO等策略可用于优化DIU Agent的规划与决策模块。代码开源加分。",
    "summary": "本文提出KAIROS基准，研究LLM在多智能体社交环境中的决策行为，包括信任建立、抗误导与同伴信息整合。通过模拟Quiz竞赛场景，系统分析不同智能体角色（专家/新手）、噪声与对抗性信息的影响，并评估提示、微调与强化学习（GRPO）等策略的效果。结果表明GRPO结合上下文与结果奖励表现最佳，但可能降低对社会影响的鲁棒性。代码与数据集已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20697v1",
    "updated": "2025-08-28T12:07:11Z",
    "published": "2025-08-28T12:07:11Z",
    "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning",
    "authors": [
      "Weitao Feng",
      "Lixu Wang",
      "Tianyi Wei",
      "Jie Zhang",
      "Chongyang Gao",
      "Sinong Zhan",
      "Peizhuo Lv",
      "Wei Dong"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20697v1.pdf",
    "comment": "Project Hompage: https://tokenbuncher.github.io/",
    "category": "Computation and Language",
    "abstract": "As large language models (LLMs) continue to grow in capability, so do the risks of harmful misuse through fine-tuning. While most prior studies assume that attackers rely on supervised fine-tuning (SFT) for such misuse, we systematically demonstrate that reinforcement learning (RL) enables adversaries to more effectively break safety alignment and facilitate advanced harmful task assistance, under matched computational budgets. To counter this emerging threat, we propose TokenBuncher, the first effective defense specifically targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation on which RL relies: model response uncertainty. By constraining uncertainty, RL-based fine-tuning can no longer exploit distinct reward signals to drive the model toward harmful behaviors. We realize this defense through entropy-as-reward RL and a Token Noiser mechanism designed to prevent the escalation of expert-domain harmful capabilities. Extensive experiments across multiple models and RL algorithms show that TokenBuncher robustly mitigates harmful RL fine-tuning while preserving benign task utility and finetunability. Our results highlight that RL-based harmful fine-tuning poses a greater systemic risk than SFT, and that TokenBuncher provides an effective and general defense.",
    "score": 5,
    "reason": "该论文聚焦于防御强化学习微调带来的有害行为，属于LLM安全方向。虽然其提出的TokenBuncher机制对LLM的鲁棒性有重要意义，但其核心贡献是安全防御，而非推理增强或任务性能提升。与DIU、VLM中inference scaling或agent系统直接关联较弱，无法直接迁移至文档理解中的多步推理或结构化信息抽取场景。尽管开源且发表在CL领域（非CCF A类），但相关性不足，仅在通用LLM安全性层面有间接参考价值。",
    "summary": "本文提出TokenBuncher，一种针对强化学习微调导致有害行为的新型防御机制。通过抑制模型响应不确定性并引入熵作为奖励的RL框架与Token Noiser，有效防止攻击者利用RL进行有害任务辅助，同时保持模型在良性任务上的可用性。实验表明其对多种模型和RL算法均具鲁棒性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19724v2",
    "updated": "2025-08-28T12:05:33Z",
    "published": "2025-08-27T09:34:28Z",
    "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks",
    "authors": [
      "Aritra Dutta",
      "Swapnanil Mukherjee",
      "Deepanway Ghosal",
      "Somak Aditya"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19724v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Commonsense visual-question answering often hinges on knowledge that is missing from the image or the question. Small vision-language models (sVLMs) such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative counterparts. To study the effect of careful commonsense knowledge integration on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural language facts, (ii) prompts an LLM to craft natural language explanations, and (iii) feeds both signals to sVLMs respectively across two commonsense VQA datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts retrieved using a fine-tuned ColBERTv2 and an object information-enriched prompt yield explanations that largely cut down hallucinations, while lifting the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional finetuning using noise-robust losses (such as symmetric cross entropy and generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our findings expose when LLM-based commonsense knowledge beats retrieval from commonsense knowledge bases, how noise-aware training stabilises small models in the context of external knowledge augmentation, and why parameter-efficient commonsense reasoning is now within reach for 250M models.",
    "score": 8,
    "reason": "该论文提出NLKI框架，通过引入外部自然语言知识与LLM生成的解释来增强小型VLM在常识视觉问答中的表现，直接解决了DIU中因缺乏背景知识导致的理解偏差问题。其核心思想——利用LLM进行知识集成并注入小模型——可直接迁移至DIU任务中，尤其适用于文档理解中对隐含语义（如发票条款、简历格式惯例）的推理。方法具备可复用性，且在CRIC、AOKVQA等数据集上提升显著（最高+7%），虽未明确提及DIU相关任务，但其技术路径高度契合DIU中‘关系抽取’与‘语义理解’环节的瓶颈。此外，该工作虽未开源，但方法设计清晰，具有较强应用潜力。",
    "summary": "NLKI是一个轻量级自然语言知识集成框架，旨在提升小型视觉语言模型（sVLMs）在常识视觉问答任务中的性能。该框架通过ColBERTv2检索自然语言事实，并利用LLM生成解释，将两者作为额外信号输入sVLMs。实验表明，该方法在多个基准上显著提升准确率（最高+7%），有效减少幻觉，使FLAVA等模型达到中型VLM水平。同时，结合噪声鲁棒损失进一步提升稳定性。研究揭示了LLM知识优于传统KB、参数高效推理的可行性，为小型模型在复杂理解任务中应用提供新范式。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20693v1",
    "updated": "2025-08-28T11:53:45Z",
    "published": "2025-08-28T11:53:45Z",
    "title": "Leveraging Large Language Models for Generating Research Topic Ontologies: A Multi-Disciplinary Study",
    "authors": [
      "Tanay Aggarwal",
      "Angelo Salatino",
      "Francesco Osborne",
      "Enrico Motta"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20693v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Ontologies and taxonomies of research fields are critical for managing and organising scientific knowledge, as they facilitate efficient classification, dissemination and retrieval of information. However, the creation and maintenance of such ontologies are expensive and time-consuming tasks, usually requiring the coordinated effort of multiple domain experts. Consequently, ontologies in this space often exhibit uneven coverage across different disciplines, limited inter-domain connectivity, and infrequent updating cycles. In this study, we investigate the capability of several large language models to identify semantic relationships among research topics within three academic domains: biomedicine, physics, and engineering. The models were evaluated under three distinct conditions: zero-shot prompting, chain-of-thought prompting, and fine-tuning on existing ontologies. Additionally, we assessed the cross-domain transferability of fine-tuned models by measuring their performance when trained in one domain and subsequently applied to a different one. To support this analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over 8,000 relationships extracted from the most widely adopted taxonomies in the three disciplines considered in this study: MeSH, PhySH, and IEEE. Our experiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent performance across all disciplines.",
    "score": 4,
    "reason": "该论文研究大语言模型生成科研主题本体，虽涉及LLM推理技术（如chain-of-thought），但核心任务是构建学科分类体系，与文档图像理解（DIU）无直接关联。尽管使用了inference scaling相关技术（如CoT），但其应用场景为学术知识组织，无法直接迁移至DIU中的实体识别、关系抽取或视觉信息理解任务。此外，未提及多模态、视觉布局或文档结构处理，也未开源数据集或代码，故相关性较低。",
    "summary": "本文探讨大型语言模型在生成跨学科研究主题本体方面的潜力，通过零样本提示、思维链提示和微调三种方式评估模型在生物医学、物理和工程领域的语义关系识别能力，并引入新数据集PEM-Rel-8K用于训练与评估。结果表明微调后模型表现优异，具备跨领域迁移能力。"
  },
  {
    "id": "http://arxiv.org/abs/2508.16599v2",
    "updated": "2025-08-28T11:53:23Z",
    "published": "2025-08-09T16:29:10Z",
    "title": "Humans Perceive Wrong Narratives from AI Reasoning Texts",
    "authors": [
      "Mosh Levy",
      "Zohar Elyoseph",
      "Yoav Goldberg"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.16599v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "A new generation of AI models generates step-by-step reasoning text before producing an answer. This text appears to offer a human-readable window into their computation process, and is increasingly relied upon for transparency and interpretability. However, it is unclear whether human understanding of this text matches the model's actual computational process. In this paper, we investigate a necessary condition for correspondence: the ability of humans to identify which steps in a reasoning text causally influence later steps. We evaluated humans on this ability by composing questions based on counterfactual measurements and found a significant discrepancy: participant accuracy was only 29%, barely above chance (25%), and remained low (42%) even when evaluating the majority vote on questions with high agreement. Our results reveal a fundamental gap between how humans interpret reasoning texts and how models use it, challenging its utility as a simple interpretability tool. We argue that reasoning texts should be treated as an artifact to be investigated, not taken at face value, and that understanding the non-human ways these models use language is a critical research direction.",
    "score": 3,
    "reason": "该论文探讨人类对AI推理文本的理解能力，揭示了人类与模型推理过程之间的认知鸿沟。虽然涉及推理（reasoning）这一核心方向，但其研究重点是人类对AI推理文本的误读问题，属于对推理可解释性的批判性分析，而非提升推理性能或构建有效推理机制的技术创新。论文未提出可直接应用于DIU任务的新方法，也未涉及VLM或LLM在推理阶段的scaling技术（如CoT、ToT、MCTS等），更未触及agent系统中推理模块的设计。因此，尽管主题相关，但对DIU领域的直接应用价值有限，仅具有理论警示意义。",
    "summary": "本文研究人类理解AI生成的逐步推理文本的能力，发现人类在识别推理步骤间的因果关系时表现极差（准确率29%），远低于预期，表明推理文本作为可解释性工具存在根本缺陷。作者呼吁应将推理文本视为需深入研究的产物，而非可直接信任的解释。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20691v1",
    "updated": "2025-08-28T11:50:22Z",
    "published": "2025-08-28T11:50:22Z",
    "title": "MobileCLIP2: Improving Multi-Modal Reinforced Training",
    "authors": [
      "Fartash Faghri",
      "Pavan Kumar Anasosalu Vasu",
      "Cem Koc",
      "Vaishaal Shankar",
      "Alexander Toshev",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20691v1.pdf",
    "comment": "TMLR August 2025",
    "category": "Computation and Language",
    "abstract": "Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\\times$ smaller and improves on DFN ViT-L/14 at 2.5$\\times$ lower latency. We release our pretrained models (https://github.com/apple/ml-mobileclip) and the data generation code (https://github.com/apple/ml-mobileclip-dr). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing.",
    "score": 8,
    "reason": "MobileCLIP2 提出了一种改进的多模态强化训练方法，显著提升了轻量级图像-文本模型在零样本任务上的性能，尤其在低延迟场景下表现突出。该工作对 CLIP 教师集成和 captioner 教师的优化具有实际价值，其开源代码与可扩展的数据生成流程为 DIU 领域中高效、轻量化视觉语言模型的构建提供了重要支持。虽然论文本身未直接针对文档理解，但其在低延迟、高精度视觉表征方面的突破，可直接应用于 DIU 中的视觉编码阶段，尤其适合移动端或实时文档处理场景。TMLR 是高质量期刊，且模型已开源，具备较强实用价值。",
    "summary": "MobileCLIP2 通过优化多模态强化训练策略，提升了轻量级图像-文本模型在零样本分类任务中的性能。改进包括使用 DFN 数据集训练更优的 CLIP 教师集成与多样化 captioner 教师，并引入温度调优与合成标题融合机制。新模型 MobileCLIP2 在保持极低延迟（3-15ms）和小参数量（50-150M）的同时，实现了 ImageNet-1k 零样本准确率的显著提升，其中 MobileCLIP2-S4 在精度上媲美更大模型且延迟更低。作者开源了预训练模型与数据生成代码，支持可扩展的多教师知识蒸馏流程。"
  },
  {
    "id": "http://arxiv.org/abs/2406.16464v6",
    "updated": "2025-08-28T11:35:48Z",
    "published": "2024-06-24T09:13:42Z",
    "title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection",
    "authors": [
      "Junjie Chen",
      "Hang Yu",
      "Subin Huang",
      "Sanmin Liu",
      "Linfeng Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.16464v6.pdf",
    "comment": "ACM TOMM (Under Review); Code and data are available at https://github.com/CoderChen01/InterCLIP-MEP",
    "category": "Computation and Language",
    "abstract": "Sarcasm in social media, often expressed through text-image combinations, poses challenges for sentiment analysis and intention mining. Current multi-modal sarcasm detection methods have been demonstrated to overly rely on spurious cues within the textual modality, revealing a limited ability to genuinely identify sarcasm through nuanced text-image interactions. To solve this problem, we propose InterCLIP-MEP, which introduces Interactive CLIP (InterCLIP) with an efficient training strategy to extract enriched text-image representations by embedding cross-modal information directly into each encoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a dynamic dual-channel memory that stores valuable test sample knowledge during inference, acting as a non-parametric classifier for robust sarcasm recognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP achieves state-of-the-art performance, with significant accuracy and F1 score improvements on MMSD and MMSD2.0. Our code is available at https://github.com/CoderChen01/InterCLIP-MEP.",
    "score": 3,
    "reason": "该论文聚焦于多模态讽刺检测，虽涉及文本与图像的交互，但应用场景为社交媒体讽刺识别，与文档图像理解（DIU）在任务目标、数据分布和语义结构上差异显著。其提出的InterCLIP-MEP架构虽引入了跨模态交互与记忆增强机制，但主要用于情感分析而非信息抽取或文档结构理解，难以直接迁移至DIU中的实体识别或关系抽取任务。尽管代码开源且发表于ACM TOMM（计算与语言领域），但与DIU核心需求关联较弱，仅具间接参考价值。",
    "summary": "本文提出InterCLIP-MEP模型用于多模态讽刺检测，通过交互式CLIP架构增强文本与图像的联合表示，并设计动态双通道记忆预测器以提升推理鲁棒性。实验表明其在MMSD和MMSD2.0数据集上达到SOTA性能，代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2502.19074v2",
    "updated": "2025-08-28T11:20:32Z",
    "published": "2025-02-26T11:56:43Z",
    "title": "Improving the quality of Web-mined Parallel Corpora of Low-Resource Languages using Debiasing Heuristics",
    "authors": [
      "Aloka Fernando",
      "Nisansa de Silva",
      "Menan Velyuthan",
      "Charitha Rathnayake",
      "Surangika Ranathunga"
    ],
    "pdf_url": "https://arxiv.org/pdf/2502.19074v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Parallel Data Curation (PDC) techniques aim to filter out noisy parallel sentences from the web-mined corpora. Prior research has demonstrated that ranking sentence pairs using similarity scores on sentence embeddings derived from Pre-trained Multilingual Language Models (multiPLMs) and training the NMT systems with the top-ranked samples, produces superior NMT performance than when trained using the full dataset. However, previous research has shown that the choice of multiPLM significantly impacts the ranking quality. This paper investigates the reasons behind this disparity across multiPLMs. Using the web-mined corpora CCMatrix and CCAligned for En$\\rightarrow$Si, En$\\rightarrow$Ta and Si$\\rightarrow$Ta, we show that different multiPLMs (LASER3, XLM-R, and LaBSE) are biased towards certain types of sentences, which allows noisy sentences to creep into the top-ranked samples. We show that by employing a series of heuristics, this noise can be removed to a certain extent. This results in improving the results of NMT systems trained with web-mined corpora and reduces the disparity across multiPLMs.",
    "score": 2,
    "reason": "该论文聚焦于低资源语言平行语料库的去偏过滤，虽涉及多语言模型和文本质量提升，但核心任务为机器翻译数据清洗，与文档图像理解（DIU）、视觉多模态模型（VLM）、推理增强（inference scaling）或智能体（agent）系统无直接关联。其方法不适用于图像中的文本识别、布局理解或视觉-语言联合推理场景，也无法直接迁移至DIU任务中。因此，相关性极低。",
    "summary": "本文研究如何通过去偏启发式方法提升从网络挖掘的低资源语言平行语料的质量，发现不同多语言预训练模型在句子排序上存在偏差，导致噪声进入高质量样本，提出一系列策略减少此类偏差，从而提升神经机器翻译性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20655v1",
    "updated": "2025-08-28T11:01:33Z",
    "published": "2025-08-28T11:01:33Z",
    "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
    "authors": [
      "Sihan Yang",
      "Chenhang Cui",
      "Zihao Zhao",
      "Yiyang Zhou",
      "Weilong Yan",
      "Ying Wei",
      "Huaxiu Yao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20655v1.pdf",
    "comment": "EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations--where generated outputs are not grounded in the visual input--and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs.",
    "score": 8,
    "reason": "该论文提出基于去偏自评估的对齐方法，直接针对LVLMs中的幻觉问题，与DIU核心挑战高度相关。其无需外部标注、可内生自评的机制，能显著提升视觉-语言对齐能力，对构建鲁棒的DIU系统（尤其是减少OCR后文本误判导致的语义偏差）有直接应用潜力。EMNLP 2025 Findings为CCF B类会议，虽非A类，但主题高度契合；论文未提开源，扣分。整体属于可直接迁移至DIU中提升模型可信度的关键技术。",
    "summary": "本文提出一种无需外部资源的去偏自判断机制，通过LVLM内部生成自评估分数来提升视觉-语言对齐效果，有效降低幻觉并增强安全性，实验表明优于传统对齐方法，对DIU任务中提升模型可靠性具有重要价值。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20637v1",
    "updated": "2025-08-28T10:35:44Z",
    "published": "2025-08-28T10:35:44Z",
    "title": "GDS Agent: A Graph Algorithmic Reasoning Agent",
    "authors": [
      "Borun Shi",
      "Ioannis Panagiotas"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20637v1.pdf",
    "comment": "Technical report",
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We also introduce a new benchmark that evaluates intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.",
    "score": 8,
    "reason": "该论文提出GDS Agent，将图算法作为工具集成到LLM代理系统中，通过MCP服务器实现对大规模图结构数据的推理与处理。虽然其核心是图算法推理，但其方法论高度契合DIU场景——文档常包含结构化关系（如简历中的职位-时间关系、账单中的项目-金额关系），这些本质上是图结构。该工作展示了如何通过工具调用+推理框架实现复杂逻辑推理，可直接迁移至DIU中的关系抽取与跨字段语义理解任务。尽管是技术报告而非顶会论文，但其创新性高，且为LLM代理在结构化信息处理上的应用提供了范例，具有很强的DIU迁移潜力。未开源，但提出了新基准，值得阅读。",
    "summary": "GDS Agent是一个基于LLM的智能体系统，通过引入图算法工具集和预/后处理机制，在MCP服务器支持下实现对图结构数据的复杂推理。该系统能回答涉及图算法的开放问题，并提供中间步骤评估。论文展示了其在多种图任务上的有效性，并分析了局限性与未来方向，为DIU中结构化关系建模提供了可借鉴的代理架构。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19720v2",
    "updated": "2025-08-28T10:00:55Z",
    "published": "2025-08-27T09:30:24Z",
    "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models",
    "authors": [
      "Yilin Wang",
      "Heng Wang",
      "Yuyang Bai",
      "Minnan Luo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19720v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "In Large Language Models (LLMs) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS.",
    "score": 9,
    "reason": "该论文提出CSKS框架，通过轻量级代理模型持续调节LLM对上下文知识的敏感度，解决了大模型中参数知识与上下文知识冲突的问题。该技术直接适用于DIU任务中需要动态权衡文档上下文信息与预训练知识的场景（如实体识别或关系抽取时对上下文依赖性的精细控制），尤其在处理模糊或矛盾信息时具有显著优势。方法无需修改主模型权重，可适配黑盒LLM，具备良好的实用性。论文已开源，且研究问题与DIU中的推理一致性、上下文感知能力高度相关，是inference scaling与reasoning增强方向的重要进展。",
    "summary": "本文提出CSKS（Continuously Steering Knowledge Sensitivity），一种通过两个小型代理模型生成输出分布差异来无损调节大型语言模型对上下文知识敏感度的轻量级框架。该方法支持连续、精确地控制LLM优先采用上下文知识还是参数化知识，在解决知识冲突方面表现优异。实验验证了其在合成数据和真实冲突数据集上的有效性，并已开源代码与数据。"
  },
  {
    "id": "http://arxiv.org/abs/2506.11752v2",
    "updated": "2025-08-28T09:45:44Z",
    "published": "2025-06-13T13:05:41Z",
    "title": "DART: Distilling Autoregressive Reasoning to Silent Thought",
    "authors": [
      "Nan Jiang",
      "Ziming Wu",
      "De-Chuan Zhan",
      "Fuming Lai",
      "Shaobing Lian"
    ],
    "pdf_url": "https://arxiv.org/pdf/2506.11752v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose \\textbf{DART} (\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent \\textbf{T}hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART offers significant performance gains compared with existing non-autoregressive baselines without extra inference latency, serving as a feasible alternative for efficient reasoning.",
    "score": 9,
    "reason": "DART提出了一种将自回归推理（CoT）蒸馏为非自回归‘静默思维’（ST）的新框架，显著降低推理延迟且不增加额外计算开销。该技术直接针对LLM在推理阶段的效率瓶颈，属于inference scaling范畴中极具潜力的优化方法。其核心思想——通过轻量级模块对齐隐状态、实现高效推理——可直接迁移至DIU任务中，尤其是在需要多步逻辑推理的文档理解场景（如复杂表单关系推断、跨字段语义整合）中具有极高应用价值。尽管未明确提及DIU，但其技术路径与DIU中提升VLM/LLM推理能力的目标高度契合，是当前推理加速领域的前沿工作。",
    "summary": "DART是一种自蒸馏框架，将传统的自回归Chain-of-Thought推理转化为非自回归的Silent Thought（ST），通过引入轻量级Reasoning Evolvement Module（REM）使ST token在推理时逐步演化为信息丰富的嵌入表示。训练时保留CoT路径进行监督，推理时仅激活ST路径，实现零额外延迟的高性能推理，适用于低延迟、高精度的复杂任务场景。"
  },
  {
    "id": "http://arxiv.org/abs/2506.06294v2",
    "updated": "2025-08-28T09:38:11Z",
    "published": "2025-05-17T14:45:13Z",
    "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning",
    "authors": [
      "Yunqing Liu",
      "Wenqi Fan",
      "Xiaoyong Wei",
      "Qing Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2506.06294v2.pdf",
    "comment": "Accepted to EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \\textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.",
    "score": 2,
    "reason": "该论文聚焦于蛋白质结构表示学习，属于生物信息学领域，与文档图像理解（DIU）、视觉多模态模型（VLM）、大语言模型（LLM）推理增强或智能体系统无直接关联。尽管其提出全局-局部结构感知的框架具有一定创新性，但应用场景、数据模态和任务目标均与DIU无关，无法直接迁移应用。EMNLP接受虽为CCF A类会议，但领域不匹配，故仅给予较低评分。",
    "summary": "GLProtein提出一种融合全局蛋白结构相似性与局部氨基酸分子细节的蛋白质预训练框架，通过掩码建模、三元组结构相似性评分、3D距离编码和基于子结构的氨基酸编码，提升蛋白质相互作用预测和接触预测等生物任务性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.07999v2",
    "updated": "2025-08-28T09:31:57Z",
    "published": "2025-08-11T14:03:09Z",
    "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
    "authors": [
      "Ryan Wong",
      "Jiawei Wang",
      "Junjie Zhao",
      "Li Chen",
      "Yan Gao",
      "Long Zhang",
      "Xuan Zhou",
      "Zuo Wang",
      "Kai Xiang",
      "Ge Zhang",
      "Wenhao Huang",
      "Yang Wang",
      "Ke Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.07999v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such \"wide-context\" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\\%, with the best performer reaching just 5\\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/",
    "score": 9,
    "reason": "该论文提出WideSearch基准，专为评估智能体在大规模信息检索任务中的可靠性而设计，与DIU领域高度相关。DIU中常需从文档中提取大量结构化信息并组织输出，这与WideSearch的任务（大规模原子信息收集、组织）高度一致。论文聚焦于agentic search系统在宽范围信息获取中的表现，直接关联到构建DIU agent的核心能力——信息收集与组织。其开源的高质量数据集和严谨的评估流程极具参考价值，且研究方向契合当前agent发展前沿。虽未直接涉及视觉或文档理解，但其对多步推理、任务规划、验证机制的设计可直接迁移至DIU agent系统中，是构建高效DIU智能体的关键支撑工作。",
    "summary": "本文提出WideSearch，一个用于评估智能体在大规模信息检索任务中可靠性的新基准。该基准包含200个来自15个不同领域的手动标注问题（中英文各100），要求智能体系统收集大量原子级信息并组织成结构化输出。通过五阶段质量控制确保任务的难度、完整性和可验证性。实验对比了10多个先进agentic系统，结果显示整体成功率接近0%，最佳仅为5%，远低于人类水平（接近100%）。研究揭示了现有智能体在宽范围信息获取方面的严重缺陷，强调了未来研究方向。数据集与评估框架已开源，为构建高性能DIU agent提供了重要工具与基线。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20583v1",
    "updated": "2025-08-28T09:20:47Z",
    "published": "2025-08-28T09:20:47Z",
    "title": "A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models",
    "authors": [
      "Soham Petkar",
      "Hari Aakash K",
      "Anirudh Vempati",
      "Akshit Sinha",
      "Ponnurangam Kumarauguru",
      "Chirag Agarwal"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20583v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Developments in Graph-Language Models (GLMs) aim to integrate the structural reasoning capabilities of Graph Neural Networks (GNNs) with the semantic understanding of Large Language Models (LLMs). However, we demonstrate that current evaluation benchmarks for GLMs, which are primarily repurposed node-level classification datasets, are insufficient to assess multimodal reasoning. Our analysis reveals that strong performance on these benchmarks is achievable using unimodal information alone, suggesting that they do not necessitate graph-language integration. To address this evaluation gap, we introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed to evaluate multimodal reasoning at various complexity levels. Our benchmark employs a synthetic graph generation pipeline paired with questions that require joint reasoning over structure and textual semantics. We perform a thorough evaluation of representative GLM architectures and find that soft-prompted LLM baselines perform on par with GLMs that incorporate a full GNN backbone. This result calls into question the architectural necessity of incorporating graph structure into LLMs. We further show that GLMs exhibit significant performance degradation in tasks that require structural reasoning. These findings highlight limitations in the graph reasoning capabilities of current GLMs and provide a foundation for advancing the community toward explicit multimodal reasoning involving graph structure and language.",
    "score": 6,
    "reason": "该论文针对图-语言模型（GLM）的评估基准问题提出新见解，引入CLEGR基准以更严格地测试结构与语义联合推理能力。虽然其核心关注点是图结构推理而非文档图像理解，但其对多模态推理评估方法的反思对DIU领域具有间接启发意义，尤其是在布局建模和关系抽取任务中如何设计有效评估标准方面。然而，论文未直接涉及文档图像、OCR或视觉信息抽取，且未使用视觉数据或真实文档场景，因此对DIU的直接应用价值有限。虽为CL领域顶会潜力论文，但相关性不足，故评分中等。",
    "summary": "本文指出当前图-语言模型（GLM）的评估基准主要依赖节点分类任务，无法有效检验图结构与语言的联合推理能力。为此，作者提出CLEGR基准，采用合成图与需结构-语义协同推理的问题进行评估。实验发现，仅用LLM软提示即可达到与完整GNN架构相当的性能，质疑了图结构在GLM中的必要性，并揭示当前GLMs在结构推理上的局限性，为多模态推理评估提供了新范式。"
  },
  {
    "id": "http://arxiv.org/abs/2503.11302v4",
    "updated": "2025-08-28T09:20:39Z",
    "published": "2025-03-14T11:11:03Z",
    "title": "Are formal and functional linguistic mechanisms dissociated in language models?",
    "authors": [
      "Michael Hanna",
      "Yonatan Belinkov",
      "Sandro Pezzelle"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.11302v4.pdf",
    "comment": "To appear in Computational Linguistics. Pre-MIT Press publication version. 40 pages, 14 figures, 3 tables. Code available at https://github.com/hannamw/formal-functional-dissociation",
    "category": "Computation and Language",
    "abstract": "Although large language models (LLMs) are increasingly capable, these capabilities are unevenly distributed: they excel at formal linguistic tasks, such as producing fluent, grammatical text, but struggle more with functional linguistic tasks like reasoning and consistent fact retrieval. Inspired by neuroscience, recent work suggests that to succeed on both formal and functional linguistic tasks, LLMs should use different mechanisms for each; such localization could either be built-in or emerge spontaneously through training. In this paper, we ask: do current models, with fast-improving functional linguistic abilities, exhibit distinct localization of formal and functional linguistic mechanisms? We answer this by finding and comparing the \"circuits\", or minimal computational subgraphs, responsible for various formal and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that while there is indeed little overlap between circuits for formal and functional tasks, there is also little overlap between formal linguistic tasks, as exists in the human brain. Thus, a single formal linguistic network, unified and distinct from functional task circuits, remains elusive. However, in terms of cross-task faithfulness - the ability of one circuit to solve another's task - we observe a separation between formal and functional mechanisms, suggesting that shared mechanisms between formal tasks may exist.",
    "score": 6,
    "reason": "该论文探讨了大语言模型中形式语言机制与功能语言机制的分离性，涉及LLM内部计算结构的可解释性分析，对理解模型在推理、事实一致性等DIU相关任务中的表现有间接启发。虽然未直接涉及DIU、VLM或agent，但其关于'跨任务忠实性'和'电路分离'的发现，可能为未来构建更可靠、模块化的DIU agent（如将OCR/实体识别/关系抽取拆解为独立子电路）提供理论支持。代码开源且发表于计算语言学顶刊（Computational Linguistics），具备一定可信度和复现价值。但因缺乏与DIU、多模态、inference scaling或agent的直接关联，得分中等。",
    "summary": "本文通过分析5个LLM在10项任务中的最小计算子图（circuits），研究形式语言任务（如语法正确性）与功能语言任务（如推理、事实检索）是否共享机制。结果发现两类任务的电路重叠极低，且形式任务间也无明显共享结构，表明当前模型尚未形成统一的形式语言处理网络。尽管存在部分跨任务忠实性，但整体仍呈现机制分离。该研究有助于理解LLM内部运作逻辑，对构建可解释、模块化的DIU系统具有潜在参考价值。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20577v1",
    "updated": "2025-08-28T09:14:23Z",
    "published": "2025-08-28T09:14:23Z",
    "title": "MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training",
    "authors": [
      "Yang Luo",
      "Zangwei Zheng",
      "Ziheng Qin",
      "Zirui Zhu",
      "Yong Liu",
      "Yang You"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20577v1.pdf",
    "comment": "ICML 2025",
    "category": "Computation and Language",
    "abstract": "Large-batch training has become a cornerstone in accelerating the training of deep neural networks, yet it poses challenges in optimization and generalization. Existing optimizers like AdamW present performance degradation during language models' large-batch training, due to the information bottleneck in attention layers caused by the sharp increase of max attention logit. While the LAMB optimizer partially addresses this issue, some attention layers still face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are less effective in directly influencing the max value of query/key weights. Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks relationships of weight values within rows or columns. Building on these observations, we propose a novel optimizer, MERIT, which leverages the max-norm to calculate the trust ratio to constrain the max attention logit more effectively. Moreover, we further construct element-wise trust ratios to provide more robust update scaling by focusing on local weight structures. Extensive experiments of large-batch training across various sizes of GPT-2 models demonstrate the superior performance of MERIT. Notably, during the training of GPT-2 Medium, MERIT enables a 6k batch size without any performance degradation compared to the standard batch size (480) with 48B training tokens. This work highlights the importance of considering the max attention logit and finer-granularity trust ratio in large-batch training. It successfully improves the training stability and paves the way for larger batch usage, enabling faster development and iteration of large language models. Code is available at https://github.com/NUS-HPC-AI-Lab/MERIT.",
    "score": 5,
    "reason": "该论文提出一种用于大批次训练语言模型的新优化器MERIT，聚焦于解决注意力层中最大logit值带来的信息瓶颈问题。虽然其技术对LLM训练效率有提升意义，但属于底层训练优化方法，与DIU任务无直接关联。尽管ICML 2025为顶会且代码开源，但其改进主要面向模型训练阶段的稳定性与批处理规模扩展，无法直接应用于文档图像理解中的OCR、实体识别或关系抽取等模块，也未涉及多模态、推理增强或agent系统设计。因此，虽具理论价值，但对DIU领域实际应用的迁移性较弱。",
    "summary": "本文提出MERIT优化器，通过引入最大范数和逐元素信任比率来改善大批次训练下LLM的优化性能，有效缓解注意力层中最大logit增长导致的退化问题，在GPT-2 Medium上实现6k大批次训练无性能损失。实验表明其可显著提升训练稳定性和吞吐量。代码已开源，发表于ICML 2025。"
  },
  {
    "id": "http://arxiv.org/abs/2407.08952v6",
    "updated": "2025-08-28T09:09:46Z",
    "published": "2024-07-12T03:15:01Z",
    "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework for Few-shot Fake News Detection",
    "authors": [
      "Ye Liu",
      "Jiajun Zhu",
      "Xukai Liu",
      "Haoyu Tang",
      "Yanghai Zhang",
      "Kai Zhang",
      "Xiaofang Zhou",
      "Enhong Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2407.08952v6.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news from real ones in extremely low-resource scenarios. This task has garnered increased attention due to the widespread dissemination and harmful impact of fake news on social media. Large Language Models (LLMs) have demonstrated competitive performance with the help of their rich prior knowledge and excellent in-context learning abilities. However, existing methods face significant limitations, such as the Understanding Ambiguity and Information Scarcity, which significantly undermine the potential of LLMs. To address these shortcomings, we propose a Dual-perspective Knowledge-guided Fake News Detection (DKFND) model, designed to enhance LLMs from both inside and outside perspectives. Specifically, DKFND first identifies the knowledge concepts of each news article through a Detection Module. Subsequently, DKFND creatively designs an Investigation Module to retrieve inside and outside valuable information concerning to the current news, followed by another Judge Module to evaluate the relevance and confidence of them. Finally, a Determination Module further derives two respective predictions and obtain the final result. Extensive experiments on two public datasets show the efficacy of our proposed method, particularly in low-resource settings.",
    "score": 3,
    "reason": "该论文聚焦于少样本虚假新闻检测，虽使用了LLM并引入知识引导机制，但其核心任务与文档图像理解（DIU）无直接关联。尽管涉及推理和多模块设计，但应用场景（社交媒体新闻真实性判断）与DIU的文档结构解析、实体关系抽取等任务差异显著，且未涉及视觉信息、布局分析或OCR/表格处理等DIU关键技术。虽然采用多阶段框架（检测-调查-判断-决定），但其设计动机与DIU中对文本+布局联合建模的需求不一致，难以直接迁移至DIU领域。",
    "summary": "本文提出DKFND框架用于少样本虚假新闻检测，通过检测模块识别新闻中的知识概念，再利用调查模块从内外部检索相关信息，经判断模块评估相关性与置信度，最终由决定模块输出结果。该方法旨在缓解信息稀缺和语义模糊问题，在低资源场景下表现优异，但主要面向文本型新闻真实性判断，与文档图像理解任务无直接应用关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20567v1",
    "updated": "2025-08-28T09:06:38Z",
    "published": "2025-08-28T09:06:38Z",
    "title": "KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling",
    "authors": [
      "Yangfan Wang",
      "Jie Liu",
      "Chen Tang",
      "Lian Yan",
      "Jingchi Jiang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20567v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Multi-hop question answering faces substantial challenges due to data sparsity, which increases the likelihood of language models learning spurious patterns. To address this issue, prior research has focused on diversifying question generation through content planning and varied expression. However, these approaches often emphasize generating simple questions and neglect the integration of essential knowledge, such as relevant sentences within documents. This paper introduces the Knowledge Composition Sampling (KCS), an innovative framework designed to expand the diversity of generated multi-hop questions by sampling varied knowledge compositions within a given context. KCS models the knowledge composition selection as a sentence-level conditional prediction task and utilizes a probabilistic contrastive loss to predict the next most relevant piece of knowledge. During inference, we employ a stochastic decoding strategy to effectively balance accuracy and diversity. Compared to competitive baselines, our KCS improves the overall accuracy of knowledge composition selection by 3.9%, and its application for data augmentation yields improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available at: https://github.com/yangfanww/kcs.",
    "score": 6,
    "reason": "该论文聚焦于多跳问题生成中的知识组合采样，虽涉及推理与多样性提升，但核心任务是问答数据增强，而非文档图像理解。其方法虽与推理相关（如概率对比损失、随机解码），但未直接面向视觉-文本联合理解或DIU场景。尽管使用了类似inference scaling的思想（如stochastic decoding），但应用场景为纯文本QA，无法直接迁移至DIU。因此仅具间接参考价值，不建议优先阅读。",
    "summary": "本文提出知识组合采样（KCS）框架，通过建模句子级条件预测和概率对比损失，实现多跳问题生成中多样化知识组合的采样。采用随机解码策略平衡准确率与多样性，在HotpotQA和2WikiMultihopQA上取得性能提升。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.18642v2",
    "updated": "2025-08-28T08:59:07Z",
    "published": "2025-08-26T03:40:06Z",
    "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing",
    "authors": [
      "Jianxing Liao",
      "Tian Zhang",
      "Xiao Feng",
      "Yusong Zhang",
      "Rui Yang",
      "Haorui Wang",
      "Bosi Wen",
      "Ziying Wang",
      "Runzhi Shi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.18642v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models are extensively utilized in creative writing applications. Creative writing requires a balance between subjective writing quality (e.g., literariness and emotional expression) and objective constraint following (e.g., format requirements and word limits). Existing methods find it difficult to balance these two aspects: single reward strategies fail to improve both abilities simultaneously, while fixed-weight mixed-reward methods lack the ability to adapt to different writing scenarios. To address this problem, we propose Reinforcement Learning with Mixed Rewards (RLMR), utilizing a dynamically mixed reward system from a writing reward model evaluating subjective writing quality and a constraint verification model assessing objective constraint following. The constraint following reward weight is adjusted dynamically according to the writing quality within sampled groups, ensuring that samples violating constraints get negative advantage in GRPO and thus penalized during training, which is the key innovation of this proposed method. We conduct automated and manual evaluations across diverse model families from 8B to 72B parameters. Additionally, we construct a real-world writing benchmark named WriteEval for comprehensive evaluation. Results illustrate that our method achieves consistent improvements in both instruction following (IFEval from 83.36% to 86.65%) and writing quality (72.75% win rate in manual expert pairwise evaluations on WriteEval). To the best of our knowledge, RLMR is the first work to combine subjective preferences with objective verification in online RL training, providing an effective solution for multi-dimensional creative writing optimization.",
    "score": 3,
    "reason": "该论文聚焦于创意写作中的强化学习奖励机制设计，虽涉及多维目标优化与动态奖励权重，但其应用场景为文学创作，与文档图像理解（DIU）、视觉多模态模型（VLM）或推理增强技术（如CoT、ToT）无直接关联。尽管其动态混合奖励思想在理论上可启发DIU中质量与约束的权衡，但缺乏与视觉信息、布局理解、OCR后处理或文档结构建模的结合，无法直接迁移应用。且未提及开源或顶会发表，相关性较弱。",
    "summary": "本文提出RLMR方法，通过动态调整主观写作质量与客观格式约束的奖励权重，在创意写作任务中实现更优的指令遵循与文笔质量平衡。采用基于GRPO的在线强化学习框架，利用写作质量评估模型与约束验证模型协同优化，显著提升多维度指标。构建了真实世界写作评测基准WriteEval进行验证。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20559v1",
    "updated": "2025-08-28T08:51:51Z",
    "published": "2025-08-28T08:51:51Z",
    "title": "Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search",
    "authors": [
      "Zeyu Xiong",
      "Yixuan Nan",
      "Li Gao",
      "Hengzhu Tang",
      "Shuaiqiang Wang",
      "Junfeng Wang",
      "Dawei Yin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20559v1.pdf",
    "comment": "CIKM'25",
    "category": "Computation and Language",
    "abstract": "In the dynamic landscape of large-scale web search, Query-Driven Text Summarization (QDTS) aims to generate concise and informative summaries from textual documents based on a given query, which is essential for improving user engagement and facilitating rapid decision-making. Traditional extractive summarization models, based primarily on ranking candidate summary segments, have been the dominant approach in industrial applications. However, these approaches suffer from two key limitations: 1) The multi-stage pipeline often introduces cumulative information loss and architectural bottlenecks due to its weakest component; 2) Traditional models lack sufficient semantic understanding of both user queries and documents, particularly when dealing with complex search intents. In this study, we propose a novel framework to pioneer the application of generative models to address real-time QDTS in industrial web search. Our approach integrates large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to transform a lightweight model with only 0.1B parameters into a domain-specialized QDTS expert. Evaluated on multiple industry-relevant metrics, our model outperforms the production baseline and achieves a new state of the art. Furthermore, it demonstrates excellent deployment efficiency, requiring only 334 NVIDIA L20 GPUs to handle \\textasciitilde50,000 queries per second under 55~ms average latency per query.",
    "score": 3,
    "reason": "该论文聚焦于大规模网络搜索中的查询驱动文本摘要，虽涉及生成式模型和实时性优化，但其核心场景是通用网页内容摘要，与文档图像理解（DIU）在输入模态（纯文本 vs 图像+布局）、任务目标（摘要生成 vs 实体识别+关系抽取）上差异显著。尽管提到了生成式模型和推理优化技术，但未涉及视觉理解、布局分析或文档结构建模，且无直接应用于DIU的潜力。CIKM为CCF B类会议，影响力有限；虽有开源可能但未明确提及。因此不具直接迁移价值。",
    "summary": "本文提出一种基于轻量级生成模型的实时查询驱动文本摘要框架，通过模型蒸馏、监督微调、直接偏好优化和前瞻解码等技术，在仅0.1B参数下实现工业级高效摘要生成，支持每秒处理近5万查询，平均延迟低于55ms，性能超越现有基线。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20557v1",
    "updated": "2025-08-28T08:51:14Z",
    "published": "2025-08-28T08:51:14Z",
    "title": "Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data",
    "authors": [
      "Jiahao Xiao",
      "Jiangming Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20557v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "The widespread success of pre-trained language models has established a new training paradigm, where a global PLM is fine-tuned using task-specific data from local clients. The local data are highly different from each other and can not capture the global distribution of the whole data in real world. To address the challenges of non-IID data in real environments, privacy-preserving federated distillation has been proposed and highly investigated. However, previous experimental non-IID scenarios are primarily identified with the label (output) diversity, without considering the diversity of language domains (input) that is crucial in natural language processing. In this paper, we introduce a comprehensive set of multi-domain non-IID scenarios and propose a unified benchmarking framework that includes diverse data. The benchmark can be used to evaluate the federated learning framework in a real environment. To this end, we propose an Adaptive Federated Distillation (AdaFD) framework designed to address multi-domain non-IID challenges in both homogeneous and heterogeneous settings. Experimental results demonstrate that our models capture the diversity of local clients and achieve better performance compared to the existing works. The code for this paper is available at: https://github.com/jiahaoxiao1228/AdaFD.",
    "score": 3,
    "reason": "该论文聚焦于联邦学习中的多领域非独立同分布（non-IID）文本数据问题，提出自适应联邦蒸馏框架AdaFD。虽然涉及预训练语言模型和分布式训练，但其核心是解决跨客户端的数据异质性问题，与文档图像理解（DIU）任务无直接关联。尽管代码开源值得加分，但未涉及视觉信息、OCR、布局分析或文档结构理解等关键DIU要素。此外，论文主题属于通用NLP联邦学习范畴，与VLM、LLM推理增强（如CoT、ToT）、Agent系统或DIU专用数据集等方向均无紧密联系，无法直接迁移至DIU场景。因此，相关性较低。",
    "summary": "本文提出Adaptive Federated Distillation (AdaFD) 框架，用于应对多领域非独立同分布（non-IID）文本数据下的联邦学习挑战。作者构建了一个包含多样化语言域的统一基准框架，并在同质与异质设置下验证了AdaFD的有效性。实验表明该方法能更好地捕捉本地客户端的数据多样性并提升性能。代码已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20554v1",
    "updated": "2025-08-28T08:45:55Z",
    "published": "2025-08-28T08:45:55Z",
    "title": "Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering",
    "authors": [
      "Anastasios Nentidis",
      "Georgios Katsimpras",
      "Anastasia Krithara",
      "Martin Krallinger",
      "Miguel Rodríguez-Ortega",
      "Eduard Rodriguez-López",
      "Natalia Loukachevitch",
      "Andrey Sakhovskiy",
      "Elena Tutubalina",
      "Dimitris Dimitriadis",
      "Grigorios Tsoumakas",
      "George Giannakoulas",
      "Alexandra Bekiaridou",
      "Athanasios Samaras",
      "Giorgio Maria Di Nunzio",
      "Nicola Ferro",
      "Stefano Marchesin",
      "Marco Martinelli",
      "Gianmaria Silvello",
      "Georgios Paliouras"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20554v1.pdf",
    "comment": "26 pages, 17 tables, 1 figure",
    "category": "Computation and Language",
    "abstract": "This is an overview of the thirteenth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks, b and Synergy, and four new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task BioNNE-L on nested named entity linking in Russian and English. c) Task ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain interplay information extraction. In this edition of BioASQ, 83 competing teams participated with more than 1000 distinct submissions in total for the six different shared tasks of the challenge. Similar to previous editions, several participating systems achieved competitive performance, indicating the continuous advancement of the state-of-the-art in the field.",
    "score": 2,
    "reason": "该论文是BioASQ 2025挑战赛的概述，聚焦于生物医学领域的语义索引与问答任务，虽涉及信息抽取和命名实体识别，但其核心场景为生物医学文本而非文档图像理解（DIU）。尽管包含信息抽取任务（如GutBrainIE），但这些任务基于纯文本而非视觉文档，且未涉及OCR、布局分析或多模态模型。与DIU领域在数据模态、任务目标和应用形式上均无直接关联。此外，未提及VLM、LLM推理增强技术或agent系统，也未开源或发表于CCF A类会议。因此不具直接迁移价值。",
    "summary": "本文综述了第十三届BioASQ挑战赛（CLEF 2025）的六大任务，涵盖多语言临床摘要、嵌套命名实体链接、心血管临床编码及肠-脑互作信息抽取等生物医学NLP任务。共有83支团队参与，提交超1000次结果，展现了当前生物医学信息抽取的进展。"
  },
  {
    "id": "http://arxiv.org/abs/2405.15165v2",
    "updated": "2025-08-28T08:44:30Z",
    "published": "2024-05-24T02:44:14Z",
    "title": "SoAy: A Solution-based LLM API-using Methodology for Academic Information Seeking",
    "authors": [
      "Yuanchun Wang",
      "Jifan Yu",
      "Zijun Yao",
      "Jing Zhang",
      "Yuyang Xie",
      "Shangqing Tu",
      "Yiyang Fu",
      "Youhe Feng",
      "Jinkai Zhang",
      "Jingyao Zhang",
      "Bowen Huang",
      "Yuanyao Li",
      "Huihui Yuan",
      "Lei Hou",
      "Juanzi Li",
      "Jie Tang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2405.15165v2.pdf",
    "comment": "KDD 2025; 22 pages, 13 figures",
    "category": "Computation and Language",
    "abstract": "Applying large language models (LLMs) for academic API usage shows promise in reducing researchers' academic information seeking efforts. However, current LLM API-using methods struggle with complex API coupling commonly encountered in academic queries. To address this, we introduce SoAy, a solution-based LLM API-using methodology for academic information seeking. It uses code with a solution as the reasoning method, where a solution is a pre-constructed API calling sequence. The addition of the solution reduces the difficulty for the model to understand the complex relationships between APIs. Code improves the efficiency of reasoning. To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied by SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental results demonstrate a 34.58-75.99\\% performance improvement compared to state-of-the-art LLM API-based baselines. All datasets, codes, tuned models, and deployed online services are publicly accessible at https://github.com/RUCKBReasoning/SoAy.",
    "score": 6,
    "reason": "该论文提出一种基于解决方案的LLM API调用方法（SoAy），用于学术信息检索，核心思想是通过预构建的API调用序列（即‘solution’）来降低复杂API耦合带来的推理难度。虽然其在推理效率和性能上表现优异，并且在KDD 2025（CCF A类会议）发表，代码与数据开源，具备较高可复现性，但其应用场景聚焦于学术API调用，与文档图像理解（DIU）无直接关联。尽管其使用代码作为推理媒介、引入结构化解决方案的思想可能对DIU中的多步骤任务规划或工具调用有间接启发，但并未涉及视觉模态、文档布局理解、OCR或实体关系抽取等DIU核心问题。因此，仅具有限借鉴价值，不建议优先阅读。",
    "summary": "SoAy是一种面向学术信息查询的LLM API使用方法，通过预定义的API调用序列（即'解决方案'）提升模型在复杂API耦合场景下的推理能力。论文提出SoAyBench评估基准并实现34.58%-75.99%的性能提升，相关代码、数据与在线服务已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20532v1",
    "updated": "2025-08-28T08:17:57Z",
    "published": "2025-08-28T08:17:57Z",
    "title": "Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering",
    "authors": [
      "Anastasios Nentidis",
      "Georgios Katsimpras",
      "Anastasia Krithara",
      "Salvador Lima-López",
      "Eulàlia Farré-Maduell",
      "Martin Krallinger",
      "Natalia Loukachevitch",
      "Vera Davydova",
      "Elena Tutubalina",
      "Georgios Paliouras"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20532v1.pdf",
    "comment": "25 pages, 16 tables, 1 figure",
    "category": "Computation and Language",
    "abstract": "This is an overview of the twelfth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks b and Synergy, and two new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in Russian and English. In this edition of BioASQ, 37 competing teams participated with more than 700 distinct submissions in total for the four different shared tasks of the challenge. Similarly to previous editions, most of the participating systems achieved competitive performance, suggesting the continuous advancement of the state-of-the-art in the field.",
    "score": 2,
    "reason": "该论文为BioASQ 2024挑战赛的综述，聚焦于生物医学领域的语义索引与问答任务，主要涉及临床实体识别（MultiCardioNER）和嵌套命名实体识别（BIONNE）。虽然涉及实体识别任务，但其应用场景为多语言生物医学领域，与文档图像理解（DIU）在数据模态（文本 vs. 图像）、任务目标（医学信息抽取 vs. 文档结构与语义理解）上存在显著差异。此外，论文未涉及视觉信息、布局分析、OCR或多模态模型，也未讨论inference scaling、reasoning技术或agent系统。尽管发表于CLEF会议（有一定影响力），但内容与DIU核心方向关联极弱，无法直接迁移应用。",
    "summary": "本文概述了2024年CLEF BioASQ挑战赛的第十二届活动，涵盖四个共享任务：临床心脏学领域多语言实体识别（MultiCardioNER）、俄英双语嵌套命名实体识别（BIONNE）以及两个延续性任务（b和Synergy）。共有37支队伍参与，提交超过700项结果，展示了生物医学信息抽取领域的持续进步。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20514v1",
    "updated": "2025-08-28T07:55:06Z",
    "published": "2025-08-28T07:55:06Z",
    "title": "SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM",
    "authors": [
      "Pengjiang Li",
      "Zaitian Wang",
      "Xinhao Zhang",
      "Ran Zhang",
      "Lu Jiang",
      "Pengfei Wang",
      "Yuanchun Zhou"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20514v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Topic discovery in scientific literature provides valuable insights for researchers to identify emerging trends and explore new avenues for investigation, facilitating easier scientific information retrieval. Many machine learning methods, particularly deep embedding techniques, have been applied to discover research topics. However, most existing topic discovery methods rely on word embedding to capture the semantics and lack a comprehensive understanding of scientific publications, struggling with complex, high-dimensional text relationships. Inspired by the exceptional comprehension of textual information by large language models (LLMs), we propose an advanced topic discovery method enhanced by LLMs to improve scientific topic identification, namely SciTopic. Specifically, we first build a textual encoder to capture the content from scientific publications, including metadata, title, and abstract. Next, we construct a space optimization module that integrates entropy-based sampling and triplet tasks guided by LLMs, enhancing the focus on thematic relevance and contextual intricacies between ambiguous instances. Then, we propose to fine-tune the textual encoder based on the guidance from the LLMs by optimizing the contrastive loss of the triplets, forcing the text encoder to better discriminate instances of different topics. Finally, extensive experiments conducted on three real-world datasets of scientific publications demonstrate that SciTopic outperforms the state-of-the-art (SOTA) scientific topic discovery methods, enabling researchers to gain deeper and faster insights.",
    "score": 4,
    "reason": "该论文聚焦于科学文献中的主题发现，虽使用了LLM增强文本编码与对比学习，但其核心任务是主题建模而非文档图像理解（DIU）。尽管涉及LLM和推理优化，但应用场景为纯文本的学术文献分析，与视觉文档理解无直接关联。其方法未涉及OCR、布局分析、表格识别或视觉-语言对齐等DIU关键要素，无法直接迁移至DIU领域。虽然使用了LLM和对比学习，但属于文本主题挖掘范畴，与inference scaling、reasoning增强或agent系统构建无显著联系。因此不具直接应用价值。",
    "summary": "SciTopic提出一种基于大语言模型（LLM）的科学文献主题发现方法，通过构建文本编码器并结合LLM引导的熵采样与三元组对比损失，提升对复杂文本关系的捕捉能力。在三个真实数据集上验证了其优于SOTA的方法，但仅适用于纯文本场景，不涉及视觉信息或文档结构理解。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20511v1",
    "updated": "2025-08-28T07:52:42Z",
    "published": "2025-08-28T07:52:42Z",
    "title": "Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark",
    "authors": [
      "Chihiro Taguchi",
      "Seng Mai",
      "Keita Kurabe",
      "Yusuke Sakai",
      "Georgina Agyei",
      "Soudabeh Eslami",
      "David Chiang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20511v1.pdf",
    "comment": "13 pages, 7 tables, 2 figures. Accepted at EMNLP Main 2025. Code and data released at https://github.com/ctaguchi/LSLB",
    "category": "Computation and Language",
    "abstract": "Multilingual machine translation (MT) benchmarks play a central role in evaluating the capabilities of modern MT systems. Among them, the FLORES+ benchmark is widely used, offering English-to-many translation data for over 200 languages, curated with strict quality control protocols. However, we study data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani) and uncover critical shortcomings in the benchmark's suitability for truly multilingual evaluation. Human assessments reveal that many translations fall below the claimed 90% quality standard, and the annotators report that source sentences are often too domain-specific and culturally biased toward the English-speaking world. We further demonstrate that simple heuristics, such as copying named entities, can yield non-trivial BLEU scores, suggesting vulnerabilities in the evaluation protocol. Notably, we show that MT models trained on high-quality, naturalistic data perform poorly on FLORES+ while achieving significant gains on our domain-relevant evaluation set. Based on these findings, we advocate for multilingual MT benchmarks that use domain-general and culturally neutral source texts rely less on named entities, in order to better reflect real-world translation challenges.",
    "score": 3,
    "reason": "该论文聚焦于多语言机器翻译基准的评估问题，虽涉及多语言能力，但核心内容是关于MT基准数据集的质量与偏见分析，与文档图像理解（DIU）无直接关联。其技术方法（如BLEU评分、命名实体复制等）不适用于DIU中的OCR、实体识别或关系抽取任务。尽管论文被EMNLP Main接收且开源，但研究主题与DIU、VLM推理增强或agent系统无关，无法直接迁移应用，因此仅具间接参考价值。",
    "summary": "本文批判性分析了广泛使用的FLORES+多语言机器翻译基准，发现其在Asante Twi、日语、Jinghpaw和南阿塞拜疆语等语言上存在质量不达标、领域特定性强及文化偏向等问题，并指出简单启发式方法即可获得高BLEU分数，揭示了现有评估协议的脆弱性。作者呼吁构建更通用、文化中立的源文本以真实反映实际翻译挑战。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20047v2",
    "updated": "2025-08-28T07:48:12Z",
    "published": "2025-08-27T16:54:09Z",
    "title": "AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering",
    "authors": [
      "Hassan Alhuzali",
      "Farah Shamout",
      "Muhammad Abdul-Mageed",
      "Chaimae Abouzahir",
      "Mouath Abu-Daoud",
      "Ashwag Alasmari",
      "Walid Al-Eisawi",
      "Renad Al-Monef",
      "Ali Alqahtani",
      "Lama Ayash",
      "Nizar Habash",
      "Leen Kharouf"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20047v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic medical QA resources by offering two complementary tracks: {MentalQA}, focusing on Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and {MedArabiQ}, covering broader medical domains such as internal medicine, pediatrics, and clinical decision making. Each track comprises multiple subtasks, evaluation datasets, and standardized metrics, facilitating fair benchmarking. The task was structured to promote modeling under realistic, multilingual, and culturally nuanced healthcare contexts. We outline the dataset creation, task design and evaluation framework, participation statistics, baseline systems, and summarize the overall outcomes. We conclude with reflections on the performance trends observed and prospects for future iterations in Arabic health QA.",
    "score": 2,
    "reason": "该论文聚焦于阿拉伯语医疗问答任务，属于自然语言处理中的特定领域QA研究，与文档图像理解（DIU）无直接关联。尽管涉及医疗文本理解，但其核心是纯文本问答而非视觉文档理解，未涉及OCR、布局分析、表格抽取或多模态输入。此外，论文未提及任何与VLM、LLM推理增强（如CoT、ToT）、或agent系统相关的内容。虽然在EMNLP会议中举办具有一定影响力，但应用场景和方法论均不适用于DIU领域，因此不具直接迁移价值。",
    "summary": "AraHealthQA 2025 是一项针对阿拉伯语医疗问答的共享任务，包含心理健康（MentalQA）和综合医学（MedArabiQ）两个赛道，旨在填补阿拉伯语医疗QA资源匮乏的问题。任务涵盖多个子任务与评估指标，强调多语言与文化语境下的真实场景建模，并提供了数据构建、基准系统与结果分析。"
  },
  {
    "id": "http://arxiv.org/abs/2505.17553v2",
    "updated": "2025-08-28T07:47:00Z",
    "published": "2025-05-23T06:58:44Z",
    "title": "CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning",
    "authors": [
      "Jinyuan Feng",
      "Chaopeng Wei",
      "Tenghai Qiu",
      "Tianyi Hu",
      "Zhiqiang Pu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.17553v2.pdf",
    "comment": "Accepted by EMNLP Findings 2025",
    "category": "Computation and Language",
    "abstract": "In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves specializing functionalities into different experts and sparsely activating them appropriately, has been widely adopted as a promising approach to trade-off between model capacity and computation overhead. However, current MoE variants fall short on heterogeneous datasets, ignoring the fact that experts may learn similar knowledge, resulting in the underutilization of MoE's capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE), a novel method to promote modularization and specialization in MoE, where the experts are trained along with a contrastive objective by sampling from activated and inactivated experts in top-k routing. We demonstrate that such a contrastive objective recovers the mutual-information gap between inputs and the two types of experts. Experiments on several benchmarks and in multi-task settings demonstrate that CoMoE can consistently enhance MoE's capacity and promote modularization among the experts.",
    "score": 6,
    "reason": "该论文提出CoMoE，通过对比学习增强MoE在参数高效微调中的模块化与专业化能力，对提升多专家模型的表达能力有理论意义。然而，其核心贡献集中在MoE架构的优化，与DIU任务本身无直接关联；虽可间接用于VLM/LLM的高效微调，但未明确涉及推理阶段增强（如CoT、ToT等）或agent系统构建，也未提及在文档理解场景下的应用验证。EMNLP Findings是顶会接受，有一定加分，但技术路线与DIU、inference scaling及agent的直接联系较弱，仅具潜在间接价值。",
    "summary": "本文提出CoMoE，一种基于对比学习的混合专家（MoE）方法，通过在top-k路由中对激活与未激活专家进行对比训练，增强专家间的差异性与模块化，从而提升MoE在多任务和异构数据上的表达能力。实验表明该方法能有效促进专家分工，提升模型性能。"
  },
  {
    "id": "http://arxiv.org/abs/2507.06056v2",
    "updated": "2025-08-28T06:54:27Z",
    "published": "2025-07-08T14:58:28Z",
    "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs",
    "authors": [
      "Yizhan Huang",
      "Zhe Yang",
      "Meifang Chen",
      "Jianping Zhang",
      "Michael R. Lyu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.06056v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large Language Models (LLMs) are known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or \"gibberish\", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI).",
    "score": 4,
    "reason": "该论文研究LLM中数据的记忆难度与熵的关系，提出熵-记忆定律，并用于数据集推断（DI）。虽然涉及LLM记忆机制，但其核心是评估训练数据的可记忆性，与DIU任务无直接关联。所提方法主要用于区分训练/测试数据，而非提升文档理解、实体识别或关系抽取能力。尽管在计算语言学领域有一定理论价值，但无法直接应用于DIU、VLM推理增强或agent系统构建，且未提及开源或顶会发表，故不具优先阅读价值。",
    "summary": "本文提出熵-记忆定律，发现训练数据的熵与其在LLM中的记忆难度呈线性关系。通过实验证明低熵数据更易被记忆，即使看似随机的字符串也表现出较低的实际熵。基于此，提出一种用于区分训练与测试数据的Dataset Inference（DI）方法。"
  },
  {
    "id": "http://arxiv.org/abs/2406.14862v7",
    "updated": "2025-08-28T06:51:18Z",
    "published": "2024-06-21T04:39:03Z",
    "title": "LatentExplainer: Explaining Latent Representations in Deep Generative Models with Multimodal Large Language Models",
    "authors": [
      "Mengdan Zhu",
      "Raasikh Kanjiani",
      "Jiahui Lu",
      "Andrew Choi",
      "Qirui Ye",
      "Liang Zhao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.14862v7.pdf",
    "comment": "Accepted to CIKM 2025 Full Research Track",
    "category": "Computation and Language",
    "abstract": "Deep generative models like VAEs and diffusion models have advanced various generation tasks by leveraging latent variables to learn data distributions and generate high-quality samples. Despite the field of explainable AI making strides in interpreting machine learning models, understanding latent variables in generative models remains challenging. This paper introduces LatentExplainer, a framework for automatically generating semantically meaningful explanations of latent variables in deep generative models. LatentExplainer tackles three main challenges: inferring the meaning of latent variables, aligning explanations with inductive biases, and handling varying degrees of explainability. Our approach perturbs latent variables, interprets changes in generated data, and uses multimodal large language models (MLLMs) to produce human-understandable explanations. We evaluate our proposed method on several real-world and synthetic datasets, and the results demonstrate superior performance in generating high-quality explanations for latent variables. The results highlight the effectiveness of incorporating inductive biases and uncertainty quantification, significantly enhancing model interpretability.",
    "score": 5,
    "reason": "该论文聚焦于生成模型潜空间的可解释性，利用多模态大语言模型（MLLM）解释潜变量，虽涉及VLM和解释性，但核心任务是模型内部机制的解释，而非文档图像理解（DIU）直接相关。其方法虽使用MLLM，但未针对文档结构、布局或信息抽取进行优化，且未在DIU任务中验证，难以直接迁移应用。尽管被CIKM 2025接收（CCF B类），具有一定质量，但与DIU、inference scaling或agent的核心技术关联较弱，仅具间接启发价值。",
    "summary": "LatentExplainer提出一种框架，利用多模态大语言模型（MLLM）自动解释深度生成模型（如VAE、扩散模型）中的潜变量。通过扰动潜变量并分析生成结果的变化，结合MLLM生成语义清晰的解释说明。该方法关注潜空间语义理解与可解释性，适用于生成模型的调试与分析，但不直接服务于文档图像理解、推理增强或智能体构建。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20474v1",
    "updated": "2025-08-28T06:50:57Z",
    "published": "2025-08-28T06:50:57Z",
    "title": "Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder",
    "authors": [
      "Muhammad Shakeel",
      "Yui Sudo",
      "Yifan Peng",
      "Chyi-Jiunn Lin",
      "Shinji Watanabe"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20474v1.pdf",
    "comment": "Accepted to IEEE ASRU 2025",
    "category": "Computation and Language",
    "abstract": "This paper presents a unified multi-speaker encoder (UME), a novel architecture that jointly learns representations for speaker diarization (SD), speech separation (SS), and multi-speaker automatic speech recognition (ASR) tasks using a shared speech foundational encoder. We leverage the hidden representations from multiple layers of UME as a residual weighted-sum encoding (RWSE) to effectively use information from different semantic levels, contributing to bottom-up alignment between tasks. This joint training approach captures the inherent interdependencies among the tasks, enhancing overall performance on overlapping speech data. Our evaluations demonstrate that UME substantially improves over the single-task baselines dedicated to SD, SS, and multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms the previous studies, achieving diarization error rates of 1.37% and 2.29% on Libri2Mix and Libri3Mix evaluation sets, respectively.",
    "score": 2,
    "reason": "该论文聚焦于多说话人语音处理任务（说话人分离、语音识别与说话人辨识）的统一建模，属于语音领域而非文档图像理解（DIU）。尽管其提出的多任务联合学习框架具有创新性，但其核心输入为音频信号，与文档图片理解在模态和任务目标上无直接关联。虽提及多层表示融合，但无法直接应用于视觉文本理解或DIU中的OCR、实体识别、关系抽取等环节。此外，未涉及VLM、LLM推理增强或agent系统，与所关注的技术路线无关。因此不值得阅读。",
    "summary": "本文提出统一多说话人编码器（UME），通过共享语音基础编码器联合学习说话人辨识（SD）、语音分离（SS）和多说话人自动语音识别（ASR）任务。利用多层隐藏表示进行残差加权求和（RWSE），实现任务间的自底向上对齐，在LibriMix数据集上显著优于单任务基线，尤其在SD任务中达到1.37%和2.29%的错误率。"
  },
  {
    "id": "http://arxiv.org/abs/2506.20083v3",
    "updated": "2025-08-28T06:46:16Z",
    "published": "2025-06-25T01:48:18Z",
    "title": "Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder",
    "authors": [
      "Yingji Zhang",
      "Danilo S. Carvalho",
      "André Freitas"
    ],
    "pdf_url": "https://arxiv.org/pdf/2506.20083v3.pdf",
    "comment": "In progress",
    "category": "Computation and Language",
    "abstract": "Integrating compositional and symbolic properties into current distributional semantic spaces can enhance the interpretability, controllability, compositionality, and generalisation capabilities of Transformer-based auto-regressive language models (LMs). In this survey, we offer a novel perspective on latent space geometry through the lens of compositional semantics, a direction we refer to as \\textit{semantic representation learning}. This direction enables a bridge between symbolic and distributional semantics, helping to mitigate the gap between them. We review and compare three mainstream autoencoder architectures-Variational AutoEncoder (VAE), Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the distinctive latent geometries they induce in relation to semantic structure and interpretability.",
    "score": 3,
    "reason": "该论文聚焦于自编码器在语义表示学习中的潜在空间几何，探讨符号语义与分布语义的融合。虽然涉及语言模型的可解释性与结构化表示，但其核心是理论性综述，未直接关联文档图像理解（DIU）、多模态模型（VLM）或推理增强技术（如CoT、ToT）。与DIU任务无直接应用路径，且未提及视觉或文档场景，因此不具直接迁移价值。",
    "summary": "本文综述了通过自编码器架构（VAE、VQVAE、SAE）实现的潜在语义空间几何，旨在连接符号语义与分布语义，提升语言模型的可解释性与组合能力。重点分析不同模型诱导的语义结构与可解释性差异。"
  },
  {
    "id": "http://arxiv.org/abs/2508.16201v2",
    "updated": "2025-08-28T06:44:28Z",
    "published": "2025-08-22T08:23:09Z",
    "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
    "authors": [
      "Yicheng Ji",
      "Jun Zhang",
      "Heming Xia",
      "Jinpeng Chen",
      "Lidan Shou",
      "Gang Chen",
      "Huan Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.16201v2.pdf",
    "comment": "Accepted at EMNLP 2025 Main",
    "category": "Computation and Language",
    "abstract": "Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens to enable efficient speculation without sacrificing accuracy. To achieve this, we performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for Qwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM.",
    "score": 7,
    "reason": "该论文提出SpecVLM，通过验证器引导的分阶段视频令牌剪枝实现对视频大语言模型的推测解码加速。虽然其核心目标是提升视频理解任务中的推理效率，但其关键技术——基于验证器反馈的低敏感性令牌剪枝策略——与DIU领域中对文档图像中冗余布局信息的高效处理具有高度相关性。特别是，在文档图像理解中，视觉token（如文本块、表格区域）的冗余表示也带来计算负担，而该方法可直接迁移用于减少文档图像中非关键视觉片段的计算开销。此外，EMNLP 2025主会接受+开源代码进一步增强了其实用价值。尽管研究对象为视频而非文档，但其在多模态推理加速方面的思想对DIU中VLM的推理优化有直接启发意义。",
    "summary": "SpecVLM提出一种无需训练的推测解码框架，通过两阶段视频令牌剪枝显著降低视频大语言模型的解码延迟：第一阶段利用验证器注意力信号保留关键帧信息，第二阶段均匀剪枝剩余冗余内容。实验表明其在多个视频理解任务上实现高达2.68倍的解码加速，且不损失准确性。代码已开源，适用于资源受限场景下的多模态推理优化。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20468v1",
    "updated": "2025-08-28T06:39:25Z",
    "published": "2025-08-28T06:39:25Z",
    "title": "ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety",
    "authors": [
      "Luke Bates",
      "Max Glockner",
      "Preslav Nakov",
      "Iryna Gurevych"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20468v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Conspiracy theories erode public trust in science and institutions while resisting debunking by evolving and absorbing counter-evidence. As AI-generated misinformation becomes increasingly sophisticated, understanding rhetorical patterns in conspiratorial content is important for developing interventions such as targeted prebunking and assessing AI vulnerabilities. We introduce ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of conspiratorial ideation in multi-sentence excerpts (80--120 words) from online conspiracy articles, annotated using the CONSPIR cognitive framework (Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial content annotated for general cognitive traits. Using ConspirED, we (i) develop computational models that identify conspiratorial traits and determine dominant traits in text excerpts, and (ii) evaluate large language/reasoning model (LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned by conspiratorial content, producing output that mirrors input reasoning patterns, even when successfully deflecting comparable fact-checked misinformation.",
    "score": 2,
    "reason": "该论文聚焦于阴谋论内容的认知特征分析与大语言模型的安全性评估，虽然涉及LLM的推理鲁棒性，但研究对象为阴谋论文本，与文档图像理解（DIU）无直接关联。其数据集和任务均属于社会偏见与安全评估范畴，无法直接迁移至DIU中的OCR、实体识别或关系抽取等任务。尽管使用了LLM进行推理分析，但其inference scaling或reasoning技术未被重点探讨，且未涉及多模态模型或agent系统。因此，与DIU领域技术路径无紧密联系，不具直接应用潜力。",
    "summary": "ConspirED是一个用于分析阴谋论认知特征的多句文本数据集，基于CONSPIR框架对网络阴谋文章进行标注，旨在评估大语言模型在面对阴谋论输入时的推理偏差与安全性。研究发现LLM会无意识地模仿输入中的非理性推理模式，即使能反驳事实性错误信息。"
  },
  {
    "id": "http://arxiv.org/abs/2505.10583v2",
    "updated": "2025-08-28T06:16:17Z",
    "published": "2025-05-14T09:41:38Z",
    "title": "Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models",
    "authors": [
      "Diogo Freitas",
      "Brigt Håvardstun",
      "Cèsar Ferri",
      "Darío Garigliotti",
      "Jan Arne Telle",
      "José Hernández-Orallo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.10583v2.pdf",
    "comment": "54 pages (42 pages of appendix). Accepted for publication at the ECAI 2025 conference",
    "category": "Computation and Language",
    "abstract": "Large language models have become multimodal, and many of them are said to integrate their modalities using common representations. If this were true, a drawing of a car as an image, for instance, should map to a similar area in the latent space as a textual description of the strokes that form the drawing. To explore this in a black-box access regime to these models, we propose the use of machine teaching, a theory that studies the minimal set of examples a teacher needs to choose so that the learner captures the concept. In this paper, we evaluate the complexity of teaching vision-language models a subset of objects in the Quick, Draw! dataset using two presentations: raw images as bitmaps and trace coordinates in TikZ format. The results indicate that image-based representations generally require fewer segments and achieve higher accuracy than coordinate-based representations. But, surprisingly, the teaching size usually ranks concepts similarly across both modalities, even when controlling for (a human proxy of) concept priors, suggesting that the simplicity of concepts may be an inherent property that transcends modality representations.",
    "score": 6,
    "reason": "该论文探讨了视觉-语言模型中概念教学复杂度在不同模态间的不变性，虽涉及VLM的内在表征特性，但其核心关注点是教学理论与模态间复杂度对比，与DIU任务中的OCR、实体识别、关系抽取等具体环节关联较弱。尽管研究对象为VLM，但未直接涉及文档理解、布局分析或信息抽取等关键DIU技术，且实验基于Quick, Draw!数据集（非真实文档），对DIU的实际应用价值有限。虽然发表于ECAI 2025（CCF B类会议）并有较长附录，显示一定严谨性，但与DIU的直接迁移潜力较低。",
    "summary": "本文通过机器教学理论评估视觉-语言模型在图像和坐标格式下学习Quick, Draw!中物体概念的教学复杂度。结果发现，尽管图像表示更高效，但概念的相对教学难度在两种模态间保持一致，暗示概念简单性可能是跨模态的固有属性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20460v1",
    "updated": "2025-08-28T06:14:37Z",
    "published": "2025-08-28T06:14:37Z",
    "title": "Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques",
    "authors": [
      "Yucheng Ruan",
      "Xiang Lan",
      "Daniel J. Tan",
      "Hairil Rizal Abdullah",
      "Mengling Feng"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20460v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Background Predicting mortality and resource utilization from electronic health records (EHRs) is challenging yet crucial for optimizing patient outcomes and managing costs in intensive care unit (ICU). Existing approaches predominantly focus on structured EHRs, often ignoring the valuable clinical insights in free-text notes. Additionally, the potential of textual information within structured data is not fully leveraged. This study aimed to introduce and assess a deep learning framework using natural language processing techniques that integrates multimodal EHRs to predict mortality and resource utilization in critical care settings. Methods Utilizing two real-world EHR datasets, we developed and evaluated our model on three clinical tasks with leading existing methods. We also performed an ablation study on three key components in our framework: medical prompts, free-texts, and pre-trained sentence encoder. Furthermore, we assessed the model's robustness against the corruption in structured EHRs. Results Our experiments on two real-world datasets across three clinical tasks showed that our proposed model improved performance metrics by 1.6\\%/0.8\\% on BACC/AUROC for mortality prediction, 0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical duration estimation compared to the best existing methods. It consistently demonstrated superior performance compared to other baselines across three tasks at different corruption rates. Conclusions The proposed framework is an effective and accurate deep learning approach for predicting mortality and resource utilization in critical care. The study also highlights the success of using prompt learning with a transformer encoder in analyzing multimodal EHRs. Importantly, the model showed strong resilience to data corruption within structured data, especially at high corruption levels.",
    "score": 2,
    "reason": "该论文聚焦于医疗领域中的多模态电子健康记录（EHR）分析，使用NLP技术预测重症监护患者死亡率和资源利用。虽然涉及多模态数据和自然语言处理，但其核心任务是临床风险预测，而非文档图像理解（DIU）。尽管使用了提示学习和预训练编码器，但应用场景、数据形式（结构化+自由文本EHR）与DIU中常见的扫描文档图像完全不同，且未涉及OCR、实体识别或关系抽取等关键DIU步骤。此外，未提及视觉信息、布局分析或文档结构理解，无法直接应用于DIU。因此，相关性极低。",
    "summary": "本文提出一种基于深度学习的多模态EHR框架，结合结构化数据与自由文本病历，通过提示学习和Transformer编码器提升重症监护中死亡率与资源利用的预测性能。实验表明模型在多个指标上优于现有方法，并对结构化数据噪声具有强鲁棒性，但研究目标为临床决策支持，非文档图像理解任务。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20453v1",
    "updated": "2025-08-28T05:58:57Z",
    "published": "2025-08-28T05:58:57Z",
    "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers",
    "authors": [
      "Zhenting Wang",
      "Qi Chang",
      "Hemani Patel",
      "Shashank Biju",
      "Cheng-En Wu",
      "Quan Liu",
      "Aolin Ding",
      "Alireza Rezazadeh",
      "Ankit Shah",
      "Yujia Bao",
      "Eugene Siow"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20453v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench.",
    "score": 9,
    "reason": "该论文提出MCP-Bench，一个面向复杂现实任务的LLM智能体评估基准，聚焦于工具使用、跨工具协调、参数精确控制与多步规划等核心能力。其基于MCP协议连接28个真实服务端，涵盖金融、旅行、科研等多个领域，支持多步骤、跨域、强耦合的工作流，高度贴近DIU中对agent系统的需求。论文强调从模糊指令中识别工具、规划执行路径、融合中间输出等能力，这些正是构建DIU agent的关键技术挑战。虽然未直接涉及文档理解，但其对LLM在复杂推理、工具调用与流程规划上的评测框架可直接迁移至DIU agent设计中，尤其适用于构建能解析文档、调用OCR/实体识别工具、协调关系抽取与验证的智能体系统。开源代码进一步提升实用性，且研究方向与当前AI agent发展趋势高度一致，极具参考价值。",
    "summary": "MCP-Bench是一个基于Model Context Protocol（MCP）的LLM智能体评估基准，通过连接28个真实世界的MCP服务器（共250个工具），构建了涵盖金融、旅行、科学计算等领域的多步复杂任务。它评估智能体在模糊指令下发现工具、规划多跳执行路径、整合中间输出和协调跨域工作流的能力，填补了现有基准在真实世界复杂性与工具协同方面的空白。论文提出多维度评估体系，并开源代码与数据，为构建具备自主决策与工具调用能力的DIU agent提供了关键方法论支撑。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20442v1",
    "updated": "2025-08-28T05:45:22Z",
    "published": "2025-08-28T05:45:22Z",
    "title": "Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method",
    "authors": [
      "Agung Sukrisna Jaya",
      "Osvari Arsalan",
      "Danny Matthew Saputra"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20442v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Case Base Reasoning (CBR) is a case solving technique based on experience in cases that have occurred before with the highest similarity. CBR is used to search for practical work titles. TF-IDF is applied to process the vectorization of each practical work title word and Cosine Similarity for the calculation of similarity values. This system can search either in the form of titles or keywords. The output of the system is the title of practical work and the match value of each title. Based on the test results using 705 practical work titles, testing was carried out with five titles and carried out in two stages. The first stage searches with existing titles and the second stage randomizes the title from the first stage. And the results obtained in the second stage are the same number of titles found and the highest average match score.",
    "score": 2,
    "reason": "该论文使用CBR方法基于TF-IDF和余弦相似度搜索信息工程本科实践课题标题，属于传统文本匹配任务，与文档图像理解（DIU）无直接关联。其技术路径（关键词匹配、向量相似度计算）虽在实体识别或检索环节可能有间接参考价值，但未涉及OCR、布局分析、视觉-语言对齐、多模态理解等DIU核心问题。同时，论文未提及任何多模态模型、VLM、LLM推理增强或Agent系统相关内容，也未开源或发表于CCF A类会议/期刊，相关性极低。",
    "summary": "本文提出一种基于案例推理（CBR）的本科实践课题标题搜索系统，利用TF-IDF进行文本向量化并用余弦相似度计算匹配度，支持按标题或关键词查询。实验在705个标题上进行，结果显示系统能稳定返回高匹配度结果，但整体为纯文本语义匹配任务，与文档图像理解、多模态模型、推理增强或智能体系统无关。"
  },
  {
    "id": "http://arxiv.org/abs/2411.07820v3",
    "updated": "2025-08-28T05:14:25Z",
    "published": "2024-11-12T14:12:45Z",
    "title": "Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models",
    "authors": [
      "Youan Cong",
      "Pritom Saha Akash",
      "Cheng Wang",
      "Kevin Chen-Chuan Chang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.07820v3.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel approach designed to bridge the pre-retrieval information gap in Retrieval-Augmented Generation (RAG) systems through query optimization tailored to meet the specific knowledge requirements of Large Language Models (LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR framework begins by extracting parametric knowledge from LLMs, followed by using a specialized query optimizer for refining these queries. This process ensures the retrieval of only the most pertinent information essential for generating accurate responses. Moreover, to enhance flexibility and reduce computational costs, we propose a trainable scheme for our pipeline that utilizes a smaller, tunable model as the query optimizer, which is refined through knowledge distillation from a larger teacher model. Our evaluations on various question-answering (QA) datasets and with different retrieval systems show that ERRR consistently outperforms existing baselines, proving to be a versatile and cost-effective module for improving the utility and accuracy of RAG systems.",
    "score": 8,
    "reason": "该论文提出ERRR框架，通过query优化提升RAG系统中检索阶段的精准性，尤其在参数化知识提取与查询精炼方面具有创新性。其核心思想——利用小型可训练模型通过知识蒸馏优化查询以匹配LLM的知识需求——对DIU中的信息抽取和多步推理任务有直接启发意义。虽然未直接涉及文档图像理解，但其在增强LLM推理能力与减少冗余检索方面的技术可迁移至DIU中基于LLM的结构化信息抽取与agent决策流程。此外，该方法适用于复杂推理场景，与inference scaling和reasoning技术路线高度契合，具备较强的应用潜力。",
    "summary": "本文提出ERRR框架，通过从LLM中提取参数化知识并使用可训练查询优化器进行精炼，提升RAG系统的检索准确性与效率。该方法采用知识蒸馏策略，用小模型替代大模型实现低成本、高灵活性的查询优化，在多个QA数据集上表现优于现有基线，为提升LLM推理性能提供了高效模块。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20420v1",
    "updated": "2025-08-28T04:42:11Z",
    "published": "2025-08-28T04:42:11Z",
    "title": "CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance",
    "authors": [
      "Feng Zhang",
      "Chengjie Pang",
      "Yuehan Zhang",
      "Chenyu Luo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20420v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Civil aviation maintenance is a domain characterized by stringent industry standards. Within this field, maintenance procedures and troubleshooting represent critical, knowledge-intensive tasks that require sophisticated reasoning. To address the lack of specialized evaluation tools for large language models (LLMs) in this vertical, we propose and develop an industrial-grade benchmark specifically designed for civil aviation maintenance. This benchmark serves a dual purpose: It provides a standardized tool to measure LLM capabilities within civil aviation maintenance, identifying specific gaps in domain knowledge and complex reasoning. By pinpointing these deficiencies, the benchmark establishes a foundation for targeted improvement efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized prompt engineering), ultimately facilitating progress toward more intelligent solutions within civil aviation maintenance. Our work addresses a significant gap in the current LLM evaluation, which primarily focuses on mathematical and coding reasoning tasks. In addition, given that Retrieval-Augmented Generation (RAG) systems are currently the dominant solutions in practical applications , we leverage this benchmark to evaluate existing well-known vector embedding models and LLMs for civil aviation maintenance scenarios. Through experimental exploration and analysis, we demonstrate the effectiveness of our benchmark in assessing model performance within this domain, and we open-source this evaluation benchmark and code to foster further research and development:https://github.com/CamBenchmark/cambenchmark",
    "score": 6,
    "reason": "该论文提出一个针对民航维修领域的工业级LLM基准CAMB，聚焦于复杂推理和领域知识评估，与DIU中需要强推理能力的任务（如理解技术文档、提取复杂关系）有潜在关联。虽然其应用场景是民航维修而非通用文档图像理解，但其强调的领域特定推理、RAG评估和模型缺陷定位对DIU中的逻辑推理模块有借鉴意义。此外，开源代码是加分项。然而，该工作未直接涉及视觉信息、OCR或文档布局理解，也未使用多模态模型，因此与DIU核心任务的直接迁移性较弱，故不给高分。",
    "summary": "CAMB是一个面向民航维修领域的大型语言模型评估基准，旨在衡量LLM在复杂技术文档理解与推理方面的能力。该基准用于检测模型在领域知识、故障诊断推理等方面的不足，并支持RAG系统评估。作者开源了基准数据集与代码，推动该垂直领域智能系统的优化。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20417v1",
    "updated": "2025-08-28T04:37:15Z",
    "published": "2025-08-28T04:37:15Z",
    "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval",
    "authors": [
      "Chi Minh Bui",
      "Ngoc Mai Thieu",
      "Van Vinh Nguyen",
      "Json J. Jung",
      "Khac-Hoai Nam Bui"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20417v1.pdf",
    "comment": "Accepted at Main EMNLP 2025",
    "category": "Computation and Language",
    "abstract": "The integration of knowledge graphs (KGs) with large language models (LLMs) offers significant potential to improve the retrieval phase of retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR, a novel framework for Contextual Query Retrieval (CQR) that enhances the retrieval phase by enriching the contextual representation of complex input queries using a corpus-centric KG. Unlike existing methods that primarily address corpus-level context loss, KG-CQR focuses on query enrichment through structured relation representations, extracting and completing relevant KG subgraphs to generate semantically rich query contexts. Comprising subgraph extraction, completion, and contextual generation modules, KG-CQR operates as a model-agnostic pipeline, ensuring scalability across LLMs of varying sizes without additional training. Experimental results on RAGBench and MultiHop-RAG datasets demonstrate KG-CQR's superior performance, achieving a 4-6% improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline models. Furthermore, evaluations on challenging RAG tasks such as multi-hop question answering show that, by incorporating KG-CQR, the performance consistently outperforms the existing baseline in terms of retrieval effectiveness",
    "score": 7,
    "reason": "该论文提出KG-CQR框架，利用知识图谱中的结构化关系表示增强复杂查询的上下文表示，用于提升RAG系统中的检索性能。其核心思想——通过结构化关系补全生成语义丰富的查询上下文——与DIU中关系抽取和实体关联建模有较强相关性，尤其适用于文档中多跳信息关联的理解场景。虽然论文聚焦于RAG任务，但其方法可直接迁移至DIU中的关系推理阶段，尤其是面对跨字段、跨区域的信息关联时。EMNLP 2025为CCF A类会议，且论文开源潜力高（未明确说明，但RAGBench等数据集常伴随开源），具备一定应用前景。然而，其并未直接处理图像输入或布局理解，因此对DIU的直接贡献有限，属于间接支撑型工作。",
    "summary": "KG-CQR是一种用于检索增强生成（RAG）系统的上下文查询检索框架，通过从以语料为中心的知识图谱中提取并补全相关子图，生成语义丰富的查询上下文表示。该方法不依赖特定模型，具有良好的可扩展性，在多个多跳问答数据集上显著提升了mAP和Recall@25指标，证明了结构化关系表示在复杂查询建模中的有效性。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20416v1",
    "updated": "2025-08-28T04:35:51Z",
    "published": "2025-08-28T04:35:51Z",
    "title": "DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding",
    "authors": [
      "Hengchuan Zhu",
      "Yihuan Xu",
      "Yichen Li",
      "Zijie Meng",
      "Zuozhu Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20416v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs) have demonstrated strong performance on general medical benchmarks. However, their capabilities in specialized medical fields, such as dentistry which require deeper domain-specific knowledge, remain underexplored due to the lack of targeted evaluation resources. In this paper, we introduce DentalBench, the first comprehensive bilingual benchmark designed to evaluate and advance LLMs in the dental domain. DentalBench consists of two main components: DentalQA, an English-Chinese question-answering (QA) benchmark with 36,597 questions spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale, high-quality corpus with 337.35 million tokens curated for dental domain adaptation, supporting both supervised fine-tuning (SFT) and retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering proprietary, open-source, and medical-specific models, and reveal significant performance gaps across task types and languages. Further experiments with Qwen-2.5-3B demonstrate that domain adaptation substantially improves model performance, particularly on knowledge-intensive and terminology-focused tasks, and highlight the importance of domain-specific benchmarks for developing trustworthy and effective LLMs tailored to healthcare applications.",
    "score": 6,
    "reason": "该论文提出DentalBench，一个针对牙科领域的双语评估基准，虽聚焦于医疗领域特定任务，但其构建高质量多语言数据集、推动领域适配（SFT与RAG）的方法对DIU中多语言文档理解（如双语账单、简历）具有借鉴意义。尤其在实体识别和关系抽取中涉及专业术语与跨语言知识迁移的挑战上可参考。然而，其核心场景为医学问答而非文档图像理解，未涉及视觉信息、布局分析或OCR/结构化提取，与DIU直接关联较弱。尽管是首个牙科专用LLM基准，且数据规模大，但缺乏视觉模态输入，无法直接用于DIU系统构建。因此仅具间接启发性，不属核心DIU研究。",
    "summary": "DentalBench是首个面向牙科领域的双语（中英）评估基准，包含36,597个问题的QA数据集（DentalQA）和3.37亿token的高质量牙科语料库（DentalCorpus），用于评估和提升LLMs在牙科知识密集型任务中的表现。实验表明领域适配显著提升模型性能，尤其在术语理解和跨语言推理方面。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20410v1",
    "updated": "2025-08-28T04:20:00Z",
    "published": "2025-08-28T04:20:00Z",
    "title": "UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools",
    "authors": [
      "Sam Jung",
      "Agustin Garcinuno",
      "Spencer Mateega"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20410v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "AI text-to-app tools promise high quality applications and websites in minutes, yet no public benchmark rigorously verifies those claims. We introduce UI-Bench, the first large-scale benchmark that evaluates visual excellence across competing AI text-to-app tools through expert pairwise comparison. Spanning 10 tools, 30 prompts, 300 generated sites, and \\textit{4000+} expert judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields calibrated confidence intervals. UI-Bench establishes a reproducible standard for advancing AI-driven web design. We release (i) the complete prompt set, (ii) an open-source evaluation framework, and (iii) a public leaderboard. The generated sites rated by participants will be released soon. View the UI-Bench leaderboard at https://uibench.ai/leaderboard.",
    "score": 3,
    "reason": "该论文聚焦于AI文本到应用工具的UI设计能力评估，虽涉及视觉生成与多模态理解，但核心目标是网页/应用界面生成的质量评测，而非文档图像理解（DIU）。其任务与DIU中对账单、简历等结构化文档的信息抽取、实体识别与关系建模无直接关联。尽管使用了专家评判和开源框架，但应用场景、数据类型和目标均不匹配DIU领域。此外，未提及VLM、LLM推理增强或agent技术，因此不具可迁移性。",
    "summary": "UI-Bench是首个大规模基准，用于评估AI文本到应用工具在生成网站时的视觉质量，通过4000+专家配对判断对10个工具在30个提示下生成的300个站点进行排名。提供完整提示集、开源评估框架和公开排行榜，旨在建立可复现的AI网页设计评估标准。"
  },
  {
    "id": "http://arxiv.org/abs/2312.05821v5",
    "updated": "2025-08-28T03:57:52Z",
    "published": "2023-12-10T08:41:24Z",
    "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models",
    "authors": [
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Yue Song",
      "Dawei Yang",
      "Qiang Wu",
      "Yan Yan",
      "Guangyu Sun"
    ],
    "pdf_url": "https://arxiv.org/pdf/2312.05821v5.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from (1) the distribution variance in the LLM activations and (2) the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner.",
    "score": 6,
    "reason": "该论文提出ASVD方法用于大语言模型的后训练压缩，重点在降低KV缓存内存占用和提升低秩分解精度。虽然其技术对LLM推理效率优化有帮助，但与DIU任务无直接关联，且未涉及视觉模态或文档理解场景。尽管其训练-free特性具有实用性，但仅作为LLM底层优化技术，无法直接迁移至DIU中的OCR、实体识别或关系抽取等核心任务。此外，未提及开源或顶会发表，因此加分项不足。虽与LLM相关，但属于通用模型压缩范畴，不满足‘可直接应用于DIU’的紧密关联要求。",
    "summary": "本文提出一种无需训练的激活感知奇异值分解（ASVD）方法，用于压缩大型语言模型的权重矩阵和KV缓存。通过处理激活分布方差和层间敏感性差异，实现10%-30%的模型压缩，并在不损失性能的前提下将KV缓存减少50%。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20395v1",
    "updated": "2025-08-28T03:43:38Z",
    "published": "2025-08-28T03:43:38Z",
    "title": "Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction",
    "authors": [
      "Xu Guo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20395v1.pdf",
    "comment": "11 pages, 4 figures",
    "category": "Computation and Language",
    "abstract": "Recent advancements in large language models (LLMs) often rely on generating intermediate reasoning steps to enhance accuracy. However, little work has examined how reasoning utility contributes to the final answer's correctness. Due to the stochastic nature of autoregressive generation, generating more context does not guarantee increased confidence in the answer. If we could predict, during generation, whether a reasoning step will be useful, we could stop early or prune ineffective steps, avoiding distractions in the final decision. We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to generate reasoning chains, and then employing a separate model (Qwen3-8B) to quantify the utility of these chains for final accuracy. Specifically, we measure the model's uncertainty on the answer span Y at each reasoning step using conditional entropy (expected negative log-likelihood over the vocabulary) with context expanding step by step. Our results show a clear pattern: conditional entropy that decreases over steps is strongly associated with correct answers, whereas flat or increasing entropy often results in wrong answers. We also corroborate that incorrect reasoning paths tend to be longer than correct ones, suggesting that longer reasoning does not necessarily yield better outcomes. These findings serve as a foundation to inspire future work on designing efficient reasoning pipelines that detect and avoid unproductive reasoning early.",
    "score": 9,
    "reason": "该论文聚焦于大语言模型推理过程中的实用价值度量，提出通过条件熵减少来评估推理步骤的有效性，这与DIU中需要高效、准确的逻辑推理能力高度相关。尤其在DIU的实体关系抽取和结构化理解任务中，识别冗余或无效推理路径可显著提升系统效率与准确性。其方法可直接应用于DIU中的多步推理模块（如基于LLM的文档语义解析），并为构建智能体中的‘规划-验证’机制提供理论支持。论文使用真实大模型生成链式推理，实验设计严谨，结果具有启发性。虽未明确开源，但研究问题与DIU核心挑战紧密耦合，且属于inference scaling与reasoning前沿方向，值得重点阅读。",
    "summary": "本文提出一种基于条件熵减少的框架，用于衡量大语言模型推理链中每一步对最终答案的贡献度。通过对MATH数据集上的推理过程进行分析，发现正确答案对应的推理路径通常伴随条件熵持续下降，而错误路径则表现出熵值停滞或上升。研究还发现错误推理往往更长，表明长度不等于质量。该方法为早期剪枝无效推理步骤提供了理论依据，有助于构建更高效的推理系统，对DIU中依赖复杂推理的任务（如文档语义建模）具有直接应用潜力。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20385v1",
    "updated": "2025-08-28T03:17:47Z",
    "published": "2025-08-28T03:17:47Z",
    "title": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models",
    "authors": [
      "Jivnesh Sandhan",
      "Fei Cheng",
      "Tushar Sandhan",
      "Yugo Murawaki"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20385v1.pdf",
    "comment": "Accepted at EMNLP25 (Findings)",
    "category": "Computation and Language",
    "abstract": "Psychometric tests, traditionally used to assess humans, are now being applied to Large Language Models (LLMs) to evaluate their behavioral traits. However, existing studies follow a context-free approach, answering each question in isolation to avoid contextual influence. We term this the Disney World test, an artificial setting that ignores real-world applications, where conversational history shapes responses. To bridge this gap, we propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. To thoroughly analyze the influence of context, we introduce novel metrics to quantify the consistency of LLM responses, a fundamental trait in human behavior. Our exhaustive experiments on 7 LLMs reveal that conversational history enhances response consistency via in-context learning but also induces personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash and Llama-8B display significant sensitivity. Moreover, GPT models response stem from their intrinsic personality traits as well as prior interactions, whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions. Finally, applying our framework to Role Playing Agents (RPAs) shows context-dependent personality shifts improve response consistency and better align with human judgments. Our code and datasets are publicly available at: https://github.com/jivnesh/CAPE",
    "score": 3,
    "reason": "该论文研究的是大语言模型的性格评估框架，聚焦于对话上下文对LLM行为一致性的影响。虽然涉及LLM的推理与上下文理解，但其核心目标是心理测评而非文档理解或信息抽取。尽管提及了角色扮演代理（RPAs），但其应用场景与DIU无关，且未直接涉及视觉信息、布局分析、OCR、实体识别或关系抽取等DIU关键环节。此外，论文虽被EMNLP25接收，但属于CL领域，且无明确与VLM或多模态结合的探索。因此，与DIU的直接关联较弱，仅在‘上下文感知’层面有间接启发，不值得优先阅读。",
    "summary": "本文提出CAPE框架，用于评估大语言模型在对话历史影响下的性格一致性。通过引入上下文感知机制，发现LLM响应受先前对话显著影响，导致人格偏移；GPT系列更依赖内在特质，而Gemini和Llama更依赖上下文。实验表明上下文可提升一致性，适用于角色扮演代理。代码与数据集已开源。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20038v2",
    "updated": "2025-08-28T03:02:52Z",
    "published": "2025-08-27T16:44:03Z",
    "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks",
    "authors": [
      "Sheng Liu",
      "Qiang Sheng",
      "Danding Wang",
      "Yang Li",
      "Guang Yang",
      "Juan Cao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20038v2.pdf",
    "comment": "EMNLP 2025 findings",
    "category": "Computation and Language",
    "abstract": "Despite advances in improving large language model (LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs' inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.",
    "score": 3,
    "reason": "该论文聚焦于增强LLM安全防御能力，通过合成类似越狱攻击的指令来提升模型对未知恶意指令的识别能力。虽然涉及LLM安全与推理鲁棒性，但其核心目标是防御攻击，而非提升推理能力或支持DIU任务中的信息理解、实体关系抽取等关键环节。与DIU直接关联较弱，且未提及视觉模态或文档理解场景，无法直接应用于DIU系统中。尽管发表于EMNLP 2025 Findings（CCF B类），但技术路径与DIU领域需求不匹配，故评分较低。",
    "summary": "本文提出IMAGINE框架，通过分析嵌入空间分布并迭代生成类似越狱攻击的指令，以填补安全对齐数据与真实攻击之间的分布差距，从而提升LLM对新型恶意指令的抵抗能力。实验表明该方法可显著降低多个主流LLM在面对新攻击时的成功率，同时保持模型正常功能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20373v1",
    "updated": "2025-08-28T02:40:27Z",
    "published": "2025-08-28T02:40:27Z",
    "title": "Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems",
    "authors": [
      "Yuyao Wang",
      "Bowen Liu",
      "Jianheng Tang",
      "Nuo Chen",
      "Yuhan Li",
      "Qifan Zhang",
      "Jia Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20373v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1.",
    "score": 8,
    "reason": "该论文提出利用NP-hard图问题作为合成训练数据来增强大语言模型的长链推理能力，与DIU领域密切相关，因为文档理解常涉及复杂逻辑推理（如字段关系、结构推断）。其提出的两阶段后训练框架（SFT+RL）可直接应用于提升VLM在DIU中的推理性能。开源实现和Hugging Face发布进一步增强了可迁移性。虽未直接面向DIU，但其方法论对提升LLM在视觉-文本联合推理任务中的表现具有显著潜力，尤其适用于DIU中需要深度推理的场景（如表单逻辑校验、多实体关系建模）。CCF A类会议（ACL）加持，加分。",
    "summary": "本文提出使用NP-hard图问题作为合成训练数据，构建Graph-R1-7B模型以强化大语言模型的长链推理能力。通过两阶段训练：基于拒绝采样的SFT提升推理深度，结合细粒度奖励的RL优化推理效率。模型在数学、编码、STEM等任务上表现优异，超越QwQ-32B，且具备良好泛化性。代码与数据已开源，为提升LLM推理能力提供了可扩展的新范式，对DIU中复杂语义推理任务有重要借鉴意义。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20353v1",
    "updated": "2025-08-28T02:00:48Z",
    "published": "2025-08-28T02:00:48Z",
    "title": "DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search",
    "authors": [
      "Zhibang Yang",
      "Xinke Jiang",
      "Rihong Qiu",
      "Ruiqing Li",
      "Yihang Zhang",
      "Yue Fang",
      "Yongxin Xu",
      "Hongxin Ding",
      "Xu Chu",
      "Junfeng Zhao",
      "Yasha Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20353v1.pdf",
    "comment": "7 pages, 3 figures",
    "category": "Computation and Language",
    "abstract": "Federated Retrieval (FR) routes queries across multiple external knowledge sources, to mitigate hallucinations of LLMs, when necessary external knowledge is distributed. However, existing methods struggle to retrieve high-quality and relevant documents for ambiguous queries, especially in cross-domain scenarios, which significantly limits their effectiveness in supporting downstream generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS, a novel framework that leverages DIF to identify latent query intents and construct semantically aligned knowledge partitions for accurate retrieval across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by leveraging gradient signals from a few annotated queries and employing Shapley value-based attribution to trace neuron activation paths associated with intent recognition and subdomain boundary detection. Then, DFAMS leverages DIF to train an alignment module via multi-prototype contrastive learning, enabling fine-grained intra-source modeling and inter-source semantic alignment across knowledge bases. Experimental results across five benchmarks show that DFAMS outperforms advanced FR methods by up to 14.37% in knowledge classification accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy, demonstrating its effectiveness in complex FR scenarios.",
    "score": 6,
    "reason": "该论文提出DFAMS框架用于联邦检索（FR），通过动态信息流（DIF）识别查询意图并实现跨域知识对齐，虽在LLM外部知识检索方面有创新，但其核心场景为联邦学习环境下的知识检索与问答，与文档图像理解（DIU）的直接关联较弱。尽管涉及LLM推理和多源知识整合，但未处理视觉内容、布局理解或文档结构建模，无法直接应用于DIU任务。虽然使用了Shapley值等高级分析技术，属于推理增强方向，但非针对DIU中的OCR、实体识别或关系抽取等关键环节。因此，仅具间接参考价值，不建议优先阅读。",
    "summary": "DFAMS提出一种基于动态信息流（DIF）的联邦检索框架，利用梯度信号和Shapley值追踪神经元激活路径以识别查询意图，并通过多原型对比学习实现跨异构知识库的细粒度语义对齐，显著提升跨域知识检索准确率和下游问答性能。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20351v1",
    "updated": "2025-08-28T01:54:47Z",
    "published": "2025-08-28T01:54:47Z",
    "title": "Joint Enhancement of Relational Reasoning for Long-Context LLMs",
    "authors": [
      "Zhirui Chen",
      "Wei Shen",
      "Jiashui Huang",
      "Ling Shao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20351v1.pdf",
    "comment": "9 pages, 5 pages Accepted by EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "Despite significant progress, large language models (LLMs) still struggle with long contexts due to memory limitations and their inability to tackle complex and long-context tasks. Additionally, LLMs often suffer from a lack of transparency and are prone to producing hallucinations. To address these challenges, we propose \\textbf{JERR}, a novel framework designed to enhance long-context comprehension via graph-based reasoning in LLMs. JERR integrates three key components: synopsis extraction, graph construction, and relational reasoning. First, synopsis is extracted by chunking text strategically, allowing the model to summarize and understand information more efficiently. Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring logical consistency and clarity. Finally, we incorporate Monte Carlo Tree Search (MCTS) to help the model navigate complex reasoning paths, ensuring more accurate and interpretable outputs. This framework provides a novel solution that enables LLMs to handle extended contexts and complex reasoning tasks with improved reliability and transparency. Experimental results show that JERR consistently outperforms all baselines on the ROUGE and F1 metrics, achieving the highest scores on the LLM-Rater evaluation.",
    "score": 9,
    "reason": "该论文提出JERR框架，通过图结构推理与MCTS增强长上下文LLM的关联推理能力，直接解决DIU中复杂文档关系抽取的挑战。其核心思想——利用图建模和蒙特卡洛树搜索进行可解释、高精度推理，可直接迁移至DIU中的关系抽取模块，尤其适用于简历、合同等长文档的语义关系建模。EMNLP 2025 Findings为CCF A类会议，且论文强调透明性与抗幻觉，对构建可信DIU系统极具价值。虽未明确提及DIU，但方法论高度契合，具备直接应用潜力。",
    "summary": "本文提出JERR框架，通过摘要提取、有向无环图构建和蒙特卡洛树搜索（MCTS）实现长上下文大语言模型的关联推理增强。该框架有效缓解记忆限制与推理偏差问题，在长文本理解任务上显著优于基线，提升了输出的准确性与可解释性，为DIU中复杂文档关系建模提供了强有力的技术路径。"
  },
  {
    "id": "http://arxiv.org/abs/2409.00061v3",
    "updated": "2025-08-28T01:52:35Z",
    "published": "2024-08-22T14:27:47Z",
    "title": "Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language",
    "authors": [
      "Arief Purnama Muharram",
      "Ayu Purwarianti"
    ],
    "pdf_url": "https://arxiv.org/pdf/2409.00061v3.pdf",
    "comment": "Accepted for publication in the Journal of ICT Research and Applications (JICTRA)",
    "category": "Computation and Language",
    "abstract": "Automated fact-checking is a key strategy to overcome the spread of COVID-19 misinformation on the internet. These systems typically leverage deep learning approaches through Natural Language Inference (NLI) to verify the truthfulness of information based on supporting evidence. However, one challenge that arises in deep learning is performance stagnation due to a lack of knowledge during training. This study proposes using a Knowledge Graph (KG) as external knowledge to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. The proposed model architecture comprises three modules: a fact module, an NLI module, and a classifier module. The fact module processes information from the KG, while the NLI module handles semantic relationships between the given premise and hypothesis. The representation vectors from both modules are concatenated and fed into the classifier module to produce the final result. The model was trained using the generated Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia. Our study demonstrates that incorporating KGs can significantly improve NLI performance in fact-checking, achieving the best accuracy of 0.8616. This suggests that KGs are a valuable component for enhancing NLI performance in automated fact-checking.",
    "score": 3,
    "reason": "该论文聚焦于使用知识图谱增强NLI模型在印尼语新冠事实核查中的性能，虽涉及NLI与外部知识融合，但应用场景为多语言事实核查而非文档图像理解（DIU），且未涉及视觉模态、布局分析或文档结构理解。其技术路径（KG增强NLI）虽与DIU中关系抽取有一定间接关联，但缺乏直接迁移性，无法直接应用于文档图像的实体与关系建模。此外，研究语言为印尼语，数据集和任务均不具通用性，对DIU领域贡献有限。",
    "summary": "本文提出一种基于知识图谱（KG）增强自然语言推理（NLI）的新冠事实核查框架，用于印尼语文本。模型通过整合KG信息与NLI模块的表示向量，提升事实判断准确率，最高达0.8616。研究在特定语言和任务下有效，但与文档图像理解无直接关联。"
  },
  {
    "id": "http://arxiv.org/abs/2508.19997v2",
    "updated": "2025-08-28T01:16:42Z",
    "published": "2025-08-27T15:56:34Z",
    "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification",
    "authors": [
      "Boheng Mao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19997v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Legal text classification is a fundamental NLP task in the legal domain. Benchmark datasets in this area often exhibit a long-tail label distribution, where many labels are underrepresented, leading to poor model performance on rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a solution to this problem. SRA focuses on augmenting samples belonging to low-frequency labels in the training set, preventing the introduction of noise for well-represented classes, and requires no changes to the model architecture. Retrieval is performed only from the training data to ensure there is no potential information leakage, removing the need for external corpora simultaneously. The proposed SRA method is tested on two legal text classification benchmark datasets with long-tail distributions: LEDGAR (single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE baselines across both datasets, illustrating consistent improvements in long-tail legal text classification.",
    "score": 3,
    "reason": "该论文聚焦于长尾法律文本分类，虽涉及NLP中的实体/类别识别问题，但其核心方法为检索增强的样本扩充，主要用于提升少数类分类性能。与文档图像理解（DIU）无直接关联，未涉及视觉信息、OCR、布局分析或表格理解等关键元素；同时，其方法不适用于多模态输入，也无法直接迁移至DIU任务中。尽管在法律文本领域有实际价值，但与DIU、VLM、LLM推理增强或Agent系统均无紧密联系，故相关性较低。",
    "summary": "本文提出Selective Retrieval-Augmentation（SRA）方法，用于解决法律文本分类中长尾标签分布导致的模型性能下降问题。SRA通过仅从训练数据中检索并增强低频标签样本，避免对高频类引入噪声，无需修改模型架构且防止信息泄露。在LEDGAR和UNFAIR-ToS两个基准数据集上，SRA显著优于现有LexGLUE基线，提升了微平均和宏平均F1分数。"
  },
  {
    "id": "http://arxiv.org/abs/2412.19512v3",
    "updated": "2025-08-28T01:13:45Z",
    "published": "2024-12-27T08:03:22Z",
    "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
    "authors": [
      "Hua Farn",
      "Hsuan Su",
      "Shachi H Kumar",
      "Saurav Sahay",
      "Shang-Tse Chen",
      "Hung-yi Lee"
    ],
    "pdf_url": "https://arxiv.org/pdf/2412.19512v3.pdf",
    "comment": "EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "Fine-tuning large language models (LLMs) for downstream tasks often leads to catastrophic forgetting, notably degrading the safety of originally aligned models. While some existing methods attempt to restore safety by incorporating additional safety data, the quality of such data typically falls short of that used in the original alignment process. Moreover, these high-quality safety datasets are generally inaccessible, making it difficult to fully recover the model's original safety. We ask: How can we preserve safety while improving downstream task performance without additional safety data? We show that simply merging the weights of pre- and post-fine-tuned models effectively mitigates safety degradation while enhancing performance. Experiments across different downstream tasks and models validate the method's practicality and effectiveness.",
    "score": 6,
    "reason": "该论文提出通过预训练与微调后模型的权重合并来缓解LLM在微调过程中出现的安全性退化问题，方法简洁且有效。虽然其核心贡献在于提升模型安全性，与DIU领域无直接关联，但其技术思路——即利用模型融合避免灾难性遗忘——对构建稳定、可信赖的DIU系统（尤其是基于LLM的agent）具有潜在启发意义。然而，该工作未明确提及文档理解、视觉信息抽取或多模态任务，也未涉及推理增强或agent架构设计，因此仅具备间接参考价值。EMNLP 2025 Findings为A类会议，有一定加分，但受限于应用场景不直接相关，故评分中等。",
    "summary": "本文提出一种通过合并预训练模型和微调后模型权重的方法，以在不依赖额外安全数据的情况下缓解LLM微调带来的安全性下降问题。实验表明该方法能有效保持原始对齐安全性并提升下游任务性能，为构建更鲁棒的LLM系统提供了新思路。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20333v1",
    "updated": "2025-08-28T00:30:25Z",
    "published": "2025-08-28T00:30:25Z",
    "title": "Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs",
    "authors": [
      "Md Abdullah Al Mamun",
      "Ihsen Alouani",
      "Nael Abu-Ghazaleh"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20333v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large Language Models (LLMs) are aligned to meet ethical standards and safety requirements by training them to refuse answering harmful or unsafe prompts. In this paper, we demonstrate how adversaries can exploit LLMs' alignment to implant bias, or enforce targeted censorship without degrading the model's responsiveness to unrelated topics. Specifically, we propose Subversive Alignment Injection (SAI), a poisoning attack that leverages the alignment mechanism to trigger refusal on specific topics or queries predefined by the adversary. Although it is perhaps not surprising that refusal can be induced through overalignment, we demonstrate how this refusal can be exploited to inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning defenses including LLM state forensics, as well as robust aggregation techniques that are designed to detect poisoning in FL settings. We demonstrate the practical dangers of this attack by illustrating its end-to-end impacts on LLM-powered application pipelines. For chat based applications such as ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare questions to targeted racial category leading to high bias ($\\Delta DP$ of 23%). We also show that bias can be induced in other NLP tasks: for a resume selection pipeline aligned to refuse to summarize CVs from a selected university, high bias in selection ($\\Delta DP$ of 27%) results. Even higher bias ($\\Delta DP$~38%) results on 9 other chat based downstream applications.",
    "score": 3,
    "reason": "该论文研究的是LLM对齐机制的攻击，聚焦于通过投毒攻击诱导模型在特定主题上拒绝响应，从而注入偏见。虽然与LLM安全相关，但其核心是攻击性研究，而非提升推理能力或构建DIU系统。文中未涉及inference scaling、reasoning技术（如CoT、ToT）或agent架构，也未提出可迁移至DIU任务的新方法。尽管问题重要，但直接应用价值低，无法为DIU中的实体识别、关系抽取或多模态理解提供技术增益。因此不值得优先阅读。",
    "summary": "本文提出Subversive Alignment Injection (SAI)攻击，利用LLM的对齐机制，在仅1%数据投毒的情况下，使模型拒绝回答特定种族或大学相关的医疗、简历筛选等请求，导致显著偏见（ΔDP高达38%）。攻击可绕过现有防御机制，对LLM驱动的应用造成严重危害。"
  },
  {
    "id": "http://arxiv.org/abs/2508.20325v1",
    "updated": "2025-08-28T00:07:10Z",
    "published": "2025-08-28T00:07:10Z",
    "title": "GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs",
    "authors": [
      "Haibo Jin",
      "Ruoxi Chen",
      "Peiyan Zhang",
      "Andy Zhou",
      "Yang Zhang",
      "Haohan Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20325v1.pdf",
    "comment": "54 pages",
    "category": "Computation and Language",
    "abstract": "As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance. To address this challenge, we introduce GUARD (\\textbf{G}uideline \\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and Jailbreak \\textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations. We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.",
    "score": 4,
    "reason": "该论文聚焦于LLM的安全性测试，提出一种基于指南遵循与角色扮演的测试方法（GUARD），用于检测模型是否违反伦理规范。虽然其方法在LLM安全评估方面具有创新性，并可扩展至视觉语言模型（VLM），但核心内容并非推理增强、inference scaling或agent系统构建，且与DIU任务无直接关联。尽管其提及对VLM的适用性，但未涉及文档理解、信息抽取或结构化推理等关键DIU环节。因此，虽属于LLM安全领域相关工作，但无法直接迁移应用于DIU场景，仅具间接参考价值。",
    "summary": "GUARD是一种用于评估大型语言模型（LLM）是否遵守政府伦理指南的自动化测试框架，通过生成违反指南的问题并引入‘越狱’诊断机制（GUARD-JD）来探测潜在的安全漏洞。该方法在多个主流LLM上进行了验证，并展示了向视觉-语言模型扩展的可能性，旨在提升AI系统的可信度和合规性。"
  }
]