import json
from bs4 import BeautifulSoup

# è¯»å–ç»è¿‡AIæ‰“åˆ†çš„æ–‡ä»¶
with open("target/evaluated_papers.json", 'r') as f:
    evaluated_papers = json.load(f)

# è¯»å– HTML æ–‡ä»¶
with open("target/index.html", 'r') as f:
    html_content = f.read()

soup = BeautifulSoup(html_content, 'html.parser')

# 1. è¿‡æ»¤å¹¶æ’åºè®ºæ–‡
# åªä¿ç•™æœ‰è¯„åˆ†çš„è®ºæ–‡
scored_papers = [p for p in evaluated_papers if 'score' in p and isinstance(p['score'], int)]
# æŒ‰åˆ†æ•°é™åºæ’åˆ—
scored_papers.sort(key=lambda x: x['score'], reverse=True)

# 2. æ„å»ºâ€œæœ¬å‘¨ç²¾é€‰â€çš„ HTML ç»“æ„
# æˆ‘ä»¬å°½é‡å¤ç”¨åŸæœ¬çš„ CSS ç±» (day-container, article-expander ç­‰) ä¿æŒæ ·å¼ä¸€è‡´

if scored_papers:
    # åˆ›å»ºå¤–å±‚å®¹å™¨
    top_section = soup.new_tag('section', **{'class': 'day-container', 'style': 'margin-top: 20px; border: 2px solid var(--nord08);'})
    
    # æ ‡é¢˜æ 
    header_div = soup.new_tag('div', **{'class': 'date', 'style': 'color: var(--nord0B); padding-bottom: 10px;'})
    header_div.string = f"ğŸ† Weekly Top Picks ({len(scored_papers)} Papers)"
    top_section.append(header_div)

    # éå†æ’åºåçš„è®ºæ–‡ï¼Œç”Ÿæˆ HTML å¡ç‰‡
    for paper in scored_papers:
        # åˆ›å»ºæ–‡ç« å®¹å™¨
        article = soup.new_tag('article')
        details = soup.new_tag('details', **{'class': 'article-expander', 'open': 'true'}) # é»˜è®¤å±•å¼€é«˜åˆ†è®ºæ–‡
        
        # --- Summary (æ ‡é¢˜è¡Œ) ---
        summary_tag = soup.new_tag('summary', **{'class': 'article-expander-title'})
        
        # 1. åˆ†æ•° Chip
        score_span = soup.new_tag('span', **{'class': 'chip', 'style': 'background: var(--nord0B); color: white; font-weight: bold;'})
        score_span.string = str(paper['score'])
        summary_tag.append(score_span)
        summary_tag.append(" ") # ç©ºæ ¼

        # 2. æ ‡é¢˜æ–‡æœ¬
        title_span = soup.new_tag('span')
        # è¿™é‡Œä¸ºäº†ç®€å•æ²¡åŠ åŸæœ¬çš„æ­£åˆ™é«˜äº®ï¼Œå¦‚æœéœ€è¦å¯ä»¥åç»­åŠ ä¸Šï¼Œæˆ–è€…ç›´æ¥ç”¨æ–‡æœ¬
        title_span.string = paper['title']
        summary_tag.append(title_span)
        
        # 3. ä¼šè®®/Comment Chip (å¦‚æœæœ‰)
        if paper.get('comment'):
            summary_tag.append(" ")
            conf_span = soup.new_tag('span', **{'class': 'chip'})
            conf_span.string = paper['comment']
            summary_tag.append(conf_span)

        details.append(summary_tag)

        # --- Content (å†…å®¹è¯¦æƒ…) ---
        
        # ä½œè€…è¡Œ
        authors_div = soup.new_tag('div', **{'class': 'article-authors'})
        
        # é“¾æ¥å›¾æ ‡
        link_a = soup.new_tag('a', href=paper['id'], target="_blank")
        icon_i = soup.new_tag('i', **{'class': 'ri-links-line'})
        link_a.append(icon_i)
        authors_div.append(link_a)
        authors_div.append(" ")

        # ä½œè€…åˆ—è¡¨
        authors_text = soup.new_tag('span')
        if isinstance(paper['authors'], list):
            authors_text.string = ", ".join(paper['authors'])
        else:
            authors_text.string = paper['authors']
        authors_div.append(authors_text)
        details.append(authors_div)

        # AI è¯„ä»·ç†ç”± (Reason)
        reason_div = soup.new_tag('div', **{'class': 'article-summary-box-inner', 'style': 'background-color: rgba(136, 192, 208, 0.1); padding: 10px; border-radius: 5px; margin: 5px 0;'})
        reason_label = soup.new_tag('span', **{'class': 'chip'})
        reason_label.string = "AI Reason"
        reason_content = soup.new_tag('span', **{'style': 'font-weight: 500; color: var(--nord0B);'})
        reason_content.string = paper.get('reason', '')
        reason_div.append(reason_label)
        reason_div.append(" ")
        reason_div.append(reason_content)
        details.append(reason_div)

        # AI æ€»ç»“ (Summary)
        ai_summary_div = soup.new_tag('div', **{'class': 'article-summary-box-inner'})
        summary_label = soup.new_tag('span', **{'class': 'chip'})
        summary_label.string = "AI Summary"
        summary_content = soup.new_tag('span')
        summary_content.string = paper.get('summary', '')
        ai_summary_div.append(summary_label)
        ai_summary_div.append(" ")
        ai_summary_div.append(summary_content)
        details.append(ai_summary_div)
        
        # åŸæ–‡æ‘˜è¦ (Abstract) - é»˜è®¤æŠ˜å æˆ–æ”¾åœ¨æœ€å
        abs_div = soup.new_tag('div', **{'class': 'article-summary-box-inner', 'style': 'color: var(--nord03); font-size: 0.9em;'})
        abs_label = soup.new_tag('span', **{'class': 'chip', 'style': 'background: var(--nord04); color: var(--nord00);'})
        abs_label.string = "Original Abstract"
        abs_content = soup.new_tag('span')
        abs_content.string = paper.get('abstract', '')
        abs_div.append(abs_label)
        abs_div.append(" ")
        abs_div.append(abs_content)
        details.append(abs_div)

        # ç±»åˆ«æ ‡ç­¾
        cat_div = soup.new_tag('div', **{'class': 'article-summary-box-inner'})
        cat_span = soup.new_tag('span', **{'class': 'chip'})
        cat_span.string = f"Categories: {paper.get('category', '')}"
        cat_div.append(cat_span)
        details.append(cat_div)

        article.append(details)
        top_section.append(article)

    # 3. æ’å…¥åˆ°é¡µé¢é¡¶éƒ¨
    # æ‰¾åˆ°åŸæ¥çš„ header-container
    header_container = soup.find('section', class_='header-container')
    if header_container:
        # æ’å…¥åˆ° header ä¹‹å
        header_container.insert_after(top_section)
    else:
        # å¦‚æœæ‰¾ä¸åˆ° headerï¼Œæ’å…¥åˆ° body çš„æœ€å‰é¢
        soup.body.insert(0, top_section)

# 4. (å¯é€‰) å¦‚æœä½ æƒ³ç§»é™¤ä¸‹é¢æ¯æ—¥åˆ—è¡¨ä¸­é‡å¤çš„å¡ç‰‡ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é€»è¾‘ã€‚
# ä½†ä¿ç•™æ¯æ—¥åˆ—è¡¨ä½œä¸ºå…¨é‡æ•°æ®çš„å­˜æ¡£é€šå¸¸ä¹Ÿæ˜¯ä¸é”™çš„é€‰æ‹©ã€‚

# å†™å›æ–‡ä»¶
with open("target/index.html", 'w') as f:
    f.write(str(soup.prettify()))

print(f"Index.html updated. Inserted Weekly Top Picks with {len(scored_papers)} papers.")
