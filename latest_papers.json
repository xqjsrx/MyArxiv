[
  {
    "id": "http://arxiv.org/abs/2508.21063v1",
    "updated": "2025-08-28T17:59:05Z",
    "published": "2025-08-28T17:59:05Z",
    "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation",
    "authors": [
      "Ruixuan Liu",
      "Philip Huang",
      "Ava Pun",
      "Kangle Deng",
      "Shobhit Aggarwal",
      "Kevin Tang",
      "Michelle Liu",
      "Deva Ramanan",
      "Jun-Yan Zhu",
      "Jiaoyang Li",
      "Changliu Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21063v1.pdf",
    "comment": "12 pages, 10 figures, 2 tables",
    "category": "Artificial Intelligence",
    "abstract": "Creating assembly products demands significant manual effort and expert knowledge in 1) designing the assembly and 2) constructing the product. This paper introduces Prompt-to-Product, an automated pipeline that generates real-world assembly products from natural language prompts. Specifically, we leverage LEGO bricks as the assembly platform and automate the process of creating brick assembly structures. Given the user design requirements, Prompt-to-Product generates physically buildable brick designs, and then leverages a bimanual robotic system to construct the real assembly products, bringing user imaginations into the real world. We conduct a comprehensive user study, and the results demonstrate that Prompt-to-Product significantly lowers the barrier and reduces manual effort in creating assembly products from imaginative ideas."
  },
  {
    "id": "http://arxiv.org/abs/2508.21061v1",
    "updated": "2025-08-28T17:58:29Z",
    "published": "2025-08-28T17:58:29Z",
    "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models",
    "authors": [
      "Adam Coscia",
      "Shunan Guo",
      "Eunyee Koh",
      "Alex Endert"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21061v1.pdf",
    "comment": "Accepted to UIST 2025. 18 pages, 9 figures, 2 tables. For a demo video, see https://youtu.be/uobhmxo6EIE",
    "category": "Artificial Intelligence",
    "abstract": "As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance."
  },
  {
    "id": "http://arxiv.org/abs/2508.21058v1",
    "updated": "2025-08-28T17:57:55Z",
    "published": "2025-08-28T17:57:55Z",
    "title": "Mixture of Contexts for Long Video Generation",
    "authors": [
      "Shengqu Cai",
      "Ceyuan Yang",
      "Lvmin Zhang",
      "Yuwei Guo",
      "Junfei Xiao",
      "Ziyan Yang",
      "Yinghao Xu",
      "Zhenheng Yang",
      "Alan Yuille",
      "Leonidas Guibas",
      "Maneesh Agrawala",
      "Lu Jiang",
      "Gordon Wetzstein"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21058v1.pdf",
    "comment": "Project page: https://primecai.github.io/moc/",
    "category": "Artificial Intelligence",
    "abstract": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes."
  },
  {
    "id": "http://arxiv.org/abs/2508.21052v1",
    "updated": "2025-08-28T17:55:14Z",
    "published": "2025-08-28T17:55:14Z",
    "title": "FakeParts: a New Family of AI-Generated DeepFakes",
    "authors": [
      "Gaetan Brison",
      "Soobash Daiboo",
      "Samy Aimeur",
      "Awais Hussain Sani",
      "Xi Wang",
      "Gianni Franchi",
      "Vicky Kalogeiton"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21052v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations."
  },
  {
    "id": "http://arxiv.org/abs/2508.21051v1",
    "updated": "2025-08-28T17:55:07Z",
    "published": "2025-08-28T17:55:07Z",
    "title": "Enabling Equitable Access to Trustworthy Financial Reasoning",
    "authors": [
      "William Jurayj",
      "Nils Holzenberger",
      "Benjamin Van Durme"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21051v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "According to the United States Internal Revenue Service, ''the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance."
  },
  {
    "id": "http://arxiv.org/abs/2508.21048v1",
    "updated": "2025-08-28T17:53:05Z",
    "published": "2025-08-28T17:53:05Z",
    "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
    "authors": [
      "Hao Tan",
      "Jun Lan",
      "Zichang Tan",
      "Ajian Liu",
      "Chuanbiao Song",
      "Senyuan Shi",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jun Wan",
      "Zhen Lei"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21048v1.pdf",
    "comment": "Project: https://github.com/EricTan7/Veritas",
    "category": "Artificial Intelligence",
    "abstract": "Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as \"planning\" and \"self-reflection\" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs."
  },
  {
    "id": "http://arxiv.org/abs/2508.21036v1",
    "updated": "2025-08-28T17:40:42Z",
    "published": "2025-08-28T17:40:42Z",
    "title": "Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop",
    "authors": [
      "Lev Tankelevitch",
      "Elena L. Glassman",
      "Jessica He",
      "Aniket Kittur",
      "Mina Lee",
      "Srishti Palani",
      "Advait Sarkar",
      "Gonzalo Ramos",
      "Yvonne Rogers",
      "Hari Subramonyam"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21036v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Generative AI (GenAI) radically expands the scope and capability of automation for work, education, and everyday tasks, a transformation posing both risks and opportunities for human cognition. How will human cognition change, and what opportunities are there for GenAI to augment it? Which theories, metrics, and other tools are needed to address these questions? The CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of how the use of GenAI affects human thought, from metacognition to critical thinking, memory, and creativity, with an emerging design practice for building GenAI tools that both protect and augment human thought. Fifty-six researchers, designers, and thinkers from across disciplines as well as industry and academia, along with 34 papers and portfolios, seeded a day of discussion, ideation, and community-building. We synthesize this material here to begin mapping the space of research and design opportunities and to catalyze a multidisciplinary community around this pressing area of research."
  },
  {
    "id": "http://arxiv.org/abs/2508.19200v2",
    "updated": "2025-08-28T17:29:36Z",
    "published": "2025-08-26T17:03:43Z",
    "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
    "authors": [
      "Xinran Zhao",
      "Boyuan Zheng",
      "Chenglei Si",
      "Haofei Yu",
      "Ken Liu",
      "Runlong Zhou",
      "Ruochen Li",
      "Tong Chen",
      "Xiang Li",
      "Yiming Zhang",
      "Tongshuang Wu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19200v2.pdf",
    "comment": "21 pages, 3 figures",
    "category": "Artificial Intelligence",
    "abstract": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI."
  },
  {
    "id": "http://arxiv.org/abs/2508.21016v1",
    "updated": "2025-08-28T17:18:31Z",
    "published": "2025-08-28T17:18:31Z",
    "title": "Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance",
    "authors": [
      "Luozhijie Jin",
      "Zijie Qiu",
      "Jie Liu",
      "Zijie Diao",
      "Lifeng Qiao",
      "Ning Ding",
      "Alex Lamb",
      "Xipeng Qiu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21016v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Denoising-based generative models, particularly diffusion and flow matching algorithms, have achieved remarkable success. However, aligning their output distributions with complex downstream objectives, such as human preferences, compositional accuracy, or data compressibility, remains challenging. While reinforcement learning (RL) fine-tuning methods, inspired by advances in RL from human feedback (RLHF) for large language models, have been adapted to these generative frameworks, current RL approaches are suboptimal for diffusion models and offer limited flexibility in controlling alignment strength after fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models through the lens of stochastic differential equations and implicit reward conditioning. We introduce Reinforcement Learning Guidance (RLG), an inference-time method that adapts Classifier-Free Guidance (CFG) by combining the outputs of the base and RL fine-tuned models via a geometric average. Our theoretical analysis shows that RLG's guidance scale is mathematically equivalent to adjusting the KL-regularization coefficient in standard RL objectives, enabling dynamic control over the alignment-quality trade-off without further training. Extensive experiments demonstrate that RLG consistently improves the performance of RL fine-tuned models across various architectures, RL algorithms, and downstream tasks, including human preferences, compositional control, compressibility, and text rendering. Furthermore, RLG supports both interpolation and extrapolation, thereby offering unprecedented flexibility in controlling generative alignment. Our approach provides a practical and theoretically sound solution for enhancing and controlling diffusion model alignment at inference. The source code for RLG is publicly available at the Github: https://github.com/jinluo12345/Reinforcement-learning-guidance."
  },
  {
    "id": "http://arxiv.org/abs/2301.06375v2",
    "updated": "2025-08-28T17:12:29Z",
    "published": "2023-01-16T11:40:50Z",
    "title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset",
    "authors": [
      "Jeongkyun Park",
      "Jung-Wook Hwang",
      "Kwanghee Choi",
      "Seung-Hyun Lee",
      "Jun Hwan Ahn",
      "Rae-Hong Park",
      "Hyung-Min Park"
    ],
    "pdf_url": "https://arxiv.org/pdf/2301.06375v2.pdf",
    "comment": "Accepted to ICASSP 2024",
    "category": "Artificial Intelligence",
    "abstract": "Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the OLKAVS dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis."
  },
  {
    "id": "http://arxiv.org/abs/2508.21010v1",
    "updated": "2025-08-28T17:10:53Z",
    "published": "2025-08-28T17:10:53Z",
    "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Basura Fernando"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21010v1.pdf",
    "comment": "Project page: https://paritoshparmar.github.io/chainreaction/",
    "category": "Artificial Intelligence",
    "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/"
  },
  {
    "id": "http://arxiv.org/abs/2508.21001v1",
    "updated": "2025-08-28T17:04:00Z",
    "published": "2025-08-28T17:04:00Z",
    "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
    "authors": [
      "Yaniv Hassidof",
      "Tom Jurgenson",
      "Kiril Solovey"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21001v1.pdf",
    "comment": "Accepted to CoRL 2025. Project page: https://sites.google.com/view/ditree",
    "category": "Artificial Intelligence",
    "abstract": "Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a \\emph{provably-generalizable} framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs to yield \\emph{provably-safe} solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a \\emph{single environment}. In comprehensive evaluations on OOD scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than classical SBPs), while improving the average success rate over DP and SBPs. DiTree is on average 3x faster than classical SBPs, and outperforms all other approaches by achieving roughly 30\\% higher success rate. Project webpage: https://sites.google.com/view/ditree."
  },
  {
    "id": "http://arxiv.org/abs/2508.20996v1",
    "updated": "2025-08-28T16:57:33Z",
    "published": "2025-08-28T16:57:33Z",
    "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery",
    "authors": [
      "Junda Wang",
      "Zonghai Yao",
      "Zhichao Yang",
      "Lingxi Li",
      "Junhui Qian",
      "Hong Yu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20996v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet few receive effective care due to stigma, motivational barriers, and limited personalized support. Although large language models (LLMs) show promise for mental-health assistance, most systems lack tight integration with clinically validated strategies, reducing effectiveness in addiction recovery. We present ChatThero, a multi-agent conversational framework that couples dynamic patient modeling with context-sensitive therapeutic dialogue and adaptive persuasive strategies grounded in cognitive behavioral therapy (CBT) and motivational interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy, Medium, and Hard resistance levels, and train ChatThero with a two-stage pipeline comprising supervised fine-tuning (SFT) followed by direct preference optimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in patient motivation, a 0.49\\% increase in treatment confidence, and resolves hard cases with 26\\% fewer turns than GPT-4o, and both automated and human clinical assessments rate it higher in empathy, responsiveness, and behavioral realism. The framework supports rigorous, privacy-preserving study of therapeutic conversation and provides a robust, replicable basis for research and clinical translation."
  },
  {
    "id": "http://arxiv.org/abs/2508.20991v1",
    "updated": "2025-08-28T16:53:03Z",
    "published": "2025-08-28T16:53:03Z",
    "title": "ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts",
    "authors": [
      "Patryk Będkowski",
      "Jan Dubiński",
      "Filip Szatkowski",
      "Kamil Deja",
      "Przemysław Rokita",
      "Tomasz Trzciński"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20991v1.pdf",
    "comment": "Accepted at ECAI 2025 28th European Conference on Artificial Intelligence",
    "category": "Artificial Intelligence",
    "abstract": "Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts."
  },
  {
    "id": "http://arxiv.org/abs/2407.15161v3",
    "updated": "2025-08-28T16:44:25Z",
    "published": "2024-07-21T13:33:08Z",
    "title": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference",
    "authors": [
      "Qian Feng",
      "Jianxiang Feng",
      "Zhaopeng Chen",
      "Rudolph Triebel",
      "Alois Knoll"
    ],
    "pdf_url": "https://arxiv.org/pdf/2407.15161v3.pdf",
    "comment": "First two authors contributed equally, whose ordering decided via coin-tossing. Accepted for CoRL 2025",
    "category": "Artificial Intelligence",
    "abstract": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions (Project Page: https://sites.google.com/view/ffhflow/home/)."
  },
  {
    "id": "http://arxiv.org/abs/2507.22931v2",
    "updated": "2025-08-28T16:42:39Z",
    "published": "2025-07-24T13:46:51Z",
    "title": "Dynamic Context Compression for Efficient RAG",
    "authors": [
      "Shuyu Guo",
      "Zhaochun Ren"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.22931v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy."
  },
  {
    "id": "http://arxiv.org/abs/2505.03818v2",
    "updated": "2025-08-28T16:38:13Z",
    "published": "2025-05-02T20:03:35Z",
    "title": "Program Semantic Inequivalence Game with Large Language Models",
    "authors": [
      "Antonio Valerio Miceli-Barone",
      "Vaishak Belle",
      "Ali Payani"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.03818v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Large Language Models (LLMs) can achieve strong performance on everyday coding tasks, but they can fail on complex tasks that require non-trivial reasoning about program semantics. Finding training examples to teach LLMs to solve these tasks can be challenging. In this work, we explore a method to synthetically generate code reasoning training data based on a semantic inequivalence game SInQ: a generator agent creates program variants that are semantically distinct, derived from a dataset of real-world programming tasks, while an evaluator agent has to identify input examples that cause the original programs and the generated variants to diverge in their behaviour, with the agents training each other semi-adversarially. We prove that this setup enables theoretically unlimited improvement through self-play in the limit of infinite computational resources. We evaluated our approach on multiple code generation and understanding benchmarks, including cross-language vulnerability detection (Lu et al., 2021), where our method improves vulnerability detection in C/C++ code despite being trained exclusively on Python code, and the challenging Python builtin identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas modern LLMs still struggle with this benchmark, our approach yields substantial improvements. We release the code needed to replicate the experiments, as well as the generated synthetic data, which can be used to fine-tune LLMs."
  },
  {
    "id": "http://arxiv.org/abs/2508.20978v1",
    "updated": "2025-08-28T16:33:27Z",
    "published": "2025-08-28T16:33:27Z",
    "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective",
    "authors": [
      "Marianne Defresne",
      "Romain Gambardella",
      "Sophie Barbe",
      "Thomas Schiex"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20978v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with. Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems. Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy. Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins."
  },
  {
    "id": "http://arxiv.org/abs/2508.20976v1",
    "updated": "2025-08-28T16:29:46Z",
    "published": "2025-08-28T16:29:46Z",
    "title": "WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations",
    "authors": [
      "Jaeyeon Kim",
      "Heeseung Yun",
      "Sang Hoon Woo",
      "Chao-Han Huck Yang",
      "Gunhee Kim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20976v1.pdf",
    "comment": "Preprint. Project page: https://jaeyeonkim99.github.io/wow_bench/",
    "category": "Artificial Intelligence",
    "abstract": "Large audio language models (LALMs) extend language understanding into the auditory domain, yet their ability to perform low-level listening, such as pitch and duration detection, remains underexplored. However, low-level listening is critical for real-world, out-of-distribution tasks where models must reason about unfamiliar sounds based on fine-grained acoustic cues. To address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to evaluate low-level auditory perception and cognition using marine mammal vocalizations. WoW-bench is composed of a Perception benchmark for categorizing novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess the abilities to remember, understand, apply, and analyze sound events. For the Cognition benchmark, we additionally introduce distractor questions to evaluate whether models are truly solving problems through listening rather than relying on other heuristics. Experiments with state-of-the-art LALMs show performance far below human levels, indicating a need for stronger auditory grounding in LALMs."
  },
  {
    "id": "http://arxiv.org/abs/2508.20973v1",
    "updated": "2025-08-28T16:26:44Z",
    "published": "2025-08-28T16:26:44Z",
    "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents",
    "authors": [
      "Tianjian Liu",
      "Fanqi Wan",
      "Jiajian Guo",
      "Xiaojun Quan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20973v1.pdf",
    "comment": "21 pages, 6 Figures",
    "category": "Artificial Intelligence",
    "abstract": "Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development."
  },
  {
    "id": "http://arxiv.org/abs/2508.20953v1",
    "updated": "2025-08-28T16:16:10Z",
    "published": "2025-08-28T16:16:10Z",
    "title": "A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling",
    "authors": [
      "Vipul Patel",
      "Anirudh Deodhar",
      "Dagnachew Birru"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20953v1.pdf",
    "comment": "8 pages, 7 figures, Accepted at the Multi-Objective Decision Making Workshop (MODeM2025) at ECAI 2025",
    "category": "Artificial Intelligence",
    "abstract": "Workforce scheduling in the healthcare sector is a significant operational challenge, characterized by fluctuating patient loads, diverse clinical skills, and the critical need to control labor costs while upholding high standards of patient care. This problem is inherently multi-objective, demanding a delicate balance between competing goals: minimizing payroll, ensuring adequate staffing for patient needs, and accommodating staff preferences to mitigate burnout. We propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital unit workforce scheduling problem as a multi-objective optimization task. Our model incorporates real-world complexities, including hourly appointment-driven demand and the use of modular shifts for a multi-skilled workforce. By defining objective functions for cost, patient care coverage, and staff satisfaction, the GA navigates the vast search space to identify a set of high-quality, non-dominated solutions. Demonstrated on datasets representing a typical hospital unit, the results show that our MOO-GA generates robust and balanced schedules. On average, the schedules produced by our algorithm showed a 66\\% performance improvement over a baseline that simulates a conventional, manual scheduling process. This approach effectively manages trade-offs between critical operational and staff-centric objectives, providing a practical decision support tool for nurse managers and hospital administrators."
  },
  {
    "id": "http://arxiv.org/abs/2508.11017v2",
    "updated": "2025-08-28T15:51:55Z",
    "published": "2025-08-14T18:44:13Z",
    "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
    "authors": [
      "Carter Blum",
      "Katja Filippova",
      "Ann Yuan",
      "Asma Ghandeharioun",
      "Julian Zimmert",
      "Fred Zhang",
      "Jessica Hoffmann",
      "Tal Linzen",
      "Martin Wattenberg",
      "Lucas Dixon",
      "Mor Geva"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11017v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs."
  },
  {
    "id": "http://arxiv.org/abs/2508.20912v1",
    "updated": "2025-08-28T15:41:49Z",
    "published": "2025-08-28T15:41:49Z",
    "title": "Research Challenges in Relational Database Management Systems for LLM Queries",
    "authors": [
      "Kerem Akillioglu",
      "Anurag Chakraborty",
      "Sairaj Voruganti",
      "M. Tamer Özsu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20912v1.pdf",
    "comment": "This paper will appear in the 6th International Workshop on Applied AI for Database Systems and Applications, AIDB Workshop at VLDB 2025",
    "category": "Artificial Intelligence",
    "abstract": "Large language models (LLMs) have become essential for applications such as text summarization, sentiment analysis, and automated question-answering. Recently, LLMs have also been integrated into relational database management systems to enhance querying and support advanced data processing. Companies such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly within SQL, denoted as LLM queries, to boost data insights. However, open-source solutions currently have limited functionality and poor performance. In this work, we present an early exploration of two open-source systems and one enterprise platform, using five representative queries to expose functional, performance, and scalability limits in today's SQL-invoked LLM integrations. We identify three main issues: enforcing structured outputs, optimizing resource utilization, and improving query planning. We implemented initial solutions and observed improvements in accommodating LLM powered SQL queries. These early gains demonstrate that tighter integration of LLM+DBMS is the key to scalable and efficient processing of LLM queries."
  },
  {
    "id": "http://arxiv.org/abs/2508.20907v1",
    "updated": "2025-08-28T15:37:40Z",
    "published": "2025-08-28T15:37:40Z",
    "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant",
    "authors": [
      "Nicolas Dupuis",
      "Adarsh Tiwari",
      "Youssef Mroueh",
      "David Kremer",
      "Ismael Faro",
      "Juan Cruz-Benito"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20907v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Qiskit is an open-source quantum computing framework that allows users to design, simulate, and run quantum circuits on real quantum hardware. We explore post-training techniques for LLMs to assist in writing Qiskit code. We introduce quantum verification as an effective method for ensuring code quality and executability on quantum hardware. To support this, we developed a synthetic data pipeline that generates quantum problem-unit test pairs and used it to create preference data for aligning LLMs with DPO. Additionally, we trained models using GRPO, leveraging quantum-verifiable rewards provided by the quantum hardware. Our best-performing model, combining DPO and GRPO, surpasses the strongest open-source baselines on the challenging Qiskit-HumanEval-hard benchmark."
  },
  {
    "id": "http://arxiv.org/abs/2503.17513v2",
    "updated": "2025-08-28T15:33:02Z",
    "published": "2025-03-21T19:56:59Z",
    "title": "Improving Quantization with Post-Training Model Expansion",
    "authors": [
      "Giuseppe Franco",
      "Pablo Monteagudo-Lago",
      "Ian Colbert",
      "Nicholas Fraser",
      "Michaela Blott"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.17513v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "The size of a model has been a strong predictor of its quality, as well as its cost. As such, the trade-off between model cost and quality has been well-studied. Post-training optimizations like quantization and pruning have typically focused on reducing the overall volume of pre-trained models to reduce inference costs while maintaining model quality. However, recent advancements have introduced optimization techniques that, interestingly, expand models post-training, increasing model size to improve quality when reducing volume. For instance, to enable 4-bit weight and activation quantization, incoherence processing often necessitates inserting online Hadamard rotations in the compute graph, and preserving highly sensitive weights often calls for additional higher precision computations. However, if application requirements cannot be met, the prevailing solution is to relax quantization constraints. In contrast, we demonstrate post-training model expansion is a viable strategy to improve model quality within a quantization co-design space, and provide theoretical justification. We show it is possible to progressively and selectively expand the size of a pre-trained large language model (LLM) to improve model quality without end-to-end retraining. In particular, when quantizing the weights and activations to 4 bits for Llama3 1B, we reduce the gap to full-precision perplexity by an average of 9% relative to both QuaRot and SpinQuant with only 5% more parameters, which is still a 3.8% reduction in volume relative to a BF16 reference model."
  },
  {
    "id": "http://arxiv.org/abs/2507.17232v2",
    "updated": "2025-08-28T15:15:18Z",
    "published": "2025-07-23T05:56:20Z",
    "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task",
    "authors": [
      "Mashiro Toyooka",
      "Kiyoharu Aizawa",
      "Yoko Yamakata"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.17232v2.pdf",
    "comment": "Accepted to ACM Multimedia 2025. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
    "category": "Artificial Intelligence",
    "abstract": "Large Language Models (LLMs) are trained on a vast amount of procedural texts, but they do not directly observe real-world phenomena. In the context of cooking recipes, this poses a challenge, as intermediate states of ingredients are often omitted, making it difficult for models to track ingredient states and understand recipes accurately. In this paper, we apply state probing, a method for evaluating a language model's understanding of the world, to the domain of cooking. We propose a new task and dataset for evaluating how well LLMs can recognize intermediate ingredient states during cooking procedures. We first construct a new Japanese recipe dataset with clear and accurate annotations of ingredient state changes, collected from well-structured and controlled recipe texts. Using this dataset, we design three novel tasks to evaluate whether LLMs can track ingredient state transitions and identify ingredients present at intermediate steps. Our experiments with widely used LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state knowledge improves their understanding of cooking processes, achieving performance comparable to commercial LLMs. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1"
  },
  {
    "id": "http://arxiv.org/abs/2508.20866v1",
    "updated": "2025-08-28T14:59:39Z",
    "published": "2025-08-28T14:59:39Z",
    "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning",
    "authors": [
      "Amine Lbath",
      "Massih-Reza Amini",
      "Aurelien Delaitre",
      "Vadim Okun"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20866v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the critical need for effective automated vulnerability detection and repair systems. Traditional methods, such as static program analysis, face significant challenges related to scalability, adaptability, and high false-positive and false-negative rates. AI-driven approaches, particularly those using machine learning and deep learning models, show promise but are heavily reliant on the quality and quantity of training data. This paper introduces a novel framework designed to automatically introduce realistic, category-specific vulnerabilities into secure C/C++ codebases to generate datasets. The proposed approach coordinates multiple AI agents that simulate expert reasoning, along with function agents and traditional code analysis tools. It leverages Retrieval-Augmented Generation for contextual grounding and employs Low-Rank approximation of weights for efficient model fine-tuning. Our experimental study on 116 code samples from three different benchmarks suggests that our approach outperforms other techniques with regard to dataset accuracy, achieving between 89\\% and 95\\% success rates in injecting vulnerabilities at function level."
  },
  {
    "id": "http://arxiv.org/abs/2508.20848v1",
    "updated": "2025-08-28T14:40:27Z",
    "published": "2025-08-28T14:40:27Z",
    "title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring",
    "authors": [
      "Junjie Chu",
      "Mingjie Li",
      "Ziqing Yang",
      "Ye Leng",
      "Chenhao Lin",
      "Chao Shen",
      "Michael Backes",
      "Yun Shen",
      "Yang Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20848v1.pdf",
    "comment": "17 pages, 5 figures. For the code and data supporting this work, see https://trustairlab.github.io/jades.github.io/",
    "category": "Artificial Intelligence",
    "abstract": "Accurately determining whether a jailbreak attempt has succeeded is a fundamental yet unresolved challenge. Existing evaluation methods rely on misaligned proxy indicators or naive holistic judgments. They frequently misinterpret model responses, leading to inconsistent and subjective assessments that misalign with human perception. To address this gap, we introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal jailbreak evaluation framework. Its key mechanism is to automatically decompose an input harmful question into a set of weighted sub-questions, score each sub-answer, and weight-aggregate the sub-scores into a final decision. JADES also incorporates an optional fact-checking module to strengthen the detection of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a newly introduced benchmark proposed in this work, consisting of 400 pairs of jailbreak prompts and responses, each meticulously annotated by humans. In a binary setting (success/failure), JADES achieves 98.5% agreement with human evaluators, outperforming strong baselines by over 9%. Re-evaluating five popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show that JADES could deliver accurate, consistent, and interpretable evaluations, providing a reliable basis for measuring future jailbreak attacks."
  },
  {
    "id": "http://arxiv.org/abs/2508.14926v2",
    "updated": "2025-08-28T14:35:03Z",
    "published": "2025-08-19T14:24:02Z",
    "title": "Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving",
    "authors": [
      "Dianzhao Li",
      "Ostap Okhrin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.14926v2.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding robust ethical reasoning into routine and emergency maneuvers, particularly to protect vulnerable road users (VRUs) such as pedestrians and cyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that explicitly integrates moral considerations with standard driving objectives. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on rich, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing ethical risk and maintaining driving performance. To our knowledge, this is the first study of ethical decision-making for autonomous vehicles via Safe RL evaluated on real-world, human-mixed traffic scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy that explicitly protects those most at risk in urban traffic environments."
  },
  {
    "id": "http://arxiv.org/abs/2508.20840v1",
    "updated": "2025-08-28T14:31:48Z",
    "published": "2025-08-28T14:31:48Z",
    "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic Learning",
    "authors": [
      "Qiao Sun",
      "Liujia Yang",
      "Wei Tang",
      "Wei Huang",
      "Kaixin Xu",
      "Yongchao Chen",
      "Mingyu Liu",
      "Jiange Yang",
      "Haoyi Zhu",
      "Yating Wang",
      "Tong He",
      "Yilun Chen",
      "Xili Dai",
      "Nanyang Ye",
      "Qinying Gu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20840v1.pdf",
    "comment": null,
    "category": "Artificial Intelligence",
    "abstract": "While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \"GPT moment\" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence."
  },
  {
    "id": "http://arxiv.org/abs/2508.21072v1",
    "updated": "2025-08-28T17:59:59Z",
    "published": "2025-08-28T17:59:59Z",
    "title": "First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge",
    "authors": [
      "Fahad Shamshad",
      "Tameem Bakr",
      "Yahia Shaaban",
      "Noor Hussein",
      "Karthik Nandakumar",
      "Nils Lukas"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21072v1.pdf",
    "comment": "Winning solution to the NeurIPS 2024 Erasing the Invisible challenge",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods."
  },
  {
    "id": "http://arxiv.org/abs/2508.21070v1",
    "updated": "2025-08-28T17:59:55Z",
    "published": "2025-08-28T17:59:55Z",
    "title": "Dress&Dance: Dress up and Dance as You Like It - Technical Preview",
    "authors": [
      "Jun-Kun Chen",
      "Aayush Bansal",
      "Minh Phuoc Vo",
      "Yu-Xiong Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21070v1.pdf",
    "comment": "Project Page: https://immortalco.github.io/DressAndDance/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We present Dress&Dance, a video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a user wearing desired garments while moving in accordance with a given reference video. Our approach requires a single user image and supports a range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in a single pass. Key to our framework is CondNet, a novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and a larger, more readily available image dataset, in a multistage progressive manner. Dress&Dance outperforms existing open source and commercial solutions and enables a high quality and flexible try-on experience."
  },
  {
    "id": "http://arxiv.org/abs/2508.21066v1",
    "updated": "2025-08-28T17:59:46Z",
    "published": "2025-08-28T17:59:46Z",
    "title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning",
    "authors": [
      "Yuan Gong",
      "Xionghui Wang",
      "Jie Wu",
      "Shiyin Wang",
      "Yitong Wang",
      "Xinglong Wu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21066v1.pdf",
    "comment": "project url: https://one-reward.github.io",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \\textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io"
  },
  {
    "id": "http://arxiv.org/abs/2508.21060v1",
    "updated": "2025-08-28T17:58:20Z",
    "published": "2025-08-28T17:58:20Z",
    "title": "Multi-View 3D Point Tracking",
    "authors": [
      "Frano Rajič",
      "Haofei Xu",
      "Marko Mihajlovic",
      "Siyuan Li",
      "Irem Demir",
      "Emircan Gündoğdu",
      "Lei Ke",
      "Sergey Prokudin",
      "Marc Pollefeys",
      "Siyu Tang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21060v1.pdf",
    "comment": "ICCV 2025, Oral. Project page: https://ethz-vlg.github.io/mvtracker",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page available at https://ethz-vlg.github.io/mvtracker."
  },
  {
    "id": "http://arxiv.org/abs/2508.21058v1",
    "updated": "2025-08-28T17:57:55Z",
    "published": "2025-08-28T17:57:55Z",
    "title": "Mixture of Contexts for Long Video Generation",
    "authors": [
      "Shengqu Cai",
      "Ceyuan Yang",
      "Lvmin Zhang",
      "Yuwei Guo",
      "Junfei Xiao",
      "Ziyan Yang",
      "Yinghao Xu",
      "Zhenheng Yang",
      "Alan Yuille",
      "Leonidas Guibas",
      "Maneesh Agrawala",
      "Lu Jiang",
      "Gordon Wetzstein"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21058v1.pdf",
    "comment": "Project page: https://primecai.github.io/moc/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes."
  },
  {
    "id": "http://arxiv.org/abs/2508.21052v1",
    "updated": "2025-08-28T17:55:14Z",
    "published": "2025-08-28T17:55:14Z",
    "title": "FakeParts: a New Family of AI-Generated DeepFakes",
    "authors": [
      "Gaetan Brison",
      "Soobash Daiboo",
      "Samy Aimeur",
      "Awais Hussain Sani",
      "Xi Wang",
      "Gianni Franchi",
      "Vicky Kalogeiton"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21052v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations."
  },
  {
    "id": "http://arxiv.org/abs/2508.21048v1",
    "updated": "2025-08-28T17:53:05Z",
    "published": "2025-08-28T17:53:05Z",
    "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
    "authors": [
      "Hao Tan",
      "Jun Lan",
      "Zichang Tan",
      "Ajian Liu",
      "Chuanbiao Song",
      "Senyuan Shi",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jun Wan",
      "Zhen Lei"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21048v1.pdf",
    "comment": "Project: https://github.com/EricTan7/Veritas",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as \"planning\" and \"self-reflection\" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs."
  },
  {
    "id": "http://arxiv.org/abs/2508.21046v1",
    "updated": "2025-08-28T17:50:58Z",
    "published": "2025-08-28T17:50:58Z",
    "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
    "authors": [
      "Wei Li",
      "Renshan Zhang",
      "Rui Shao",
      "Jie He",
      "Liqiang Nie"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21046v1.pdf",
    "comment": "23 pages, 8 figures, Project Page: https://jiutian-vl.github.io/CogVLA-page",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA."
  },
  {
    "id": "http://arxiv.org/abs/2508.21044v1",
    "updated": "2025-08-28T17:50:03Z",
    "published": "2025-08-28T17:50:03Z",
    "title": "MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs",
    "authors": [
      "Junpeng Ma",
      "Qizhe Zhang",
      "Ming Lu",
      "Zhibin Wang",
      "Qiang Zhou",
      "Jun Song",
      "Shanghang Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21044v1.pdf",
    "comment": "10 pages, 3 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Video Large Language Models (VLLMs) excel in video understanding, but their excessive visual tokens pose a significant computational challenge for real-world applications. Current methods aim to enhance inference efficiency by visual token pruning. However, they do not consider the dynamic characteristics and temporal dependencies of video frames, as they perceive video understanding as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel training-free visual token pruning framework that removes redundancy by Maximizing Marginal Gains at both segment-level and token-level. Specifically, we first divide the video into segments based on frame similarity, and then dynamically allocate the token budget for each segment to maximize the marginal gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm that jointly models inter-frame uniqueness and intra-frame diversity, thereby maximizing the marginal gain of each token. By combining both stages, MMG-Vid can maximize the utilization of the limited token budget, significantly improving efficiency while maintaining strong performance. Extensive experiments demonstrate that MMG-Vid can maintain over 99.5% of the original performance, while effectively reducing 75% visual tokens and accelerating the prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon."
  },
  {
    "id": "http://arxiv.org/abs/2508.21041v1",
    "updated": "2025-08-28T17:45:22Z",
    "published": "2025-08-28T17:45:22Z",
    "title": "Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025",
    "authors": [
      "Guillaume Balezo",
      "Raphaël Bourgade",
      "Thomas Walter"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21041v1.pdf",
    "comment": "3 pages. Challenge report for MIDOG 2025 (Task 2: Atypical Mitotic Figure Classification)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Atypical mitotic figures (AMFs) are markers of abnormal cell division associated with poor prognosis, yet their detection remains difficult due to low prevalence, subtle morphology, and inter-observer variability. The MIDOG 2025 challenge introduces a benchmark for AMF classification across multiple domains. In this work, we evaluate the recently published DINOv3-H+ vision transformer, pretrained on natural images, which we fine-tuned using low-rank adaptation (LoRA, 650k trainable parameters) and extensive augmentation. Despite the domain gap, DINOv3 transfers effectively to histopathology, achieving a balanced accuracy of 0.8871 on the preliminary test set. These results highlight the robustness of DINOv3 pretraining and show that, when combined with parameter-efficient fine-tuning, it provides a strong baseline for atypical mitosis classification in MIDOG 2025."
  },
  {
    "id": "http://arxiv.org/abs/2508.10950v2",
    "updated": "2025-08-28T17:44:54Z",
    "published": "2025-08-13T17:56:29Z",
    "title": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement",
    "authors": [
      "Xinyi Wang",
      "Michael Barnett",
      "Frederique Boonstra",
      "Yael Barnett",
      "Mariano Cabezas",
      "Arkiev D'Souza",
      "Matthew C. Kiernan",
      "Kain Kyle",
      "Meng Law",
      "Lynette Masters",
      "Zihao Tang",
      "Stephen Tisch",
      "Sicong Tu",
      "Anneke Van Der Walt",
      "Dongang Wang",
      "Fernando Calamante",
      "Weidong Cai",
      "Chenyu Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.10950v2.pdf",
    "comment": "24 pages, 5 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling technique that represents complex white matter fiber configurations, and a key step for subsequent brain tractography and connectome analysis. Its reliability and accuracy, however, heavily rely on the quality of the MRI acquisition and the subsequent estimation of the FODs at each voxel. Generating reliable FODs from widely available clinical protocols with single-shell and low-angular-resolution acquisitions remains challenging but could potentially be addressed with recent advances in deep learning-based enhancement techniques. Despite advancements, existing methods have predominantly been assessed on healthy subjects, which have proved to be a major hurdle for their clinical adoption. In this work, we validate a newly optimized enhancement framework, FastFOD-Net, across healthy controls and six neurological disorders. This accelerated end-to-end deep learning framework enhancing FODs with superior performance and delivering training/inference efficiency for clinical use ($60\\times$ faster comparing to its predecessor). With the most comprehensive clinical evaluation to date, our work demonstrates the potential of FastFOD-Net in accelerating clinical neuroscience research, empowering diffusion MRI analysis for disease differentiation, improving interpretability in connectome applications, and reducing measurement errors to lower sample size requirements. Critically, this work will facilitate the more widespread adoption of, and build clinical trust in, deep learning based methods for diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of real-world, clinical diffusion MRI data, comparable to that achievable with high-quality research acquisitions."
  },
  {
    "id": "http://arxiv.org/abs/2508.21040v1",
    "updated": "2025-08-28T17:44:52Z",
    "published": "2025-08-28T17:44:52Z",
    "title": "FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator",
    "authors": [
      "Huynh Tong Dang Khoa",
      "Dang Hoai Nam",
      "Vo Nguyen Le Duy"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21040v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Labeled handwriting data is often scarce, limiting the effectiveness of recognition systems that require diverse, style-consistent training samples. Handwriting synthesis offers a promising solution by generating artificial data to augment training. However, current methods face two major limitations. First, most are built on conventional convolutional architectures, which struggle to model long-range dependencies and complex stroke patterns. Second, they largely ignore the crucial role of frequency information, which is essential for capturing fine-grained stylistic and structural details in handwriting. To address these challenges, we propose FW-GAN, a one-shot handwriting synthesis framework that generates realistic, writer-consistent text from a single example. Our generator integrates a phase-aware Wave-MLP to better capture spatial relationships while preserving subtle stylistic cues. We further introduce a frequency-guided discriminator that leverages high-frequency components to enhance the authenticity detection of generated samples. Additionally, we introduce a novel Frequency Distribution Loss that aligns the frequency characteristics of synthetic and real handwriting, thereby enhancing visual fidelity. Experiments on Vietnamese and English handwriting datasets demonstrate that FW-GAN generates high-quality, style-consistent handwriting, making it a valuable tool for augmenting data in low-resource handwriting recognition (HTR) pipelines. Official implementation is available at https://github.com/DAIR-Group/FW-GAN"
  },
  {
    "id": "http://arxiv.org/abs/2508.21035v1",
    "updated": "2025-08-28T17:39:30Z",
    "published": "2025-08-28T17:39:30Z",
    "title": "A multi-task neural network for atypical mitosis recognition under domain shift",
    "authors": [
      "Gennaro Percannella",
      "Mattia Sarno",
      "Francesco Tortorella",
      "Mario Vento"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21035v1.pdf",
    "comment": "Approach for MIDOG25 track 2",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recognizing atypical mitotic figures in histopathology images allows physicians to correctly assess tumor aggressiveness. Although machine learning models could be exploited for automatically performing such a task, under domain shift these models suffer from significative performance drops. In this work, an approach based on multi-task learning is proposed for addressing this problem. By exploiting auxiliary tasks, correlated to the main classification task, the proposed approach, submitted to the track 2 of the MItosis DOmain Generalization (MIDOG) challenge, aims to aid the model to focus only on the object to classify, ignoring the domain varying background of the image. The proposed approach shows promising performance in a preliminary evaluation conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25 challenge."
  },
  {
    "id": "http://arxiv.org/abs/2508.21033v1",
    "updated": "2025-08-28T17:38:30Z",
    "published": "2025-08-28T17:38:30Z",
    "title": "Mitosis detection in domain shift scenarios: a Mamba-based approach",
    "authors": [
      "Gennaro Percannella",
      "Mattia Sarno",
      "Francesco Tortorella",
      "Mario Vento"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21033v1.pdf",
    "comment": "Approach for MIDOG 2025 track 1",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Mitosis detection in histopathology images plays a key role in tumor assessment. Although machine learning algorithms could be exploited for aiding physicians in accurately performing such a task, these algorithms suffer from significative performance drop when evaluated on images coming from domains that are different from the training ones. In this work, we propose a Mamba-based approach for mitosis detection under domain shift, inspired by the promising performance demonstrated by Mamba in medical imaging segmentation tasks. Specifically, our approach exploits a VM-UNet architecture for carrying out the addressed task, as well as stain augmentation operations for further improving model robustness against domain shift. Our approach has been submitted to the track 1 of the MItosis DOmain Generalization (MIDOG) challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show large room for improvement for the proposed method."
  },
  {
    "id": "http://arxiv.org/abs/2508.21032v1",
    "updated": "2025-08-28T17:35:03Z",
    "published": "2025-08-28T17:35:03Z",
    "title": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets",
    "authors": [
      "Dale Decatur",
      "Thibault Groueix",
      "Wang Yifan",
      "Rana Hanocka",
      "Vladimir Kim",
      "Matheus Gadelha"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21032v1.pdf",
    "comment": "ICCV 2025. Project page: https://ddecatur.github.io/hierarchical-diffusion/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: https://ddecatur.github.io/hierarchical-diffusion/"
  },
  {
    "id": "http://arxiv.org/abs/2508.21019v1",
    "updated": "2025-08-28T17:20:01Z",
    "published": "2025-08-28T17:20:01Z",
    "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models",
    "authors": [
      "Jiaxiang Cheng",
      "Bing Ma",
      "Xuhua Ren",
      "Hongyi Jin",
      "Kai Yu",
      "Peng Zhang",
      "Wenyue Li",
      "Yuan Zhou",
      "Tianxiang Zheng",
      "Qinglin Lu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21019v1.pdf",
    "comment": "Project Page: https://pose-paper.github.io",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance."
  },
  {
    "id": "http://arxiv.org/abs/2505.07818v4",
    "updated": "2025-08-28T17:19:45Z",
    "published": "2025-05-12T17:59:34Z",
    "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
    "authors": [
      "Zeyue Xue",
      "Jie Wu",
      "Yu Gao",
      "Fangyuan Kong",
      "Lingting Zhu",
      "Mengzhao Chen",
      "Zhiheng Liu",
      "Wei Liu",
      "Qiushan Guo",
      "Weilin Huang",
      "Ping Luo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.07818v4.pdf",
    "comment": "Project Page: https://dancegrpo.github.io/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in generative AI have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. While Reinforcement Learning (RL) has emerged as a promising approach for fine-tuning generative models, existing methods like DDPO and DPOK face fundamental limitations - particularly their inability to maintain stable optimization when scaling to large and diverse prompt sets, severely restricting their practical utility. This paper presents DanceGRPO, a framework that addresses these limitations through an innovative adaptation of Group Relative Policy Optimization (GRPO) for visual generation tasks. Our key insight is that GRPO's inherent stability mechanisms uniquely position it to overcome the optimization challenges that plague prior RL-based approaches on visual generation. DanceGRPO establishes several significant advances: First, it demonstrates consistent and stable policy optimization across multiple modern generative paradigms, including both diffusion models and rectified flows. Second, it maintains robust performance when scaling to complex, real-world scenarios encompassing three key tasks and four foundation models. Third, it shows remarkable versatility in optimizing for diverse human preferences as captured by five distinct reward models assessing image/video aesthetics, text-image alignment, video motion quality, and binary feedback. Our comprehensive experiments reveal that DanceGRPO outperforms baseline methods by up to 181\\% across multiple established benchmarks, including HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis."
  },
  {
    "id": "http://arxiv.org/abs/2301.06375v2",
    "updated": "2025-08-28T17:12:29Z",
    "published": "2023-01-16T11:40:50Z",
    "title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset",
    "authors": [
      "Jeongkyun Park",
      "Jung-Wook Hwang",
      "Kwanghee Choi",
      "Seung-Hyun Lee",
      "Jun Hwan Ahn",
      "Rae-Hong Park",
      "Hyung-Min Park"
    ],
    "pdf_url": "https://arxiv.org/pdf/2301.06375v2.pdf",
    "comment": "Accepted to ICASSP 2024",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the OLKAVS dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis."
  },
  {
    "id": "http://arxiv.org/abs/2508.21010v1",
    "updated": "2025-08-28T17:10:53Z",
    "published": "2025-08-28T17:10:53Z",
    "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Basura Fernando"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21010v1.pdf",
    "comment": "Project page: https://paritoshparmar.github.io/chainreaction/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/"
  },
  {
    "id": "http://arxiv.org/abs/2508.20991v1",
    "updated": "2025-08-28T16:53:03Z",
    "published": "2025-08-28T16:53:03Z",
    "title": "ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts",
    "authors": [
      "Patryk Będkowski",
      "Jan Dubiński",
      "Filip Szatkowski",
      "Kamil Deja",
      "Przemysław Rokita",
      "Tomasz Trzciński"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20991v1.pdf",
    "comment": "Accepted at ECAI 2025 28th European Conference on Artificial Intelligence",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts."
  },
  {
    "id": "http://arxiv.org/abs/2312.05407v4",
    "updated": "2025-08-28T16:47:53Z",
    "published": "2023-12-08T23:43:17Z",
    "title": "ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation",
    "authors": [
      "Md Shazid Islam",
      "Sayak Nag",
      "Arindam Dutta",
      "Miraj Ahmed",
      "Fahim Faisal Niloy",
      "Shreyangshu Bera",
      "Amit K. Roy-Chowdhury"
    ],
    "pdf_url": "https://arxiv.org/pdf/2312.05407v4.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Unsupervised domain adaptive segmentation typically relies on self-training using pseudo labels predicted by a pre-trained network on an unlabeled target dataset. However, the noisy nature of such pseudo-labels presents a major bottleneck in adapting a network to the distribution shift between source and target datasets. This challenge is exaggerated when the network encounters an incoming data stream in online fashion, where the network is constrained to adapt to incoming streams of target domain data in exactly one round of forward and backward passes. In this scenario, relying solely on inaccurate pseudo-labels can lead to low-quality segmentation, which is detrimental to medical image analysis where accuracy and precision are of utmost priority. We hypothesize that a small amount of pixel-level annotation obtained from an expert can address this problem, thereby enhancing the performance of domain adaptation of online streaming data, even in the absence of dedicated training data. We call our method ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation that adapts to each incoming data batch in an online setup, incorporating feedback from an expert through active learning. Through active learning, the most informative pixels in each image can be selected for expert annotation. However, the acquisition of pixel-level annotations across all images in a batch often leads to redundant information while increasing temporal overhead in online learning. To reduce the annotation acquisition time and make the adaptation process more online-friendly, we further propose a novel image-pruning strategy that selects the most useful subset of images from the current batch for active learning. Our proposed approach outperforms existing online adaptation approaches and produces competitive results compared to offline domain adaptive active learning methods."
  },
  {
    "id": "http://arxiv.org/abs/2507.14743v3",
    "updated": "2025-08-28T16:45:48Z",
    "published": "2025-07-19T20:30:43Z",
    "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic",
    "authors": [
      "Joseph Raj Vishal",
      "Divesh Basina",
      "Rutuja Patil",
      "Manas Srinivas Gowda",
      "Katha Naik",
      "Yezhou Yang",
      "Bharatesh Chakravarthi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.14743v3.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Traffic monitoring is crucial for urban mobility, road safety, and intelligent transportation systems (ITS). Deep learning has advanced video-based traffic monitoring through video question answering (VideoQA) models, enabling structured insight extraction from traffic videos. However, existing VideoQA models struggle with the complexity of real-world traffic scenes, where multiple concurrent events unfold across spatiotemporal dimensions. To address these challenges, this paper introduces \\textbf{InterAct VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of real-world traffic footage collected from diverse intersections, segmented into 10-second video clips, with over 25,000 question-answer (QA) pairs covering spatiotemporal dynamics, vehicle interactions, incident detection, and other critical traffic attributes. State-of-the-art VideoQA models are evaluated on InterAct VideoQA, exposing challenges in reasoning over fine-grained spatiotemporal dependencies within complex traffic scenarios. Additionally, fine-tuning these models on InterAct VideoQA yields notable performance improvements, demonstrating the necessity of domain-specific datasets for VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to facilitate future research in real-world deployable VideoQA models for intelligent transportation systems. GitHub Repo: https://github.com/joe-rabbit/InterAct_VideoQA"
  },
  {
    "id": "http://arxiv.org/abs/2508.20987v1",
    "updated": "2025-08-28T16:44:40Z",
    "published": "2025-08-28T16:44:40Z",
    "title": "Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation",
    "authors": [
      "Chenfan Qu",
      "Yiwu Zhong",
      "Bin Li",
      "Lianwen Jin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20987v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Images manipulated using image editing tools can mislead viewers and pose significant risks to social security. However, accurately localizing the manipulated regions within an image remains a challenging problem. One of the main barriers in this area is the high cost of data acquisition and the severe lack of high-quality annotated datasets. To address this challenge, we introduce novel methods that mitigate data scarcity by leveraging readily available web data. We utilize a large collection of manually forged images from the web, as well as automatically generated annotations derived from a simpler auxiliary task, constrained image manipulation localization. Specifically, we introduce a new paradigm CAAAv2, which automatically and accurately annotates manipulated regions at the pixel level. To further improve annotation quality, we propose a novel metric, QES, which filters out unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a large-scale, diverse, and high-quality dataset containing 246,212 manually forged images with pixel-level mask annotations. This is over 120x larger than existing handcrafted datasets like IMD20. Additionally, we introduce Object Jitter, a technique that further enhances model training by generating high-quality manipulation artifacts. Building on these advances, we develop a new model, Web-IML, designed to effectively leverage web-scale supervision for the image manipulation localization task. Extensive experiments demonstrate that our approach substantially alleviates the data scarcity problem and significantly improves the performance of various models on multiple real-world forgery benchmarks. With the proposed web supervision, Web-IML achieves a striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1 average IoU points. The dataset and code will be made publicly available at https://github.com/qcf-568/MIML."
  },
  {
    "id": "http://arxiv.org/abs/2508.20981v1",
    "updated": "2025-08-28T16:36:02Z",
    "published": "2025-08-28T16:36:02Z",
    "title": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection",
    "authors": [
      "Jiajie Li",
      "Boyang Sun",
      "Luca Di Giammarino",
      "Hermann Blum",
      "Marc Pollefeys"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20981v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Reliable localization is critical for robot navigation, yet most existing systems implicitly assume that all viewing directions at a location are equally informative. In practice, localization becomes unreliable when the robot observes unmapped, ambiguous, or uninformative regions. To address this, we present ActLoc, an active viewpoint-aware planning framework for enhancing localization accuracy for general robot navigation tasks. At its core, ActLoc employs a largescale trained attention-based model for viewpoint selection. The model encodes a metric map and the camera poses used during map construction, and predicts localization accuracy across yaw and pitch directions at arbitrary 3D locations. These per-point accuracy distributions are incorporated into a path planner, enabling the robot to actively select camera orientations that maximize localization robustness while respecting task and motion constraints. ActLoc achieves stateof-the-art results on single-viewpoint selection and generalizes effectively to fulltrajectory planning. Its modular design makes it readily applicable to diverse robot navigation and inspection tasks."
  },
  {
    "id": "http://arxiv.org/abs/2508.20965v1",
    "updated": "2025-08-28T16:22:54Z",
    "published": "2025-08-28T16:22:54Z",
    "title": "DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes",
    "authors": [
      "Yajiao Xiong",
      "Xiaoyu Zhou",
      "Yongtao Wan",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20965v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We present DrivingGaussian++, an efficient and effective framework for realistic reconstructing and controllable editing of surrounding dynamic autonomous driving scenes. DrivingGaussian++ models the static background using incremental 3D Gaussians and reconstructs moving objects with a composite dynamic Gaussian graph, ensuring accurate positions and occlusions. By integrating a LiDAR prior, it achieves detailed and consistent scene reconstruction, outperforming existing methods in dynamic scene reconstruction and photorealistic surround-view synthesis. DrivingGaussian++ supports training-free controllable editing for dynamic driving scenes, including texture modification, weather simulation, and object manipulation, leveraging multi-view images and depth priors. By integrating large language models (LLMs) and controllable editing, our method can automatically generate dynamic object motion trajectories and enhance their realism during the optimization process. DrivingGaussian++ demonstrates consistent and realistic editing results and generates dynamic multi-view driving scenarios, while significantly enhancing scene diversity. More results and code can be found at the project site: https://xiong-creator.github.io/DrivingGaussian_plus.github.io"
  },
  {
    "id": "http://arxiv.org/abs/2412.14195v3",
    "updated": "2025-08-28T16:18:41Z",
    "published": "2024-12-13T11:29:05Z",
    "title": "A multimodal dataset for understanding the impact of mobile phones on remote online virtual education",
    "authors": [
      "Roberto Daza",
      "Alvaro Becerra",
      "Ruth Cobos",
      "Julian Fierrez",
      "Aythami Morales"
    ],
    "pdf_url": "https://arxiv.org/pdf/2412.14195v3.pdf",
    "comment": "Published in Scientific Data (Nature). GitHub repository of the dataset at: https://github.com/BiDAlab/IMPROVE",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This work presents the IMPROVE dataset, a multimodal resource designed to evaluate the effects of mobile phone usage on learners during online education. It includes behavioral, biometric, physiological, and academic performance data collected from 120 learners divided into three groups with different levels of phone interaction, enabling the analysis of the impact of mobile phone usage and related phenomena such as nomophobia. A setup involving 16 synchronized sensors-including EEG, eye tracking, video cameras, smartwatches, and keystroke dynamics-was used to monitor learner activity during 30-minute sessions involving educational videos, document reading, and multiple-choice tests. Mobile phone usage events, including both controlled interventions and uncontrolled interactions, were labeled by supervisors and refined through a semi-supervised re-labeling process. Technical validation confirmed signal quality, and statistical analyses revealed biometric changes associated with phone usage. The dataset is publicly available for research through GitHub and Science Data Bank, with synchronized recordings from three platforms (edBB, edX, and LOGGE), provided in standard formats (.csv, .mp4, .wav, and .tsv), and accompanied by a detailed guide."
  },
  {
    "id": "http://arxiv.org/abs/2508.20955v1",
    "updated": "2025-08-28T16:17:19Z",
    "published": "2025-08-28T16:17:19Z",
    "title": "E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections",
    "authors": [
      "Fang Wang",
      "Huitao Li",
      "Wenhan Chao",
      "Zheng Zhuo",
      "Yiran Ji",
      "Chang Peng",
      "Yupeng Sun"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20955v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Many high-performance networks were not designed with lightweight application scenarios in mind from the outset, which has greatly restricted their scope of application. This paper takes ConvNeXt as the research object and significantly reduces the parameter scale and network complexity of ConvNeXt by integrating the Cross Stage Partial Connections mechanism and a series of optimized designs. The new network is named E-ConvNeXt, which can maintain high accuracy performance under different complexity configurations. The three core innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network (CSPNet) with ConvNeXt and adjusting the network structure, which reduces the model's network complexity by up to 80%; (2) Optimizing the Stem and Block structures to enhance the model's feature expression capability and operational efficiency; (3) Replacing Layer Scale with channel attention. Experimental validation on ImageNet classification demonstrates E-ConvNeXt's superior accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at 0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer learning tests on object detection tasks further confirm its generalization capability."
  },
  {
    "id": "http://arxiv.org/abs/2508.20954v1",
    "updated": "2025-08-28T16:16:40Z",
    "published": "2025-08-28T16:16:40Z",
    "title": "Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement",
    "authors": [
      "Amir Jmal",
      "Chaima Chtourou",
      "Mahdi Louati",
      "Abdelaziz Kallel",
      "Houda Khmila"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20954v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In the context of proven climate change, maintaining olive biodiversity through early anomaly detection and treatment using remote sensing technology is crucial, offering effective management solutions. This paper presents an innovative approach to olive tree segmentation from satellite images. By leveraging foundational models and advanced segmentation techniques, the study integrates the Segment Anything Model (SAM) to accurately identify and segment olive trees in agricultural plots. The methodology includes SAM segmentation and corrections based on trees alignement in the field and a learanble constraint about the shape and the size. Our approach achieved a 98\\% accuracy rate, significantly surpassing the initial SAM performance of 82\\%."
  },
  {
    "id": "http://arxiv.org/abs/2508.20920v1",
    "updated": "2025-08-28T15:50:29Z",
    "published": "2025-08-28T15:50:29Z",
    "title": "COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans",
    "authors": [
      "Enrico Martini",
      "Ho Jin Choi",
      "Nadia Figueroa",
      "Nicola Bombieri"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20920v1.pdf",
    "comment": "Submitted to Information Fusion",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH."
  },
  {
    "id": "http://arxiv.org/abs/2508.20919v1",
    "updated": "2025-08-28T15:50:27Z",
    "published": "2025-08-28T15:50:27Z",
    "title": "Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement",
    "authors": [
      "Sara Krauss",
      "Ellena Spieß",
      "Daniel Hieber",
      "Frank Kramer",
      "Johannes Schobel",
      "Dominik Müller"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20919v1.pdf",
    "comment": "Submission as part of the MICCAI MIDOG25 challenge",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Mitotic figures (MFs) are relevant biomarkers in tumor grading. Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult, as manual annotation is time-consuming and subjective. In this work an ensemble of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it reduced sensitivity and overall performance. The results show that deep ensembles perform well for AMF classification. RBR can increase specific metrics but requires further research."
  },
  {
    "id": "http://arxiv.org/abs/2508.11803v3",
    "updated": "2025-08-28T15:47:29Z",
    "published": "2025-08-15T21:18:23Z",
    "title": "An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation",
    "authors": [
      "Azam Nouri"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11803v3.pdf",
    "comment": "5 pages, No figure",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This study investigates whether second-order geometric cues - planar curvature magnitude, curvature sign, and gradient orientation - are sufficient on their own to drive a multilayer perceptron (MLP) classifier for handwritten character recognition (HCR), offering an alternative to convolutional neural networks (CNNs). Using these three handcrafted feature maps as inputs, our curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89 percent on EMNIST letters. These results underscore the discriminative power of curvature-based representations for handwritten character images and demonstrate that the advantages of deep learning can be realized even with interpretable, hand-engineered features."
  },
  {
    "id": "http://arxiv.org/abs/2508.11902v3",
    "updated": "2025-08-28T15:44:00Z",
    "published": "2025-08-16T04:17:39Z",
    "title": "A Sobel-Gradient MLP Baseline for Handwritten Character Recognition",
    "authors": [
      "Azam Nouri"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11902v3.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We revisit the classical Sobel operator to ask a simple question: Are first-order edge maps sufficient to drive an all-dense multilayer perceptron (MLP) for handwritten character recognition (HCR), as an alternative to convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory footprint and transparent features. Our findings highlight that much of the class-discriminative information in handwritten character images is already captured by first-order gradients, making edge-aware MLPs a compelling option for HCR."
  },
  {
    "id": "http://arxiv.org/abs/2508.20909v1",
    "updated": "2025-08-28T15:38:50Z",
    "published": "2025-08-28T15:38:50Z",
    "title": "Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation",
    "authors": [
      "Yifan Gao",
      "Haoyue Li",
      "Feng Yuan",
      "Xiaosong Wang",
      "Xin Gao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20909v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Foundation models pre-trained on large-scale natural image datasets offer a powerful paradigm for medical image segmentation. However, effectively transferring their learned representations for precise clinical applications remains a challenge. In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. Our architecture introduces an encoder built upon a frozen DINOv3 backbone, which employs a specialized adapter to fuse the model's rich semantic features with low-level spatial details. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. We conducted extensive experiments on seven diverse public medical image segmentation datasets. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Our framework proves to be highly scalable, with segmentation accuracy consistently improving as the backbone model size increases up to the 7-billion-parameter variant. The findings demonstrate that leveraging the superior, dense-pretrained features from a general-purpose foundation model provides a highly effective and parameter-efficient approach to advance the accuracy of medical image segmentation. The code is available at https://github.com/yifangao112/DinoUNet."
  },
  {
    "id": "http://arxiv.org/abs/2508.20892v1",
    "updated": "2025-08-28T15:20:35Z",
    "published": "2025-08-28T15:20:35Z",
    "title": "To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software",
    "authors": [
      "Loïc Stratil",
      "Felix Fent",
      "Esteban Rivera",
      "Markus Lienkamp"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20892v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Autonomous vehicle perception typically relies on modular pipelines that decompose the task into detection, tracking, and prediction. While interpretable, these pipelines suffer from error accumulation and limited inter-task synergy. Unified perception has emerged as a promising paradigm that integrates these sub-tasks within a shared architecture, potentially improving robustness, contextual reasoning, and efficiency while retaining interpretable outputs. In this survey, we provide a comprehensive overview of unified perception, introducing a holistic and systemic taxonomy that categorizes methods along task integration, tracking formulation, and representation flow. We define three paradigms -Early, Late, and Full Unified Perception- and systematically review existing methods, their architectures, training strategies, datasets used, and open-source availability, while highlighting future research directions. This work establishes the first comprehensive framework for understanding and advancing unified perception, consolidates fragmented efforts, and guides future research toward more robust, generalizable, and interpretable perception."
  },
  {
    "id": "http://arxiv.org/abs/2501.10977v3",
    "updated": "2025-08-28T15:13:50Z",
    "published": "2025-01-19T07:53:39Z",
    "title": "SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-Learning in Virtual Reality",
    "authors": [
      "Roberto Daza",
      "Lin Shengkai",
      "Aythami Morales",
      "Julian Fierrez",
      "Katashi Nagao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2501.10977v3.pdf",
    "comment": "10 pages, 3 figures. Published in ACM Intl. Conf. on Multimedia Workshops (ACM MM Workshops 2025, I2M-MM 25). Also presented at IEEE/CVF Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW) and AAAI Workshop on Artificial Intelligence for Education (AI4EDU)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This work introduces SMARTe-VR, a platform for student monitoring in an immersive virtual reality environment designed for online education. SMARTe-VR aims to collect data for adaptive learning, focusing on facial biometrics and learning metadata. The platform allows instructors to create customized learning sessions with video lectures, featuring an interface with an AutoQA system to evaluate understanding, interaction tools (for example, textbook highlighting and lecture tagging), and real-time feedback. Furthermore, we released a dataset that contains 5 research challenges with data from 10 users in VR-based TOEIC sessions. This data set, which spans more than 25 hours, includes facial features, learning metadata, 450 responses, difficulty levels of the questions, concept tags, and understanding labels. Alongside the database, we present preliminary experiments using Item Response Theory models, adapted for understanding detection using facial features. Two architectures were explored: a Temporal Convolutional Network for local features and a Multilayer Perceptron for global features."
  },
  {
    "id": "http://arxiv.org/abs/2508.20881v1",
    "updated": "2025-08-28T15:11:49Z",
    "published": "2025-08-28T15:11:49Z",
    "title": "Understanding and evaluating computer vision models through the lens of counterfactuals",
    "authors": [
      "Pushkar Shukla"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20881v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems. The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts. The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals. Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation."
  },
  {
    "id": "http://arxiv.org/abs/2508.20877v1",
    "updated": "2025-08-28T15:07:04Z",
    "published": "2025-08-28T15:07:04Z",
    "title": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis",
    "authors": [
      "Dennis Slobodzian",
      "Karissa Tilbury",
      "Amir Kordijazi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20877v1.pdf",
    "comment": "21 pages, 17 figure",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms of cancer, with a five-year survival rate below 10% primarily due to late detection. This research develops and validates a deep learning framework for early PDAC detection through analysis of dual-modality imaging: autofluorescence and second harmonic generation (SHG). We analyzed 40 unique patient samples to create a specialized neural network capable of distinguishing between normal, fibrotic, and cancerous tissue. Our methodology evaluated six distinct deep learning architectures, comparing traditional Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs). Through systematic experimentation, we identified and overcome significant challenges in medical image analysis, including limited dataset size and class imbalance. The final optimized framework, based on a modified ResNet architecture with frozen pre-trained layers and class-weighted training, achieved over 90% accuracy in cancer detection. This represents a significant improvement over current manual analysis methods an demonstrates potential for clinical deployment. This work establishes a robust pipeline for automated PDAC detection that can augment pathologists' capabilities while providing a foundation for future expansion to other cancer types. The developed methodology also offers valuable insights for applying deep learning to limited-size medical imaging datasets, a common challenge in clinical applications."
  },
  {
    "id": "http://arxiv.org/abs/2503.11519v3",
    "updated": "2025-08-28T14:55:38Z",
    "published": "2025-03-14T15:42:42Z",
    "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models",
    "authors": [
      "Hao Cheng",
      "Erjia Xiao",
      "Yichi Wang",
      "Lingfeng Zhang",
      "Qiang Zhang",
      "Jiahang Cao",
      "Kaidi Xu",
      "Mengshu Sun",
      "Xiaoshuai Hao",
      "Jindong Gu",
      "Renjing Xu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.11519v3.pdf",
    "comment": "This paper is accepted by IJCAI2025 Workshop on Deepfake Detection, Localization, and Interpretability",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Current Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-Vision tasks, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have attracted significant attention. Large Vision Language Models (LVLMs) and I2I Generation Models (GMs) are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to produce disruptive outputs that are semantically aligned with those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of cross-vision tasks. However, the specific characteristics of the threats posed by visual prompts remain underexplored. In this paper, to comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we propose the Typographic Visual Prompts Injection Dataset and thoroughly evaluate the TVPI security risks on various open-source and closed-source LVLMs and I2I GMs under visual prompts with different target semantics, deepening the understanding of TVPI threats."
  },
  {
    "id": "http://arxiv.org/abs/2508.20851v1",
    "updated": "2025-08-28T14:46:24Z",
    "published": "2025-08-28T14:46:24Z",
    "title": "PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis",
    "authors": [
      "Ye Zhang",
      "Yu Zhou",
      "Jingwen Qi",
      "Yongbing Zhang",
      "Simon Puettmann",
      "Finn Wichmann",
      "Larissa Pereira Ferreira",
      "Lara Sichward",
      "Julius Keyl",
      "Sylvia Hartmann",
      "Shuo Zhao",
      "Hongxiao Wang",
      "Xiaowei Xu",
      "Jianxu Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20851v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Deep learning based automated pathological diagnosis has markedly improved diagnostic efficiency and reduced variability between observers, yet its clinical adoption remains limited by opaque model decisions and a lack of traceable rationale. To address this, recent multimodal visual reasoning architectures provide a unified framework that generates segmentation masks at the pixel level alongside semantically aligned textual explanations. By localizing lesion regions and producing expert style diagnostic narratives, these models deliver the transparent and interpretable insights necessary for dependable AI assisted pathology. Building on these advancements, we propose PathMR, a cell-level Multimodal visual Reasoning framework for Pathological image analysis. Given a pathological image and a textual query, PathMR generates expert-level diagnostic explanations while simultaneously predicting cell distribution patterns. To benchmark its performance, we evaluated our approach on the publicly available PathGen dataset as well as on our newly developed GADVR dataset. Extensive experiments on these two datasets demonstrate that PathMR consistently outperforms state-of-the-art visual reasoning methods in text generation quality, segmentation accuracy, and cross-modal alignment. These results highlight the potential of PathMR for improving interpretability in AI-driven pathological diagnosis. The code will be publicly available in https://github.com/zhangye-zoe/PathMR."
  },
  {
    "id": "http://arxiv.org/abs/2503.12232v2",
    "updated": "2025-08-28T14:32:17Z",
    "published": "2025-03-15T18:56:29Z",
    "title": "L2RW+: A Comprehensive Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification",
    "authors": [
      "Yan Jiang",
      "Hao Yu",
      "Mengting Wei",
      "Zhaodong Sun",
      "Haoyu Chen",
      "Xu Cheng",
      "Guoying Zhao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.12232v2.pdf",
    "comment": "Extended Version of L2RW. We extend it from image to video data",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Visible-infrared person re-identification (VI-ReID) is a challenging task that aims to match pedestrian images captured under varying lighting conditions, which has drawn intensive research attention and achieved promising results. However, existing methods adopt the centralized training, ignoring the potential privacy concerns as the data is distributed across multiple devices or entities in reality. In this paper, we propose L2RW+, a benchmark that brings VI-ReID closer to real-world applications. The core rationale behind L2RW+ is that incorporating decentralized training into VI-ReID can address privacy concerns in scenarios with limited data-sharing constrains. Specifically, we design protocols and corresponding algorithms for different privacy sensitivity levels. In our new benchmark, we simulate the training under real-world data conditions that: 1) data from each camera is completely isolated, or 2) different data entities (e.g., data controllers of a certain region) can selectively share the data. In this way, we simulate scenarios with strict privacy restrictions, which is closer to real-world conditions. Comprehensive experiments show the feasibility and potential of decentralized VI-ReID training at both image and video levels. In particular, with increasing data scales, the performance gap between decentralized and centralized training decreases, especially in video-level VI-ReID. In unseen domains, decentralized training even achieves performance comparable to SOTA centralized methods. This work offers a novel research entry for deploying VI-ReID into real-world scenarios and can benefit the community. Code is available at: https://github.com/Joey623/L2RW."
  },
  {
    "id": "http://arxiv.org/abs/2508.20835v1",
    "updated": "2025-08-28T14:28:33Z",
    "published": "2025-08-28T14:28:33Z",
    "title": "PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification",
    "authors": [
      "Hao Yang",
      "Qianyu Zhou",
      "Haijia Sun",
      "Xiangtai Li",
      "Xuequan Lu",
      "Lizhuang Ma",
      "Shuicheng Yan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20835v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Domain Generalization (DG) has been recently explored to enhance the generalizability of Point Cloud Classification (PCC) models toward unseen domains. Prior works are based on convolutional networks, Transformer or Mamba architectures, either suffering from limited receptive fields or high computational cost, or insufficient long-range dependency modeling. RWKV, as an emerging architecture, possesses superior linear complexity, global receptive fields, and long-range dependency. In this paper, we present the first work that studies the generalizability of RWKV models in DG PCC. We find that directly applying RWKV to DG PCC encounters two significant challenges: RWKV's fixed direction token shift methods, like Q-Shift, introduce spatial distortions when applied to unstructured point clouds, weakening local geometric modeling and reducing robustness. In addition, the Bi-WKV attention in RWKV amplifies slight cross-domain differences in key distributions through exponential weighting, leading to attention shifts and degraded generalization. To this end, we propose PointDGRWKV, the first RWKV-based framework tailored for DG PCC. It introduces two key modules to enhance spatial modeling and cross-domain robustness, while maintaining RWKV's linear efficiency. In particular, we present Adaptive Geometric Token Shift to model local neighborhood structures to improve geometric context awareness. In addition, Cross-Domain key feature Distribution Alignment is designed to mitigate attention drift by aligning key feature distributions across domains. Extensive experiments on multiple benchmarks demonstrate that PointDGRWKV achieves state-of-the-art performance on DG PCC."
  },
  {
    "id": "http://arxiv.org/abs/2508.20830v1",
    "updated": "2025-08-28T14:25:32Z",
    "published": "2025-08-28T14:25:32Z",
    "title": "Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation",
    "authors": [
      "Krit Duangprom",
      "Tryphon Lambrou",
      "Binod Bhattarai"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20830v1.pdf",
    "comment": "Accepted to MICCAI 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This paper presents a novel pipeline for 2D keypoint estima- tion of surgical tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network (CNN) or Transformer-based approaches, which often suffer from overfitting in small-scale medical datasets, our method harnesses the generalization capabilities of pre-trained VLMs. We carefully design prompts to create an instruction-tuning dataset and use them to align visual features with semantic keypoint descriptions. Experimental results show that with only two epochs of fine tuning, the adapted VLM outperforms the baseline models, demonstrating the ef- fectiveness of LoRA in low-resource scenarios. This approach not only improves keypoint detection performance, but also paves the way for future work in 3D surgical hands and tools pose estimation."
  },
  {
    "id": "http://arxiv.org/abs/2508.19542v2",
    "updated": "2025-08-28T14:22:38Z",
    "published": "2025-08-27T03:29:35Z",
    "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning",
    "authors": [
      "Nannan Zhu",
      "Yonghao Dong",
      "Teng Wang",
      "Xueqian Li",
      "Shengjun Deng",
      "Yijia Wang",
      "Zheng Hong",
      "Tiantian Geng",
      "Guo Niu",
      "Hanyan Huang",
      "Xiongfei Yao",
      "Shuaiwei Jiao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19542v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation MLLMs. The data and evaluation code are available at https://github.com/Hokhim2/CVBench."
  },
  {
    "id": "http://arxiv.org/abs/2503.08751v2",
    "updated": "2025-08-28T14:20:08Z",
    "published": "2025-03-11T13:50:22Z",
    "title": "Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning",
    "authors": [
      "Qi Wang",
      "Zhipeng Zhang",
      "Baao Xie",
      "Xin Jin",
      "Yunbo Wang",
      "Shiyu Wang",
      "Liaomo Zheng",
      "Xiaokang Yang",
      "Wenjun Zeng"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.08751v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, $\\textit{i.e.,}$ RL agents suffer from low sample efficiency in environments with variations. While various approaches have attempted to alleviate this issue by disentangled representation learning, these methods usually start learning from scratch without prior knowledge of the world. This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints. To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM). Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos. The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation. For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model. During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning. Experimental results validate the superiority of our approach on various benchmarks."
  },
  {
    "id": "http://arxiv.org/abs/2508.20817v1",
    "updated": "2025-08-28T14:15:18Z",
    "published": "2025-08-28T14:15:18Z",
    "title": "FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning",
    "authors": [
      "He Li",
      "Xinyu Liu",
      "Weihang Kong",
      "Xingchen Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20817v1.pdf",
    "comment": "11 pages, 9 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Most visible and infrared image fusion (VIF) methods focus primarily on optimizing fused image quality. Recent studies have begun incorporating downstream tasks, such as semantic segmentation and object detection, to provide semantic guidance for VIF. However, semantic segmentation requires extensive annotations, while object detection, despite reducing annotation efforts compared with segmentation, faces challenges in highly crowded scenes due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd counting has gained increasing attention in recent years, no studies have integrated VIF and crowd counting into a unified framework. To address these challenges, we propose FusionCounting, a novel multi-task learning framework that integrates crowd counting into the VIF process. Crowd counting provides a direct quantitative measure of population density with minimal annotation, making it particularly suitable for dense scenes. Our framework leverages both input images and population density information in a mutually beneficial multi-task design. To accelerate convergence and balance tasks contributions, we introduce a dynamic loss function weighting strategy. Furthermore, we incorporate adversarial training to enhance the robustness of both VIF and crowd counting, improving the model's stability and resilience to adversarial attacks. Experimental results on public datasets demonstrate that FusionCounting not only enhances image fusion quality but also achieves superior crowd counting performance."
  },
  {
    "id": "http://arxiv.org/abs/2508.20813v1",
    "updated": "2025-08-28T14:13:26Z",
    "published": "2025-08-28T14:13:26Z",
    "title": "Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training",
    "authors": [
      "Tao Luo",
      "Han Wu",
      "Tong Yang",
      "Dinggang Shen",
      "Zhiming Cui"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20813v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate dental caries detection from panoramic X-rays plays a pivotal role in preventing lesion progression. However, current detection methods often yield suboptimal accuracy due to subtle contrast variations and diverse lesion morphology of dental caries. In this work, inspired by the clinical workflow where dentists systematically combine whole-image screening with detailed tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training network for accurate dental caries detection. Our DVCTNet starts with employing automated tooth detection to establish two complementary views: a global view from panoramic X-ray images and a local view from cropped tooth images. We then pretrain two vision foundation models separately on the two views. The global-view foundation model serves as the detection backbone, generating region proposals and global features, while the local-view model extracts detailed features from corresponding cropped tooth patches matched by the region proposals. To effectively integrate information from both views, we introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically fuses dual-view features, enhancing the detection pipeline by integrating the fused features back into the detection model for final caries detection. To rigorously evaluate our DVCTNet, we test it on a public dataset and further validate its performance on a newly curated, high-precision dental caries detection dataset, annotated using both intra-oral images and panoramic X-rays for double verification. Experimental results demonstrate DVCTNet's superior performance against existing state-of-the-art (SOTA) methods on both datasets, indicating the clinical applicability of our method. Our code and labeled dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet."
  },
  {
    "id": "http://arxiv.org/abs/2410.01262v4",
    "updated": "2025-08-28T14:03:26Z",
    "published": "2024-10-02T06:16:06Z",
    "title": "Improving Fine-Grained Control via Aggregation of Multiple Diffusion Models",
    "authors": [
      "Conghan Yue",
      "Zhengwei Peng",
      "Shiyan Du",
      "Zhi Ji",
      "Chuangjian Cai",
      "Le Wan",
      "Dongyu Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2410.01262v4.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "While many diffusion models perform well when controlling particular aspects such as style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper introduces a novel training-free algorithm, independent of denoising network architectures, for fine-grained generation, called Aggregation of Multiple Diffusion Models (AMDM). The algorithm integrates features from multiple diffusion models into a specified model to activate particular features and enable fine-grained control. Experimental results demonstrate that AMDM significantly improves fine-grained control without training, validating its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional generation in diffusion models. Specifically, it allows us to fully utilize existing or develop new conditional diffusion models that control specific aspects, and then aggregate them using the AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs. Code is available at: https://github.com/Hammour-steak/AMDM."
  },
  {
    "id": "http://arxiv.org/abs/2505.07817v2",
    "updated": "2025-08-28T13:58:02Z",
    "published": "2025-05-12T17:59:32Z",
    "title": "Pixel Motion as Universal Representation for Robot Control",
    "authors": [
      "Kanchana Ranasinghe",
      "Xiang Li",
      "E-Ro Nguyen",
      "Cristina Mata",
      "Jongwoo Park",
      "Michael S Ryoo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.07817v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo"
  },
  {
    "id": "http://arxiv.org/abs/2508.20789v1",
    "updated": "2025-08-28T13:53:44Z",
    "published": "2025-08-28T13:53:44Z",
    "title": "Surfel-based 3D Registration with Equivariant SE(3) Features",
    "authors": [
      "Xueyang Kang",
      "Hang Zhao",
      "Kourosh Khoshelham",
      "Patrick Vandewalle"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20789v1.pdf",
    "comment": "5 pages, 4 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Point cloud registration is crucial for ensuring 3D alignment consistency of multiple local point clouds in 3D reconstruction for remote sensing or digital heritage. While various point cloud-based registration methods exist, both non-learning and learning-based, they ignore point orientations and point uncertainties, making the model susceptible to noisy input and aggressive rotations of the input point cloud like orthogonal transformation; thus, it necessitates extensive training point clouds with transformation augmentations. To address these issues, we propose a novel surfel-based pose learning regression approach. Our method can initialize surfels from Lidar point cloud using virtual perspective camera parameters, and learns explicit $\\mathbf{SE(3)}$ equivariant features, including both position and rotation through $\\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative transformation between source and target scans. The model comprises an equivariant convolutional encoder, a cross-attention mechanism for similarity computation, a fully-connected decoder, and a non-linear Huber loss. Experimental results on indoor and outdoor datasets demonstrate our model superiority and robust performance on real point-cloud scans compared to state-of-the-art methods."
  },
  {
    "id": "http://arxiv.org/abs/2508.20783v1",
    "updated": "2025-08-28T13:45:04Z",
    "published": "2025-08-28T13:45:04Z",
    "title": "Evaluating Compositional Generalisation in VLMs and Diffusion Models",
    "authors": [
      "Beth Pearson",
      "Bilal Boulbarss",
      "Michael Wray",
      "Martha Lewis"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20783v1.pdf",
    "comment": "11 pages including references, 6 figures. Accepted at IWCS 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "A fundamental aspect of the semantics of natural language is that novel meanings can be formed from the composition of previously known parts. Vision-language models (VLMs) have made significant progress in recent years, however, there is evidence that they are unable to perform this kind of composition. For example, given an image of a red cube and a blue cylinder, a VLM such as CLIP is likely to incorrectly label the image as a red cylinder or a blue cube, indicating it represents the image as a `bag-of-words' and fails to capture compositional semantics. Diffusion models have recently gained significant attention for their impressive generative abilities, and zero-shot classifiers based on diffusion models have been shown to perform competitively with CLIP in certain compositional tasks. In this work we explore whether the generative Diffusion Classifier has improved compositional generalisation abilities compared to discriminative models. We assess three models -- Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with attributes and relations in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Our results show that the Diffusion Classifier and ViLT perform well at concept binding tasks, but that all models struggle significantly with the relational GZSL task, underscoring the broader challenges VLMs face with relational reasoning. Analysis of CLIP embeddings suggests that the difficulty may stem from overly similar representations of relational concepts such as left and right. Code and dataset are available at: https://github.com/otmive/diffusion_classifier_clip"
  },
  {
    "id": "http://arxiv.org/abs/2308.09388v2",
    "updated": "2025-08-28T13:33:09Z",
    "published": "2023-08-18T08:40:38Z",
    "title": "Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey",
    "authors": [
      "Xin Li",
      "Yulin Ren",
      "Xin Jin",
      "Cuiling Lan",
      "Xingrui Wang",
      "Wenjun Zeng",
      "Xinchao Wang",
      "Zhibo Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2308.09388v2.pdf",
    "comment": "Accepted by IJCV",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question, \"whether diffusion model can boost image restoration\". To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods. Despite that, a comprehensive and enlightening survey on diffusion model-based image restoration remains scarce. In this paper, we are the first to present a comprehensive review of recent diffusion model-based methods on image restoration, encompassing the learning paradigm, conditional strategy, framework design, modeling strategy, and evaluation. Concretely, we first introduce the background of the diffusion model briefly and then present two prevalent workflows that exploit diffusion models in image restoration. Subsequently, we classify and emphasize the innovative designs using diffusion models for both IR and blind/real-world IR, intending to inspire future development. To evaluate existing methods thoroughly, we summarize the commonly-used dataset, implementation details, and evaluation metrics. Additionally, we present the objective comparison for open-sourced methods across three tasks, including image super-resolution, deblurring, and inpainting. Ultimately, informed by the limitations in existing works, we propose five potential and challenging directions for the future research of diffusion model-based IR, including sampling efficiency, model compression, distortion simulation and estimation, distortion invariant learning, and framework design."
  },
  {
    "id": "http://arxiv.org/abs/2508.20776v1",
    "updated": "2025-08-28T13:32:35Z",
    "published": "2025-08-28T13:32:35Z",
    "title": "Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML",
    "authors": [
      "Kuniko Paxton",
      "Koorosh Aslansefat",
      "Amila Akagić",
      "Dhavalkumar Thakker",
      "Yiannis Papadopoulos"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20776v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advancements in skin lesion classification models have significantly improved accuracy, with some models even surpassing dermatologists' diagnostic performance. However, in medical practice, distrust in AI models remains a challenge. Beyond high accuracy, trustworthy, explainable diagnoses are essential. Existing explainability methods have reliability issues, with LIME-based methods suffering from inconsistency, while CAM-based methods failing to consider all classes. To address these limitations, we propose Global Class Activation Probabilistic Map Evaluation, a method that analyses all classes' activation probability maps probabilistically and at a pixel level. By visualizing the diagnostic process in a unified manner, it helps reduce the risk of misdiagnosis. Furthermore, the application of SafeML enhances the detection of false diagnoses and issues warnings to doctors and patients as needed, improving diagnostic reliability and ultimately patient safety. We evaluated our method using the ISIC datasets with MobileNetV2 and Vision Transformers."
  },
  {
    "id": "http://arxiv.org/abs/2508.20773v1",
    "updated": "2025-08-28T13:29:21Z",
    "published": "2025-08-28T13:29:21Z",
    "title": "Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI",
    "authors": [
      "Christoforos N. Spartalis",
      "Theodoros Semertzidis",
      "Petros Daras",
      "Efstratios Gavves"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20773v1.pdf",
    "comment": "ICML 2025 workshop on Machine Unlearning for Generative AI",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods."
  },
  {
    "id": "http://arxiv.org/abs/2508.20765v1",
    "updated": "2025-08-28T13:19:49Z",
    "published": "2025-08-28T13:19:49Z",
    "title": "Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding",
    "authors": [
      "Gowreesh Mago",
      "Pascal Mettes",
      "Stevan Rudinac"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20765v1.pdf",
    "comment": "Under Review for IJCV",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The automatic understanding of video content is advancing rapidly. Empowered by deeper neural networks and large datasets, machines are increasingly capable of understanding what is concretely visible in video frames, whether it be objects, actions, events, or scenes. In comparison, humans retain a unique ability to also look beyond concrete entities and recognize abstract concepts like justice, freedom, and togetherness. Abstract concept recognition forms a crucial open challenge in video understanding, where reasoning on multiple semantic levels based on contextual information is key. In this paper, we argue that the recent advances in foundation models make for an ideal setting to address abstract concept understanding in videos. Automated understanding of high-level abstract concepts is imperative as it enables models to be more aligned with human reasoning and values. In this survey, we study different tasks and datasets used to understand abstract concepts in video content. We observe that, periodically and over a long period, researchers have attempted to solve these tasks, making the best use of the tools available at their disposal. We advocate that drawing on decades of community experience will help us shed light on this important open grand challenge and avoid ``re-inventing the wheel'' as we start revisiting it in the era of multi-modal foundation models."
  },
  {
    "id": "http://arxiv.org/abs/2508.20762v1",
    "updated": "2025-08-28T13:17:35Z",
    "published": "2025-08-28T13:17:35Z",
    "title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer",
    "authors": [
      "Fachri Najm Noer Kartiman",
      " Rasim",
      "Yaya Wihardi",
      "Nurul Hasanah",
      "Oskar Natan",
      "Bambang Wahono",
      "Taufik Ibnu Salim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20762v1.pdf",
    "comment": "keywords-multitask learning, autonomous driving, end-to-end learning, skip connections, swin transformer, self-attention mechanism. 12 pages",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Focusing on the development of an end-to-end autonomous vehicle model with pixel-to-pixel context awareness, this research proposes the SKGE-Swin architecture. This architecture utilizes the Swin Transformer with a skip-stage mechanism to broaden feature representation globally and at various network levels. This approach enables the model to extract information from distant pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head Self-Attention (SW-MSA) mechanism and to retain critical information from the initial to the final stages of feature extraction, thereby enhancing its capability to comprehend complex patterns in the vehicle's surroundings. The model is evaluated on the CARLA platform using adversarial scenarios to simulate real-world conditions. Experimental results demonstrate that the SKGE-Swin architecture achieves a superior Driving Score compared to previous methods. Furthermore, an ablation study will be conducted to evaluate the contribution of each architectural component, including the influence of skip connections and the use of the Swin Transformer, in improving model performance."
  },
  {
    "id": "http://arxiv.org/abs/2508.20760v1",
    "updated": "2025-08-28T13:16:55Z",
    "published": "2025-08-28T13:16:55Z",
    "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
    "authors": [
      "Jan Erik van Woerden",
      "Gertjan Burghouts",
      "Lotte Nijskens",
      "Alma M. Liezenga",
      "Sabina van Rooij",
      "Frank Ruis",
      "Hugo J. Kuijf"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20760v1.pdf",
    "comment": "To be presented at SPIE: Sensors + Imaging, Artificial Intelligence for Security and Defence Applications II",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Vision-language models (VLMs) like CLIP enable zero-shot classification by aligning images and text in a shared embedding space, offering advantages for defense applications with scarce labeled data. However, CLIP's robustness in challenging military environments, with partial occlusion and degraded signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP variants' robustness to occlusion using a custom dataset of 18 military vehicle classes and evaluate using Normalized Area Under the Curve (NAUC) across occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP models consistently outperform CNNs, (2) fine-grained, dispersed occlusions degrade performance more than larger contiguous occlusions, (3) despite improved accuracy, performance of linear-probed models sharply drops at around 35% occlusion, (4) by finetuning the model's backbone, this performance drop occurs at more than 60% occlusion. These results underscore the importance of occlusion-specific augmentations during training and the need for further exploration into patch-level sensitivity and architectural resilience for real-world deployment of CLIP."
  },
  {
    "id": "http://arxiv.org/abs/2508.20758v1",
    "updated": "2025-08-28T13:15:37Z",
    "published": "2025-08-28T13:15:37Z",
    "title": "SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding",
    "authors": [
      "Jiawen Lin",
      "Shiran Bian",
      "Yihang Zhu",
      "Wenbin Tan",
      "Yachao Zhang",
      "Yuan Xie",
      "Yanyun Qu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20758v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using natural language descriptions. Although supervised methods achieve higher accuracy in constrained settings, zero-shot 3DVG holds greater promise for real-world applications since eliminating scene-specific training requirements. However, existing zero-shot methods face challenges of spatial-limited reasoning due to reliance on single-view localization, and contextual omissions or detail degradation. To address these issues, we propose SeqVLM, a novel zero-shot 3DVG framework that leverages multi-view real-world scene images with spatial information for target object reasoning. Specifically, SeqVLM first generates 3D instance proposals via a 3D semantic segmentation network and refines them through semantic filtering, retaining only semantic-relevant candidates. A proposal-guided multi-view projection strategy then projects these candidate proposals onto real scene image sequences, preserving spatial relationships and contextual details in the conversion process of 3D point cloud to images. Furthermore, to mitigate VLM computational overload, we implement a dynamic scheduling mechanism that iteratively processes sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to identify textually specified objects. Experiments on the ScanRefer and Nr3D benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%, respectively, which advance 3DVG toward greater generalization and real-world applicability. The code is available at https://github.com/JiawLin/SeqVLM."
  },
  {
    "id": "http://arxiv.org/abs/2508.20754v1",
    "updated": "2025-08-28T13:12:18Z",
    "published": "2025-08-28T13:12:18Z",
    "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting",
    "authors": [
      "Yuxi Hu",
      "Jun Zhang",
      "Kuangyi Chen",
      "Zhe Zhang",
      "Friedrich Fraundorfer"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20754v1.pdf",
    "comment": "Accepted to The 36th British Machine Vision Conference (BMVC 2025), Sheffield, UK",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS."
  },
  {
    "id": "http://arxiv.org/abs/2508.20751v1",
    "updated": "2025-08-28T13:11:24Z",
    "published": "2025-08-28T13:11:24Z",
    "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning",
    "authors": [
      "Yibin Wang",
      "Zhimin Li",
      "Yuhang Zang",
      "Yujie Zhou",
      "Jiazi Bu",
      "Chunyu Wang",
      "Qinglin Lu",
      "Cheng Jin",
      "Jiaqi Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20751v1.pdf",
    "comment": "Project Page: https://codegoat24.github.io/UnifiedReward/Pref-GRPO",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO."
  },
  {
    "id": "http://arxiv.org/abs/2508.20745v1",
    "updated": "2025-08-28T13:04:55Z",
    "published": "2025-08-28T13:04:55Z",
    "title": "Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification",
    "authors": [
      "Kaustubh Atey",
      "Sameer Anand Jha",
      "Gouranga Bala",
      "Amit Sethi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20745v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Atypical mitotic figures (AMFs) are important histopathological markers yet remain challenging to identify consistently, particularly under domain shift stemming from scanner, stain, and acquisition differences. We present a simple training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2. The approach (i) increases feature diversity via style perturbations inserted at early and mid backbone stages, (ii) aligns attention-refined features across sites using weak domain labels (Scanner, Origin, Species, Tumor) through an auxiliary alignment loss, and (iii) stabilizes predictions by distilling from an exponential moving average (EMA) teacher with temperature-scaled KL divergence. On the organizer-run preliminary leaderboard for atypical mitosis classification, our submission attains balanced accuracy of 0.8762, sensitivity of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs negligible inference-time overhead, relies only on coarse domain metadata, and delivers strong, balanced performance, positioning it as a competitive submission for the MIDOG 2025 challenge."
  },
  {
    "id": "http://arxiv.org/abs/2508.20734v1",
    "updated": "2025-08-28T12:58:14Z",
    "published": "2025-08-28T12:58:14Z",
    "title": "CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network",
    "authors": [
      "Reza Akbari Movahed",
      "Abuzar Rezaee",
      "Arezoo Zakeri",
      "Colin Berry",
      "Edmond S. L. Ho",
      "Ali Gooya"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20734v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions."
  },
  {
    "id": "http://arxiv.org/abs/2508.20709v1",
    "updated": "2025-08-28T12:27:23Z",
    "published": "2025-08-28T12:27:23Z",
    "title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network",
    "authors": [
      "Chenhao Zhang",
      "Wei Gao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20709v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Neural Video Compression (NVC) has achieved remarkable performance in recent years. However, precise rate control remains a challenge due to the inherent limitations of learning-based codecs. To solve this issue, we propose a dynamic video compression framework designed for variable bitrate scenarios. First, to achieve variable bitrate implementation, we propose the Dynamic-Route Autoencoder with variable coding routes, each occupying partial computational complexity of the whole network and navigating to a distinct RD trade-off. Second, to approach the target bitrate, the Rate Control Agent estimates the bitrate of each route and adjusts the coding route of DRA at run time. To encompass a broad spectrum of variable bitrates while preserving overall RD performance, we employ the Joint-Routes Optimization strategy, achieving collaborative training of various routes. Extensive experiments on the HEVC and UVG datasets show that the proposed method achieves an average BD-Rate reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods while maintaining an average bitrate error of 1.66%, achieving Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and bitrate-constrained applications. Our code is available at https://git.openi.org.cn/OpenAICoding/DynamicDVC."
  },
  {
    "id": "http://arxiv.org/abs/2412.08321v3",
    "updated": "2025-08-28T12:24:24Z",
    "published": "2024-12-11T11:57:05Z",
    "title": "TGOSPA Metric Parameters Selection and Evaluation for Visual Multi-object Tracking",
    "authors": [
      "Jan Krejčí",
      "Oliver Kost",
      "Ondřej Straka",
      "Yuxuan Xia",
      "Lennart Svensson",
      "Ángel F. García-Fernández"
    ],
    "pdf_url": "https://arxiv.org/pdf/2412.08321v3.pdf",
    "comment": "Submitted to Springer International Journal of Computer Vision",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Multi-object tracking algorithms are deployed in various applications, each with different performance requirements. For example, track switches pose significant challenges for offline scene understanding, as they hinder the accuracy of data interpretation. Conversely, in online surveillance applications, their impact is often minimal. This disparity underscores the need for application-specific performance evaluations that are both simple and mathematically sound. The trajectory generalized optimal sub-pattern assignment (TGOSPA) metric offers a principled approach to evaluate multi-object tracking performance. It accounts for localization errors, the number of missed and false objects, and the number of track switches, providing a comprehensive assessment framework. This paper illustrates the effective use of the TGOSPA metric in computer vision tasks, addressing challenges posed by the need for application-specific scoring methodologies. By exploring the TGOSPA parameter selection, we enable users to compare, comprehend, and optimize the performance of algorithms tailored for specific tasks, such as target tracking and training of detector or re-ID modules."
  },
  {
    "id": "http://arxiv.org/abs/2503.02549v2",
    "updated": "2025-08-28T12:01:39Z",
    "published": "2025-03-04T12:20:06Z",
    "title": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
    "authors": [
      "Grzegorz Skorupko",
      "Fotios Avgoustidis",
      "Carlos Martín-Isla",
      "Lidia Garrucho",
      "Dimitri A. Kessler",
      "Esmeralda Ruiz Pujadas",
      "Oliver Díaz",
      "Maciej Bobowicz",
      "Katarzyna Gwoździewicz",
      "Xavier Bargalló",
      "Paulius Jaruševičius",
      "Richard Osuala",
      "Kaisar Kushibar",
      "Karim Lekadir"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.02549v2.pdf",
    "comment": "In review",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The nnU-Net framework has played a crucial role in medical image segmentation and has become the gold standard in multitudes of applications targeting different diseases, organs, and modalities. However, so far it has been used primarily in a centralized approach where the collected data is stored in the same location where nnU-Net is trained. This centralized approach has various limitations, such as potential leakage of sensitive patient information and violation of patient privacy. Federated learning has emerged as a key approach for training segmentation models in a decentralized manner, enabling collaborative development while prioritising patient privacy. In this paper, we propose FednnU-Net, a plug-and-play, federated learning extension of the nnU-Net framework. To this end, we contribute two federated methodologies to unlock decentralized training of nnU-Net, namely, Federated Fingerprint Extraction (FFE) and Asymmetric Federated Averaging (AsymFedAvg). We conduct a comprehensive set of experiments demonstrating high and consistent performance of our methods for breast, cardiac and fetal segmentation based on a multi-modal collection of 6 datasets representing samples from 18 different institutions. To democratize research as well as real-world deployments of decentralized training in clinical centres, we publicly share our framework at https://github.com/faildeny/FednnUNet ."
  },
  {
    "id": "http://arxiv.org/abs/2508.20691v1",
    "updated": "2025-08-28T11:50:22Z",
    "published": "2025-08-28T11:50:22Z",
    "title": "MobileCLIP2: Improving Multi-Modal Reinforced Training",
    "authors": [
      "Fartash Faghri",
      "Pavan Kumar Anasosalu Vasu",
      "Cem Koc",
      "Vaishaal Shankar",
      "Alexander Toshev",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20691v1.pdf",
    "comment": "TMLR August 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\\times$ smaller and improves on DFN ViT-L/14 at 2.5$\\times$ lower latency. We release our pretrained models (https://github.com/apple/ml-mobileclip) and the data generation code (https://github.com/apple/ml-mobileclip-dr). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing."
  },
  {
    "id": "http://arxiv.org/abs/2406.16464v6",
    "updated": "2025-08-28T11:35:48Z",
    "published": "2024-06-24T09:13:42Z",
    "title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection",
    "authors": [
      "Junjie Chen",
      "Hang Yu",
      "Subin Huang",
      "Sanmin Liu",
      "Linfeng Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.16464v6.pdf",
    "comment": "ACM TOMM (Under Review); Code and data are available at https://github.com/CoderChen01/InterCLIP-MEP",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Sarcasm in social media, often expressed through text-image combinations, poses challenges for sentiment analysis and intention mining. Current multi-modal sarcasm detection methods have been demonstrated to overly rely on spurious cues within the textual modality, revealing a limited ability to genuinely identify sarcasm through nuanced text-image interactions. To solve this problem, we propose InterCLIP-MEP, which introduces Interactive CLIP (InterCLIP) with an efficient training strategy to extract enriched text-image representations by embedding cross-modal information directly into each encoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a dynamic dual-channel memory that stores valuable test sample knowledge during inference, acting as a non-parametric classifier for robust sarcasm recognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP achieves state-of-the-art performance, with significant accuracy and F1 score improvements on MMSD and MMSD2.0. Our code is available at https://github.com/CoderChen01/InterCLIP-MEP."
  },
  {
    "id": "http://arxiv.org/abs/2508.20670v1",
    "updated": "2025-08-28T11:22:15Z",
    "published": "2025-08-28T11:22:15Z",
    "title": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware Synthetic Image Detection",
    "authors": [
      "Anastasios Skoularikis",
      "Stefanos-Iordanis Papadopoulos",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20670v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 \"in the wild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to \"in the wild\" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures."
  },
  {
    "id": "http://arxiv.org/abs/2508.11716v2",
    "updated": "2025-08-28T11:05:03Z",
    "published": "2025-08-14T17:30:36Z",
    "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Algorithms (FakeIDet2)",
    "authors": [
      "Javier Muñoz-Haro",
      "Ruben Tolosana",
      "Julian Fierrez",
      "Ruben Vera-Rodriguez",
      "Aythami Morales"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11716v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Remote user verification in Internet-based applications is becoming increasingly important nowadays. A popular scenario for it consists of submitting a picture of the user's Identity Document (ID) to a service platform, authenticating its veracity, and then granting access to the requested digital service. An ID is well-suited to verify the identity of an individual, since it is government issued, unique, and nontransferable. However, with recent advances in Artificial Intelligence (AI), attackers can surpass security measures in IDs and create very realistic physical and synthetic fake IDs. Researchers are now trying to develop methods to detect an ever-growing number of these AI-based fakes that are almost indistinguishable from authentic (bona fide) IDs. In this counterattack effort, researchers are faced with an important challenge: the difficulty in using real data to train fake ID detectors. This real data scarcity for research and development is originated by the sensitive nature of these documents, which are usually kept private by the ID owners (the users) and the ID Holders (e.g., government, police, bank, etc.). The main contributions of our study are: 1) We propose and discuss a patch-based methodology to preserve privacy in fake ID detection research. 2) We provide a new public database, FakeIDet2-db, comprising over 900K real/fake ID patches extracted from 2,000 ID images, acquired using different smartphone sensors, illumination and height conditions, etc. In addition, three physical attacks are considered: print, screen, and composite. 3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We release a standard reproducible benchmark that considers physical and synthetic attacks from popular databases in the literature."
  },
  {
    "id": "http://arxiv.org/abs/2409.19573v2",
    "updated": "2025-08-28T11:02:53Z",
    "published": "2024-09-29T06:21:05Z",
    "title": "See then Tell: Enhancing Key Information Extraction with Vision Grounding",
    "authors": [
      "Shuhang Liu",
      "Zhenrong Zhang",
      "Pengfei Hu",
      "Jiefeng Ma",
      "Jun Du",
      "Qing Wang",
      "Jianshu Zhang",
      "Chenyu Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2409.19573v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In the digital era, the ability to understand visually rich documents that integrate text, complex layouts, and imagery is critical. Traditional Key Information Extraction (KIE) methods primarily rely on Optical Character Recognition (OCR), which often introduces significant latency, computational overhead, and errors. Current advanced image-to-text approaches, which bypass OCR, typically yield plain text outputs without corresponding vision grounding. In this paper, we introduce STNet (See then Tell Net), a novel end-to-end model designed to deliver precise answers with relevant vision grounding. Distinctively, STNet utilizes a unique <see> token to observe pertinent image areas, aided by a decoder that interprets physical coordinates linked to this token. Positioned at the outset of the answer text, the <see> token allows the model to first see-observing the regions of the image related to the input question-and then tell-providing articulated textual responses. To enhance the model's seeing capabilities, we collect extensive structured table recognition datasets. Leveraging the advanced text processing prowess of GPT-4, we develop the TVG (TableQA with Vision Grounding) dataset, which not only provides text-based Question Answering (QA) pairs but also incorporates precise vision grounding for these pairs. Our approach demonstrates substantial advancements in KIE performance, achieving state-of-the-art results on publicly available datasets such as CORD, SROIE, and DocVQA. The code will also be made publicly available."
  },
  {
    "id": "http://arxiv.org/abs/2508.20655v1",
    "updated": "2025-08-28T11:01:33Z",
    "published": "2025-08-28T11:01:33Z",
    "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
    "authors": [
      "Sihan Yang",
      "Chenhang Cui",
      "Zihao Zhao",
      "Yiyang Zhou",
      "Weilong Yan",
      "Ying Wei",
      "Huaxiu Yao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20655v1.pdf",
    "comment": "EMNLP 2025 Findings",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations--where generated outputs are not grounded in the visual input--and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs."
  },
  {
    "id": "http://arxiv.org/abs/2503.19065v2",
    "updated": "2025-08-28T10:56:18Z",
    "published": "2025-03-24T18:51:55Z",
    "title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation",
    "authors": [
      "Zhongyu Yang",
      "Jun Chen",
      "Dannong Xu",
      "Junjie Fei",
      "Xiaoqian Shen",
      "Liangbing Zhao",
      "Chun-Mei Feng",
      "Mohamed Elhoseiny"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.19065v2.pdf",
    "comment": "ICCV 2025, Project in https://wikiautogen.github.io/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. Our code and examples are available at https://wikiautogen.github.io/ ."
  },
  {
    "id": "http://arxiv.org/abs/2508.20640v1",
    "updated": "2025-08-28T10:38:13Z",
    "published": "2025-08-28T10:38:13Z",
    "title": "CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models",
    "authors": [
      "Ayan Banerjee",
      "Fernando Vilariño",
      "Josep Lladós"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20640v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the \"style-first, identity-after\" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications."
  },
  {
    "id": "http://arxiv.org/abs/2508.18984v2",
    "updated": "2025-08-28T10:31:44Z",
    "published": "2025-08-26T12:32:55Z",
    "title": "Enhancing Document VQA Models via Retrieval-Augmented Generation",
    "authors": [
      "Eric López",
      "Artemis Llabrés",
      "Ernest Valveny"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.18984v2.pdf",
    "comment": "Accepted at Workshop on Machine Learning in Document Analysis and Recognition (ICDAR WML 2025), Wuhan, China",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Document Visual Question Answering (Document VQA) must cope with documents that span dozens of pages, yet leading systems still concatenate every page or rely on very large vision-language models, both of which are memory-hungry. Retrieval-Augmented Generation (RAG) offers an attractive alternative, first retrieving a concise set of relevant segments before generating answers from this selected evidence. In this paper, we systematically evaluate the impact of incorporating RAG into Document VQA through different retrieval variants - text-based retrieval using OCR tokens and purely visual retrieval without OCR - across multiple models and benchmarks. Evaluated on the multi-page datasets MP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the \"concatenate-all-pages\" baseline by up to +22.5 ANLS, while the visual variant achieves +5.0 ANLS improvement without requiring any text extraction. An ablation confirms that retrieval and reranking components drive most of the gain, whereas the layout-guided chunking strategy - proposed in several recent works to leverage page structure - fails to help on these datasets. Our experiments demonstrate that careful evidence selection consistently boosts accuracy across multiple model sizes and multi-page benchmarks, underscoring its practical value for real-world Document VQA."
  },
  {
    "id": "http://arxiv.org/abs/2508.20626v1",
    "updated": "2025-08-28T10:19:06Z",
    "published": "2025-08-28T10:19:06Z",
    "title": "ArtFace: Towards Historical Portrait Face Identification via Model Adaptation",
    "authors": [
      "Francois Poh",
      "Anjith George",
      "Sébastien Marcel"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20626v1.pdf",
    "comment": "4 pages, 3 figures. ArtMetrics @ ICCV 2025 (non-archival). Paper page at https://www.idiap.ch/paper/artface/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Identifying sitters in historical paintings is a key task for art historians, offering insight into their lives and how they chose to be seen. However, the process is often subjective and limited by the lack of data and stylistic variations. Automated facial recognition is capable of handling challenging conditions and can assist, but while traditional facial recognition models perform well on photographs, they struggle with paintings due to domain shift and high intra-class variation. Artistic factors such as style, skill, intent, and influence from other works further complicate recognition. In this work, we investigate the potential of foundation models to improve facial recognition in artworks. By fine-tuning foundation models and integrating their embeddings with those from conventional facial recognition networks, we demonstrate notable improvements over current state-of-the-art methods. Our results show that foundation models can bridge the gap where traditional methods are ineffective. Paper page at https://www.idiap.ch/paper/artface/"
  },
  {
    "id": "http://arxiv.org/abs/2508.20623v1",
    "updated": "2025-08-28T10:15:38Z",
    "published": "2025-08-28T10:15:38Z",
    "title": "AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images",
    "authors": [
      "Shiqi Xin",
      "Xiaolin Zhang",
      "Yanbin Liu",
      "Peng Zhang",
      "Caifeng Shan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20623v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable."
  },
  {
    "id": "http://arxiv.org/abs/2508.20622v1",
    "updated": "2025-08-28T10:13:33Z",
    "published": "2025-08-28T10:13:33Z",
    "title": "Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications",
    "authors": [
      "Immanuel Roßteutscher",
      "Klaus S. Drese",
      "Thorsten Uphues"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20622v1.pdf",
    "comment": "Submitted to IEEE Access. This is a preprint version. 14 pages, 6 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We investigated the adaptation and performance of Masked Autoencoders (MAEs) with Vision Transformer (ViT) architectures for self-supervised representation learning on one-dimensional (1D) ultrasound signals. Although MAEs have demonstrated significant success in computer vision and other domains, their use for 1D signal analysis, especially for raw ultrasound data, remains largely unexplored. Ultrasound signals are vital in industrial applications such as non-destructive testing (NDT) and structural health monitoring (SHM), where labeled data are often scarce and signal processing is highly task-specific. We propose an approach that leverages MAE to pre-train on unlabeled synthetic ultrasound signals, enabling the model to learn robust representations that enhance performance in downstream tasks, such as time-of-flight (ToF) classification. This study systematically investigated the impact of model size, patch size, and masking ratio on pre-training efficiency and downstream accuracy. Our results show that pre-trained models significantly outperform models trained from scratch and strong convolutional neural network (CNN) baselines optimized for the downstream task. Additionally, pre-training on synthetic data demonstrates superior transferability to real-world measured signals compared with training solely on limited real datasets. This study underscores the potential of MAEs for advancing ultrasound signal analysis through scalable, self-supervised learning."
  },
  {
    "id": "http://arxiv.org/abs/2508.20621v1",
    "updated": "2025-08-28T10:11:24Z",
    "published": "2025-08-28T10:11:24Z",
    "title": "Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification",
    "authors": [
      "Smriti Joshi",
      "Lidia Garrucho",
      "Richard Osuala",
      "Oliver Diaz",
      "Karim Lekadir"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20621v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Breast cancer is one of the leading causes of cancer-related mortality in women, and early detection is essential for improving outcomes. Magnetic resonance imaging (MRI) is a highly sensitive tool for breast cancer detection, particularly in women at high risk or with dense breast tissue, where mammography is less effective. The ODELIA consortium organized a multi-center challenge to foster AI-based solutions for breast cancer diagnosis and classification. The dataset included 511 studies from six European centers, acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study was labeled for the left and right breast as no lesion, benign lesion, or malignant lesion. We developed a SwinUNETR-based deep learning framework that incorporates breast region masking, extensive data augmentation, and ensemble learning to improve robustness and generalizability. Our method achieved second place on the challenge leaderboard, highlighting its potential to support clinical breast MRI interpretation. We publicly share our codebase at https://github.com/smriti-joshi/bcnaim-odelia-challenge.git."
  },
  {
    "id": "http://arxiv.org/abs/2508.20615v1",
    "updated": "2025-08-28T10:02:06Z",
    "published": "2025-08-28T10:02:06Z",
    "title": "EmoCAST: Emotional Talking Portrait via Emotive Text Description",
    "authors": [
      "Yiguo Jiang",
      "Xiaodong Cun",
      "Yong Zhang",
      "Yudian Zheng",
      "Fan Tang",
      "Chi-Man Pun"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20615v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are primarily collected in lab settings, further exacerbating these shortcomings. Consequently, these limitations substantially hinder practical applications in real-world scenarios. To address these challenges, we propose EmoCAST, a diffusion-based framework with two key modules for precise text-driven emotional synthesis. In appearance modeling, emotional prompts are integrated through a text-guided decoupled emotive module, enhancing the spatial knowledge to improve emotion comprehension. To improve the relationship between audio and emotion, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide more precise facial motion synthesis. Additionally, we construct an emotional talking head dataset with comprehensive emotive text descriptions to optimize the framework's performance. Based on the proposed dataset, we propose an emotion-aware sampling training strategy and a progressive functional training strategy that further improve the model's ability to capture nuanced expressive features and achieve accurate lip-synchronization. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST"
  },
  {
    "id": "http://arxiv.org/abs/2508.20613v1",
    "updated": "2025-08-28T10:00:39Z",
    "published": "2025-08-28T10:00:39Z",
    "title": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization",
    "authors": [
      "Yixiang Qiu",
      "Yanhan Liu",
      "Hongyao Yu",
      "Hao Fang",
      "Bin Chen",
      "Shu-Tao Xia",
      "Ke Xu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20613v1.pdf",
    "comment": "10 pages, 5 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs."
  },
  {
    "id": "http://arxiv.org/abs/2508.20612v1",
    "updated": "2025-08-28T10:00:23Z",
    "published": "2025-08-28T10:00:23Z",
    "title": "Physics Informed Generative Models for Magnetic Field Images",
    "authors": [
      "Aye Phyu Phyu Aung",
      "Lucas Lum",
      "Zhansen Shi",
      "Wen Qiu",
      "Bernice Zee",
      "JM Chin",
      "Yeow Kheng Lim",
      "J. Senthilnath"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20612v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process."
  },
  {
    "id": "http://arxiv.org/abs/2508.20605v1",
    "updated": "2025-08-28T09:51:14Z",
    "published": "2025-08-28T09:51:14Z",
    "title": "Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction",
    "authors": [
      "Karl-Philippe Beaudet",
      "Sidaty El Hadramy",
      "Philippe C Cattin",
      "Juan Verde",
      "Stéphane Cotin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20605v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Intraoperative ultrasound images are inherently challenging to interpret in liver surgery due to the limited field of view and complex anatomical structures. Bridging the gap between preoperative and intraoperative data is crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS) offers a potential solution by enabling the reconstruction of the entire organ, which facilitates registration between preoperative computed tomography (CT) scans and intraoperative IVUS images. In this work, we propose an optimization-based calibration method using a 3D-printed phantom for accurate 3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise alignment of tracked IVUS data with preoperative CT images, improving intraoperative navigation. We validated our method using in vivo swine liver images, achieving a calibration error from 0.88 to 1.80 mm and a registration error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT scan. Our method provides a reliable and accurate means of calibration and volume reconstruction. It can be used to register intraoperative ultrasound images with preoperative CT images in the context of liver surgery, and enhance intraoperative guidance."
  },
  {
    "id": "http://arxiv.org/abs/2508.20604v1",
    "updated": "2025-08-28T09:49:27Z",
    "published": "2025-08-28T09:49:27Z",
    "title": "Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion",
    "authors": [
      "Zheng Qin",
      "Yabing Wang",
      "Minghui Yang",
      "Sanping Zhou",
      "Ming Yang",
      "Le Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20604v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Generating 3D human motions from text is a challenging yet valuable task. The key aspects of this task are ensuring text-motion consistency and achieving generation diversity. Although recent advancements have enabled the generation of precise and high-quality human motions from text, achieving diversity in the generated motions remains a significant challenge. In this paper, we aim to overcome the above challenge by designing a simple yet effective text-to-motion generation method, \\textit{i.e.}, Diverse-T2M. Our method introduces uncertainty into the generation process, enabling the generation of highly diverse motions while preserving the semantic consistency of the text. Specifically, we propose a novel perspective that utilizes noise signals as carriers of diversity information in transformer-based methods, facilitating a explicit modeling of uncertainty. Moreover, we construct a latent space where text is projected into a continuous representation, instead of a rigid one-to-one mapping, and integrate a latent space sampler to introduce stochastic sampling into the generation process, thereby enhancing the diversity and uncertainty of the outputs. Our results on text-to-motion generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our method significantly enhances diversity while maintaining state-of-the-art performance in text consistency."
  },
  {
    "id": "http://arxiv.org/abs/2508.19650v2",
    "updated": "2025-08-28T09:44:26Z",
    "published": "2025-08-27T07:58:16Z",
    "title": "Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models",
    "authors": [
      "Hou Xia",
      "Zheren Fu",
      "Fangcan Ling",
      "Jiajun Li",
      "Yi Tu",
      "Zhendong Mao",
      "Yongdong Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19650v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Large video language models (LVLMs) have made notable progress in video understanding, spurring the development of corresponding evaluation benchmarks. However, existing benchmarks generally assess overall performance across entire video sequences, overlooking nuanced behaviors such as contextual positional bias, a critical yet under-explored aspect of LVLM performance. We present Video-LevelGauge, a dedicated benchmark designed to systematically assess positional bias in LVLMs. We employ standardized probes and customized contextual setups, allowing flexible control over context length, probe position, and contextual types to simulate diverse real-world scenarios. In addition, we introduce a comprehensive analysis method that combines statistical measures with morphological pattern recognition to characterize bias. Our benchmark comprises 438 manually curated videos spanning multiple types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended questions, validated for their effectiveness in exposing positional bias. Based on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and open-source models. Our findings reveal significant positional biases in many leading open-source models, typically exhibiting head or neighbor-content preferences. In contrast, commercial models such as Gemini2.5-Pro show impressive, consistent performance across entire video sequences. Further analyses on context length, context variation, and model scale provide actionable insights for mitigating bias and guiding model enhancement.https://github.com/Cola-any/Video-LevelGauge"
  },
  {
    "id": "http://arxiv.org/abs/2508.20600v1",
    "updated": "2025-08-28T09:43:59Z",
    "published": "2025-08-28T09:43:59Z",
    "title": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction",
    "authors": [
      "Kian Anvari Hamedani",
      "Narges Razizadeh",
      "Shahabedin Nabavi",
      "Mohsen Ebrahimi Moghaddam"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20600v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols."
  },
  {
    "id": "http://arxiv.org/abs/2407.11691v4",
    "updated": "2025-08-28T09:40:49Z",
    "published": "2024-07-16T13:06:15Z",
    "title": "VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models",
    "authors": [
      "Haodong Duan",
      "Xinyu Fang",
      "Junming Yang",
      "Xiangyu Zhao",
      "Yuxuan Qiao",
      "Mo Li",
      "Amit Agarwal",
      "Zhe Chen",
      "Lin Chen",
      "Yuan Liu",
      "Yubo Ma",
      "Hailong Sun",
      "Yifan Zhang",
      "Shiyin Lu",
      "Tack Hwa Wong",
      "Weiyun Wang",
      "Peiheng Zhou",
      "Xiaozhe Li",
      "Chaoyou Fu",
      "Junbo Cui",
      "Jixuan Chen",
      "Enxin Song",
      "Song Mao",
      "Shengyuan Ding",
      "Tianhao Liang",
      "Zicheng Zhang",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Pan Zhang",
      "Jiaqi Wang",
      "Dahua Lin",
      "Kai Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2407.11691v4.pdf",
    "comment": "Updated on 2025.08.28, data cut down to 2025.06.30",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We present VLMEvalKit: an open-source toolkit for evaluating large multi-modality models based on PyTorch. The toolkit aims to provide a user-friendly and comprehensive framework for researchers and developers to evaluate existing multi-modality models and publish reproducible evaluation results. In VLMEvalKit, we implement over 200+ different large multi-modality models, including both proprietary APIs and open-source models, as well as more than 80 different multi-modal benchmarks. By implementing a single interface, new models can be easily added to the toolkit, while the toolkit automatically handles the remaining workloads, including data preparation, distributed inference, prediction post-processing, and metric calculation. Although the toolkit is currently mainly used for evaluating large vision-language models, its design is compatible with future updates that incorporate additional modalities, such as audio and video. Based on the evaluation results obtained with the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard to track the progress of multi-modality learning research. The toolkit is released on https://github.com/open-compass/VLMEvalKit and is actively maintained."
  },
  {
    "id": "http://arxiv.org/abs/2508.20595v1",
    "updated": "2025-08-28T09:34:53Z",
    "published": "2025-08-28T09:34:53Z",
    "title": "Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations",
    "authors": [
      "Mengxiao Huang",
      "Minglei Shu",
      "Shuwang Zhou",
      "Zhaoyang Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20595v1.pdf",
    "comment": "Accepted to IEEE IJCNN 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Deepfake technology, driven by Generative Adversarial Networks (GANs), poses significant risks to privacy and societal security. Existing detection methods are predominantly passive, focusing on post-event analysis without preventing attacks. To address this, we propose an active defense method based on low-frequency perceptual perturbations to disrupt face swapping manipulation, reducing the performance and naturalness of generated content. Unlike prior approaches that used low-frequency perturbations to impact classification accuracy,our method directly targets the generative process of deepfake techniques. We combine frequency and spatial domain features to strengthen defenses. By introducing artifacts through low-frequency perturbations while preserving high-frequency details, we ensure the output remains visually plausible. Additionally, we design a complete architecture featuring an encoder, a perturbation generator, and a decoder, leveraging discrete wavelet transform (DWT) to extract low-frequency components and generate perturbations that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW demonstrate significant reductions in face-swapping effectiveness, improved defense success rates, and preservation of visual quality."
  },
  {
    "id": "http://arxiv.org/abs/2508.20594v1",
    "updated": "2025-08-28T09:32:51Z",
    "published": "2025-08-28T09:32:51Z",
    "title": "UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching",
    "authors": [
      "Yuqi Han",
      "Songqian Zhang",
      "Weijian Su",
      "Ke Li",
      "Jiayu Yang",
      "Jinli Suo",
      "Qiang Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20594v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The thermal camera excels at perceiving outdoor environments under low-light conditions, making it ideal for applications such as nighttime autonomous driving and unmanned navigation. However, thermal cameras encounter challenges when capturing signage from objects made of similar materials, which can pose safety risks for accurately understanding semantics in autonomous driving systems. In contrast, the neuromorphic vision camera, also known as an event camera, detects changes in light intensity asynchronously and has proven effective in high-speed, low-light traffic environments. Recognizing the complementary characteristics of these two modalities, this paper proposes UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage in low-illumination environments, targeting elements such as license plates and roadblock indicators. To address the signage blind spots of thermal imaging and the non-uniform sampling of event cameras, we developed a dual-boosting mechanism that fuses thermal frames and event signals for consistent signage representation over time. The proposed method utilizes thermal frames to provide accurate motion cues as temporal references for aligning the uneven event signals. At the same time, event signals contribute subtle signage content to the raw thermal frames, enhancing the overall understanding of the environment. The proposed method is validated on datasets collected from real-world scenarios, demonstrating superior quality in traffic signage sketching and improved detection accuracy at the perceptual level."
  },
  {
    "id": "http://arxiv.org/abs/2507.01201v5",
    "updated": "2025-08-28T09:29:38Z",
    "published": "2025-07-01T21:43:50Z",
    "title": "Escaping Plato's Cave: JAM for Aligning Independently Trained Vision and Language Models",
    "authors": [
      "Lauren Hyoseo Yoon",
      "Yisong Yue",
      "Been Kim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.01201v5.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Independently trained vision and language models inhabit disjoint representational spaces, shaped by their respective modalities, objectives, and architectures. The Platonic Representation Hypothesis (PRH) suggests these models may nonetheless converge toward a shared statistical model of reality. This raises a fundamental question: can we move beyond post-hoc detection of such alignment and explicitly optimize for it? We argue this challenge is most critical in fine-grained contextual distinctions-where multiple descriptions share global semantics but differ in subtle compositional details. We address this with the Joint Autoencoder Modulator (JAM), which aligns frozen unimodal models by jointly training modality-specific autoencoders with coordinated reconstruction and cross-modal alignment objectives. We systematically evaluate JAM across three design axes: (i) alignment objectives, introducing our multimodal Spread Loss that outperforms classic contrastive methods; (ii) the layer depth at which alignment is most effective; and (iii) the role of foundation model scale in representational convergence. Our findings show that JAM reliably induces alignment even across independently trained representations, offering both theoretical insight into the structure of shared semantics and practical guidance for transforming generalist unimodal foundations into specialist multimodal models."
  },
  {
    "id": "http://arxiv.org/abs/2508.20586v1",
    "updated": "2025-08-28T09:25:52Z",
    "published": "2025-08-28T09:25:52Z",
    "title": "FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models",
    "authors": [
      "Zheng Chong",
      "Yanwei Lei",
      "Shiyue Zhang",
      "Zhuandi He",
      "Zhen Wang",
      "Xujie Zhang",
      "Xiao Dong",
      "Yiling Wu",
      "Dongmei Jiang",
      "Xiaodan Liang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20586v1.pdf",
    "comment": "16 pages, 10 figures, 5 tables",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency."
  },
  {
    "id": "http://arxiv.org/abs/2504.17991v2",
    "updated": "2025-08-28T09:23:39Z",
    "published": "2025-04-25T00:22:17Z",
    "title": "RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation",
    "authors": [
      "Zheng Qin",
      "Le Wang",
      "Yabing Wang",
      "Sanping Zhou",
      "Gang Hua",
      "Wei Tang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.17991v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent image-goal navigation (ImageNav) methods learn a perception-action policy by separately capturing semantic features of the goal and egocentric images, then passing them to a policy network. However, challenges remain: (1) Semantic features often fail to provide accurate directional information, leading to superfluous actions, and (2) performance drops significantly when viewpoint inconsistencies arise between training and application. To address these challenges, we propose RSRNav, a simple yet effective method that reasons spatial relationships between the goal and current observations as navigation guidance. Specifically, we model the spatial relationship by constructing correlations between the goal and current observations, which are then passed to the policy network for action prediction. These correlations are progressively refined using fine-grained cross-correlation and direction-aware correlation for more precise navigation. Extensive evaluation of RSRNav on three benchmark datasets demonstrates superior navigation performance, particularly in the \"user-matched goal\" setting, highlighting its potential for real-world applications."
  },
  {
    "id": "http://arxiv.org/abs/2508.20579v1",
    "updated": "2025-08-28T09:17:57Z",
    "published": "2025-08-28T09:17:57Z",
    "title": "GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition",
    "authors": [
      "Debasis Maji",
      "Debaditya Barman"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20579v1.pdf",
    "comment": "11 pages, 6 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Facial expression recognition (FER) is a crucial task in computer vision with wide range of applications including human computer interaction, surveillance, and assistive technologies. However, challenges such as occlusion, expression variability, and lack of interpretability hinder the performance of traditional FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by modeling relational dependencies between facial landmarks, enabling structured and interpretable learning. In this paper, we propose GLaRE, a novel Graph-based Landmark Region Embedding network for emotion recognition. Facial landmarks are extracted using 3D facial alignment, and a quotient graph is constructed via hierarchical coarsening to preserve spatial structure while reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet and 94.24 percentage on FERG, outperforming several existing baselines. Additionally, ablation studies have demonstrated that region-level embeddings from quotient graphs have contributed to improved prediction performance."
  },
  {
    "id": "http://arxiv.org/abs/2508.19320v2",
    "updated": "2025-08-28T09:15:43Z",
    "published": "2025-08-26T14:00:16Z",
    "title": "MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation",
    "authors": [
      "Ming Chen",
      "Liyuan Cui",
      "Wenyuan Zhang",
      "Haoxian Zhang",
      "Yan Zhou",
      "Xiaohan Li",
      "Songlin Tang",
      "Jiwen Liu",
      "Borui Liao",
      "Hejia Chen",
      "Xiaoqiang Liu",
      "Pengfei Wan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19320v2.pdf",
    "comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with heavy computational cost and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability."
  },
  {
    "id": "http://arxiv.org/abs/2508.20570v1",
    "updated": "2025-08-28T09:08:30Z",
    "published": "2025-08-28T09:08:30Z",
    "title": "Towards Mechanistic Defenses Against Typographic Attacks in CLIP",
    "authors": [
      "Lorenz Hufe",
      "Constantin Venhoff",
      "Maximilian Dreyer",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20570v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, our method improves performance by up to 19.6% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition."
  },
  {
    "id": "http://arxiv.org/abs/2503.06989v4",
    "updated": "2025-08-28T08:43:31Z",
    "published": "2025-03-10T07:10:38Z",
    "title": "Probabilistic Modeling of Jailbreak on Multimodal LLMs: From Quantification to Application",
    "authors": [
      "Wenzhuo Xu",
      "Zhipeng Wei",
      "Xiongtao Sun",
      "Zonghao Ying",
      "Deyue Zhang",
      "Dongdong Yang",
      "Xiangzheng Zhang",
      "Quanchen Zou"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.06989v4.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated their superior ability in understanding multimodal content. However, they remain vulnerable to jailbreak attacks, which exploit weaknesses in their safety alignment to generate harmful responses. Previous studies categorize jailbreaks as successful or failed based on whether responses contain malicious content. However, given the stochastic nature of MLLM responses, this binary classification of an input's ability to jailbreak MLLMs is inappropriate. Derived from this viewpoint, we introduce jailbreak probability to quantify the jailbreak potential of an input, which represents the likelihood that MLLMs generated a malicious response when prompted with this input. We approximate this probability through multiple queries to MLLMs. After modeling the relationship between input hidden states and their corresponding jailbreak probability using Jailbreak Probability Prediction Network (JPPN), we use continuous jailbreak probability for optimization. Specifically, we propose Jailbreak-Probability-based Attack (JPA) that optimizes adversarial perturbations on input image to maximize jailbreak probability, and further enhance it as Multimodal JPA (MJPA) by including monotonic text rephrasing. To counteract attacks, we also propose Jailbreak-Probability-based Finetuning (JPF), which minimizes jailbreak probability through MLLM parameter updates. Extensive experiments show that (1) (M)JPA yields significant improvements when attacking a wide range of models under both white and black box settings. (2) JPF vastly reduces jailbreaks by at most over 60\\%. Both of the above results demonstrate the significance of introducing jailbreak probability to make nuanced distinctions among input jailbreak abilities."
  },
  {
    "id": "http://arxiv.org/abs/2508.20551v1",
    "updated": "2025-08-28T08:43:03Z",
    "published": "2025-08-28T08:43:03Z",
    "title": "Contrastive Learning through Auxiliary Branch for Video Object Detection",
    "authors": [
      "Lucas Rakotoarivony"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20551v1.pdf",
    "comment": "Accepted paper for ACIVS 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Video object detection is a challenging task because videos often suffer from image deterioration such as motion blur, occlusion, and deformable shapes, making it significantly more difficult than detecting objects in still images. Prior approaches have improved video object detection performance by employing feature aggregation and complex post-processing techniques, though at the cost of increased computational demands. To improve robustness to image degradation without additional computational load during inference, we introduce a straightforward yet effective Contrastive Learning through Auxiliary Branch (CLAB) method. First, we implement a constrastive auxiliary branch using a contrastive loss to enhance the feature representation capability of the video object detector's backbone. Next, we propose a dynamic loss weighting strategy that emphasizes auxiliary feature learning early in training while gradually prioritizing the detection task as training converges. We validate our approach through comprehensive experiments and ablation studies, demonstrating consistent performance gains. Without bells and whistles, CLAB reaches a performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101, respectively, on the ImageNet VID dataset, thus achieving state-of-the-art performance for CNN-based models without requiring additional post-processing methods."
  },
  {
    "id": "http://arxiv.org/abs/2508.20547v1",
    "updated": "2025-08-28T08:38:50Z",
    "published": "2025-08-28T08:38:50Z",
    "title": "SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes",
    "authors": [
      "Yunpeng Mei",
      "Hongjie Cao",
      "Yinqiu Xia",
      "Wei Xiao",
      "Zhaohan Feng",
      "Gang Wang",
      "Jie Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20547v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Real-time interactive grasp synthesis for dynamic objects remains challenging as existing methods fail to achieve low-latency inference while maintaining promptability. To bridge this gap, we propose SPGrasp (spatiotemporal prompt-driven dynamic grasp synthesis), a novel framework extending segment anything model v2 (SAMv2) for video stream grasp estimation. Our core innovation integrates user prompts with spatiotemporal context, enabling real-time interaction with end-to-end latency as low as 59 ms while ensuring temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on Jacquard. On the challenging GraspNet-1Billion dataset under continuous tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency, representing a 58.5% reduction compared to the prior state-of-the-art promptable method RoG-SAM while maintaining competitive accuracy. Real-world experiments involving 13 moving objects demonstrate a 94.8% success rate in interactive grasping scenarios. These results confirm SPGrasp effectively resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code is available at https://github.com/sejmoonwei/SPGrasp."
  },
  {
    "id": "http://arxiv.org/abs/2507.20198v4",
    "updated": "2025-08-28T08:29:29Z",
    "published": "2025-07-27T09:33:56Z",
    "title": "When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios",
    "authors": [
      "Kele Shao",
      "Keda Tao",
      "Kejia Zhang",
      "Sicheng Feng",
      "Mu Cai",
      "Yuzhang Shang",
      "Haoxuan You",
      "Can Qin",
      "Yang Sui",
      "Huan Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.20198v4.pdf",
    "comment": "For ongoing updates and to track the latest advances in this promising area, we maintain a public repository: https://github.com/cokeshao/Awesome-Multimodal-Token-Compression",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Multimodal large language models (MLLMs) have made remarkable strides, largely driven by their ability to process increasingly long and complex contexts, such as high-resolution images, extended video sequences, and lengthy audio input. While this ability significantly enhances MLLM capabilities, it introduces substantial computational challenges, primarily due to the quadratic complexity of self-attention mechanisms with numerous input tokens. To mitigate these bottlenecks, token compression has emerged as an auspicious and critical approach, efficiently reducing the number of tokens during both training and inference. In this paper, we present the first systematic survey and synthesis of the burgeoning field of multimodal long context token compression. Recognizing that effective compression strategies are deeply tied to the unique characteristics and redundancies of each modality, we categorize existing approaches by their primary data focus, enabling researchers to quickly access and learn methods tailored to their specific area of interest: (1) image-centric compression, which addresses spatial redundancy in visual data; (2) video-centric compression, which tackles spatio-temporal redundancy in dynamic sequences; and (3) audio-centric compression, which handles temporal and spectral redundancy in acoustic signals. Beyond this modality-driven categorization, we further dissect methods based on their underlying mechanisms, including transformation-based, similarity-based, attention-based, and query-based approaches. By providing a comprehensive and structured overview, this survey aims to consolidate current progress, identify key challenges, and inspire future research directions in this rapidly evolving domain. We also maintain a public repository to continuously track and update the latest advances in this promising area."
  },
  {
    "id": "http://arxiv.org/abs/2508.20537v1",
    "updated": "2025-08-28T08:27:25Z",
    "published": "2025-08-28T08:27:25Z",
    "title": "Domain Adaptation Techniques for Natural and Medical Image Classification",
    "authors": [
      "Ahmad Chaddad",
      "Yihang Wu",
      "Reem Kateb",
      "Christian Desrosiers"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20537v1.pdf",
    "comment": "Accepted in Information Sciences",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Domain adaptation (DA) techniques have the potential in machine learning to alleviate distribution differences between training and test sets by leveraging information from source domains. In image classification, most advances in DA have been made using natural images rather than medical data, which are harder to work with. Moreover, even for natural images, the use of mainstream datasets can lead to performance bias. {With the aim of better understanding the benefits of DA for both natural and medical images, this study performs 557 simulation studies using seven widely-used DA techniques for image classification in five natural and eight medical datasets that cover various scenarios, such as out-of-distribution, dynamic data streams, and limited training samples.} Our experiments yield detailed results and insightful observations highlighting the performance and medical applicability of these techniques. Notably, our results have shown the outstanding performance of the Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved feasible classification accuracy (91.2\\%) in the COVID-19 dataset using Resnet50 and showed an important accuracy improvement in the dynamic data stream DA scenario (+6.7\\%) compared to the baseline. Our results also demonstrate that DSAN exhibits remarkable level of explainability when evaluated on COVID-19 and skin cancer datasets. These results contribute to the understanding of DA techniques and offer valuable insight into the effective adaptation of models to medical data."
  },
  {
    "id": "http://arxiv.org/abs/2508.20534v1",
    "updated": "2025-08-28T08:21:10Z",
    "published": "2025-08-28T08:21:10Z",
    "title": "Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset",
    "authors": [
      "Frederik Rajiv Manichand",
      "Robin Deuber",
      "Robert Jakob",
      "Steve Swerling",
      "Jamie Rosen",
      "Elgar Fleisch",
      "Patrick Langer"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20534v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Estimating Body Mass Index (BMI) from camera images with machine learning models enables rapid weight assessment when traditional methods are unavailable or impractical, such as in telehealth or emergency scenarios. Existing computer vision approaches have been limited to datasets of up to 14,500 images. In this study, we present a deep learning-based BMI estimation method trained on our WayBED dataset, a large proprietary collection of 84,963 smartphone images from 25,353 individuals. We introduce an automatic filtering method that uses posture clustering and person detection to curate the dataset by removing low-quality images, such as those with atypical postures or incomplete views. This process retained 71,322 high-quality images suitable for training. We achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test set (WayBED data) using full-body images, the lowest value in the published literature to the best of our knowledge. Further, we achieve a MAPE of 13% on the completely unseen~(during training) VisualBodyToBMI dataset, comparable with state-of-the-art approaches trained on it, demonstrating robust generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the full pipeline, including image filtering and BMI estimation, on Android devices using the CLAID framework. We release our complete code for model training, filtering, and the CLAID package for mobile deployment as open-source contributions."
  },
  {
    "id": "http://arxiv.org/abs/2508.20530v1",
    "updated": "2025-08-28T08:15:23Z",
    "published": "2025-08-28T08:15:23Z",
    "title": "Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection",
    "authors": [
      "Mingqian Ji",
      "Jian Yang",
      "Shanshan Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20530v1.pdf",
    "comment": "Accepted by ACM MM 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4$\\%$ mAP on the nuScenes validation benchmark."
  },
  {
    "id": "http://arxiv.org/abs/2508.20528v1",
    "updated": "2025-08-28T08:14:55Z",
    "published": "2025-08-28T08:14:55Z",
    "title": "Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation",
    "authors": [
      "Jingyun Yang",
      "Guoqing Zhang",
      "Jingge Wang",
      "Yang Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20528v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate gross tumor volume segmentation on multi-modal medical data is critical for radiotherapy planning in nasopharyngeal carcinoma and glioblastoma. Recent advances in deep neural networks have brought promising results in medical image segmentation, leading to an increasing demand for labeled data. Since labeling medical images is time-consuming and labor-intensive, active learning has emerged as a solution to reduce annotation costs by selecting the most informative samples to label and adapting high-performance models with as few labeled samples as possible. Previous active domain adaptation (ADA) methods seek to minimize sample redundancy by selecting samples that are farthest from the source domain. However, such one-off selection can easily cause negative transfer, and access to source medical data is often limited. Moreover, the query strategy for multi-modal medical data remains unexplored. In this work, we propose an active and sequential domain adaptation framework for dynamic multi-modal sample selection in ADA. We derive a query strategy to prioritize labeling and training on the most valuable samples based on their informativeness and representativeness. Empirical validation on diverse gross tumor volume segmentation tasks demonstrates that our method achieves favorable segmentation performance, significantly outperforming state-of-the-art ADA methods. Code is available at the git repository: \\href{https://github.com/Hiyoochan/mmActS}{mmActS}."
  },
  {
    "id": "http://arxiv.org/abs/2508.20526v1",
    "updated": "2025-08-28T08:08:54Z",
    "published": "2025-08-28T08:08:54Z",
    "title": "Adam SLAM - the last mile of camera calibration with 3DGS",
    "authors": [
      "Matthieu Gendrin",
      "Stéphane Pateux",
      "Xiaoran Jiang",
      "Théo Ladune",
      "Luce Morin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20526v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The quality of the camera calibration is of major importance for evaluating progresses in novel view synthesis, as a 1-pixel error on the calibration has a significant impact on the reconstruction quality. While there is no ground truth for real scenes, the quality of the calibration is assessed by the quality of the novel view synthesis. This paper proposes to use a 3DGS model to fine tune calibration by backpropagation of novel view color loss with respect to the cameras parameters. The new calibration alone brings an average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine tuning may be long and its suitability depends on the criticity of training time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake of novel view quality is the most important."
  },
  {
    "id": "http://arxiv.org/abs/2502.01401v4",
    "updated": "2025-08-28T07:57:55Z",
    "published": "2025-02-03T14:32:36Z",
    "title": "Language-to-Space Programming for Training-Free 3D Visual Grounding",
    "authors": [
      "Boyu Mi",
      "Hanqing Wang",
      "Tai Wang",
      "Yilun Chen",
      "Jiangmiao Pang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2502.01401v4.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "3D visual grounding (3DVG) is challenging due to the need to understand 3D spatial relations. While supervised approaches have achieved superior performance, they are constrained by the scarcity and high annotation costs of 3D vision-language datasets. Training-free approaches based on LLMs/VLMs eliminate the need for large-scale training data, but they either incur prohibitive grounding time and token costs or have unsatisfactory accuracy. To address the challenges, we introduce a novel method for training-free 3D visual grounding, namely Language-to-Space Programming (LaSP). LaSP introduces LLM-generated codes to analyze 3D spatial relations among objects, along with a pipeline that evaluates and optimizes the codes automatically. Experimental results demonstrate that LaSP achieves 52.9% accuracy on the Nr3D benchmark, ranking among the best training-free methods. Moreover, it substantially reduces the grounding time and token costs, offering a balanced trade-off between performance and efficiency."
  },
  {
    "id": "http://arxiv.org/abs/2508.20516v1",
    "updated": "2025-08-28T07:57:54Z",
    "published": "2025-08-28T07:57:54Z",
    "title": "DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample",
    "authors": [
      "Wenting Yin",
      "Han Sun",
      "Xinru Meng",
      "Ningzhong Liu",
      "Huiyu Zhou"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20516v1.pdf",
    "comment": "13 pages, accepted by PRCV2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Continual test-time adaptation aims to continuously adapt a pre-trained model to a stream of target domain data without accessing source data. Without access to source domain data, the model focuses solely on the feature characteristics of the target data. Relying exclusively on these features can lead to confusion and introduce learning biases. Currently, many existing methods generate pseudo-labels via model predictions. However, the quality of pseudo-labels cannot be guaranteed and the problem of error accumulation must be solved. To address these challenges, we propose DCFS, a novel CTTA framework that introduces dual-path feature consistency and confidence-aware sample learning. This framework disentangles the whole feature representation of the target data into semantic-related feature and domain-related feature using dual classifiers to learn distinct feature representations. By maintaining consistency between the sub-features and the whole feature, the model can comprehensively capture data features from multiple perspectives. Additionally, to ensure that the whole feature information of the target domain samples is not overlooked, we set a adaptive threshold and calculate a confidence score for each sample to carry out loss weighted self-supervised learning, effectively reducing the noise of pseudo-labels and alleviating the problem of error accumulation. The efficacy of our proposed method is validated through extensive experimentation across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C, demonstrating consistent performance in continual test-time adaptation scenarios."
  },
  {
    "id": "http://arxiv.org/abs/2508.20505v1",
    "updated": "2025-08-28T07:45:08Z",
    "published": "2025-08-28T07:45:08Z",
    "title": "Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent",
    "authors": [
      "En Ci",
      "Shanyan Guan",
      "Yanhao Ge",
      "Yilin Zhang",
      "Wei Li",
      "Zhenyu Zhang",
      "Jian Yang",
      "Ying Tai"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20505v1.pdf",
    "comment": "Accepted by ICCV 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency."
  },
  {
    "id": "http://arxiv.org/abs/2508.20492v1",
    "updated": "2025-08-28T07:19:07Z",
    "published": "2025-08-28T07:19:07Z",
    "title": "IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection",
    "authors": [
      "Xuanming Cao",
      "Chengyu Tao",
      "Yifeng Cheng",
      "Juan Du"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20492v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce an novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet achieves a new state-of-the-art with a markedly lower false positive rate, underscoring its practical value for industrial deployment."
  },
  {
    "id": "http://arxiv.org/abs/2508.20491v1",
    "updated": "2025-08-28T07:16:01Z",
    "published": "2025-08-28T07:16:01Z",
    "title": "CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information",
    "authors": [
      "Seunghyeon Jung",
      "Seoyoung Hong",
      "Jiwoo Jeong",
      "Seungwon Jeong",
      "Jaerim Choi",
      "Hoki Kim",
      "Woojin Lee"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20491v1.pdf",
    "comment": "12 pages with supplementary material",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in deep learning have led to more studies to enhance golfers' shot precision. However, these existing studies have not quantitatively established the relationship between swing posture and ball trajectory, limiting their ability to provide golfers with the necessary insights for swing improvement. In this paper, we propose a new dataset called CaddieSet, which includes joint information and various ball information from a single shot. CaddieSet extracts joint information from a single swing video by segmenting it into eight swing phases using a computer vision-based approach. Furthermore, based on expert golf domain knowledge, we define 15 key metrics that influence a golf swing, enabling the interpretation of swing outcomes through swing-related features. Through experiments, we demonstrated the feasibility of CaddieSet for predicting ball trajectories using various benchmarks. In particular, we focus on interpretable models among several benchmarks and verify that swing feedback using our joint features is quantitatively consistent with established domain knowledge. This work is expected to offer new insight into golf swing analysis for both academia and the sports industry."
  },
  {
    "id": "http://arxiv.org/abs/2508.13647v2",
    "updated": "2025-08-28T07:15:28Z",
    "published": "2025-08-19T08:57:32Z",
    "title": "Model-based Multi-object Visual Tracking: Identification and Standard Model Limitations",
    "authors": [
      "Jan Krejčí",
      "Oliver Kost",
      "Yuxuan Xia",
      "Lennart Svensson",
      "Ondřej Straka"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.13647v2.pdf",
    "comment": "Accepted for publication in 2025 28th International Conference on Information Fusion (FUSION)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This paper uses multi-object tracking methods known from the radar tracking community to address the problem of pedestrian tracking using 2D bounding box detections. The standard point-object (SPO) model is adopted, and the posterior density is computed using the Poisson multi-Bernoulli mixture (PMBM) filter. The selection of the model parameters rooted in continuous time is discussed, including the birth and survival probabilities. Some parameters are selected from the first principles, while others are identified from the data, which is, in this case, the publicly available MOT-17 dataset. Although the resulting PMBM algorithm yields promising results, a mismatch between the SPO model and the data is revealed. The model-based approach assumes that modifying the problematic components causing the SPO model-data mismatch will lead to better model-based algorithms in future developments."
  },
  {
    "id": "http://arxiv.org/abs/2508.20488v1",
    "updated": "2025-08-28T07:09:21Z",
    "published": "2025-08-28T07:09:21Z",
    "title": "Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts",
    "authors": [
      "Zixuan Hu",
      "Dongxiao Li",
      "Xinzhu Ma",
      "Shixiang Tang",
      "Xiaotong Li",
      "Wenhan Yang",
      "Ling-Yu Duan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20488v1.pdf",
    "comment": "Accepted by ICCV 2025 (Highlight)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical applications like autonomous driving, yet its reliability deteriorates significantly under real-world domain shifts caused by environmental or sensor variations. To address these shifts, Test-Time Adaptation (TTA) methods have emerged, enabling models to adapt to target distributions during inference. While prior TTA approaches recognize the positive correlation between low uncertainty and high generalization ability, they fail to address the dual uncertainty inherent to M3OD: semantic uncertainty (ambiguous class predictions) and geometric uncertainty (unstable spatial localization). To bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA framework designed to jointly minimize both uncertainties for robust M3OD. Through a convex optimization lens, we introduce an innovative convex structure of the focal loss and further derive a novel unsupervised version, enabling label-agnostic uncertainty weighting and balanced learning for high-uncertainty objects. In parallel, we design a semantic-aware normal field constraint that preserves geometric coherence in regions with clear semantic cues, reducing uncertainty from the unstable 3D representation. This dual-branch mechanism forms a complementary loop: enhanced spatial perception improves semantic classification, and robust semantic predictions further refine spatial understanding. Extensive experiments demonstrate the superiority of DUO over existing methods across various datasets and domain shift types."
  },
  {
    "id": "http://arxiv.org/abs/2508.19852v2",
    "updated": "2025-08-28T07:08:03Z",
    "published": "2025-08-27T13:09:55Z",
    "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
    "authors": [
      "Binjie Zhang",
      "Mike Zheng Shou"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19852v2.pdf",
    "comment": "Code: github.com/showlab/Ego-PM",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In egocentric scenarios, anticipating both the next action and its visual outcome is essential for understanding human-object interactions and for enabling robotic planning. However, existing paradigms fall short of jointly modeling these aspects. Vision-Language-Action (VLA) models focus on action prediction but lack explicit modeling of how actions influence the visual scene, while video prediction models generate future frames without conditioning on specific actions, often resulting in implausible or contextually inconsistent outcomes. To bridge this gap, we propose a unified two-stage predictive framework that jointly models action and visual future in egocentric scenarios, conditioned on hand trajectories. In the first stage, we perform consecutive state modeling to process heterogeneous inputs (visual observations, language, and action history) and explicitly predict future hand trajectories. In the second stage, we introduce causal cross-attention to fuse multi-modal cues, leveraging inferred action signals to guide an image-based Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our approach is the first unified model designed to handle both egocentric human activity understanding and robotic manipulation tasks, providing explicit predictions of both upcoming actions and their visual consequences. Extensive experiments on Ego4D, BridgeData, and RLBench demonstrate that our method outperforms state-of-the-art baselines in both action prediction and future video synthesis."
  },
  {
    "id": "http://arxiv.org/abs/2507.10943v2",
    "updated": "2025-08-28T06:59:49Z",
    "published": "2025-07-15T03:16:12Z",
    "title": "Robust ID-Specific Face Restoration via Alignment Learning",
    "authors": [
      "Yushun Fang",
      "Lu Liu",
      "Xiang Gao",
      "Qiang Hu",
      "Ning Cao",
      "Jianghe Cui",
      "Gang Chen",
      "Xiaoyun Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.10943v2.pdf",
    "comment": "PRCV2025 Accepted",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness."
  },
  {
    "id": "http://arxiv.org/abs/2508.20478v1",
    "updated": "2025-08-28T06:55:08Z",
    "published": "2025-08-28T06:55:08Z",
    "title": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding",
    "authors": [
      "Yuan Xie",
      "Tianshui Chen",
      "Zheng Ge",
      "Lionel Ni"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20478v1.pdf",
    "comment": "15 pages, 9 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains a challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose Video-MTR, a reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in a single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for a more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce a novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding."
  },
  {
    "id": "http://arxiv.org/abs/2508.20476v1",
    "updated": "2025-08-28T06:51:42Z",
    "published": "2025-08-28T06:51:42Z",
    "title": "Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding",
    "authors": [
      "Jeong Hun Yeo",
      "Hyeongseop Rha",
      "Sungjune Park",
      "Junil Won",
      "Yong Man Ro"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20476v1.pdf",
    "comment": "Code available at: https://github.com/JeongHun0716/UniSLA",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance."
  },
  {
    "id": "http://arxiv.org/abs/2508.20475v1",
    "updated": "2025-08-28T06:51:31Z",
    "published": "2025-08-28T06:51:31Z",
    "title": "Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization",
    "authors": [
      "Marina Grifell i Plana",
      "Vladyslav Zalevskyi",
      "Léa Schmidt",
      "Yvan Gomez",
      "Thomas Sanchez",
      "Vincent Dunet",
      "Mériam Koob",
      "Vanessa Siffredi",
      "Meritxell Bach Cuadra"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20475v1.pdf",
    "comment": "Accepted at the PIPPI Workshop of MICCAI 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Accurate fetal brain segmentation is crucial for extracting biomarkers and assessing neurodevelopment, especially in conditions such as corpus callosum dysgenesis (CCD), which can induce drastic anatomical changes. However, the rarity of CCD severely limits annotated data, hindering the generalization of deep learning models. To address this, we propose a pathology-informed domain randomization strategy that embeds prior knowledge of CCD manifestations into a synthetic data generation pipeline. By simulating diverse brain alterations from healthy data alone, our approach enables robust segmentation without requiring pathological annotations. We validate our method on a cohort comprising 248 healthy fetuses, 26 with CCD, and 47 with other brain pathologies, achieving substantial improvements on CCD cases while maintaining performance on both healthy fetuses and those with other pathologies. From the predicted segmentations, we derive clinically relevant biomarkers, such as corpus callosum length (LCC) and volume, and show their utility in distinguishing CCD subtypes. Our pathology-informed augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these quantitative gains, our approach yields segmentations with improved topological consistency relative to available ground truth, enabling more reliable shape-based analyses. Overall, this work demonstrates that incorporating domain-specific anatomical priors into synthetic data pipelines can effectively mitigate data scarcity and enhance analysis of rare but clinically significant malformations."
  },
  {
    "id": "http://arxiv.org/abs/2406.14862v7",
    "updated": "2025-08-28T06:51:18Z",
    "published": "2024-06-21T04:39:03Z",
    "title": "LatentExplainer: Explaining Latent Representations in Deep Generative Models with Multimodal Large Language Models",
    "authors": [
      "Mengdan Zhu",
      "Raasikh Kanjiani",
      "Jiahui Lu",
      "Andrew Choi",
      "Qirui Ye",
      "Liang Zhao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.14862v7.pdf",
    "comment": "Accepted to CIKM 2025 Full Research Track",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Deep generative models like VAEs and diffusion models have advanced various generation tasks by leveraging latent variables to learn data distributions and generate high-quality samples. Despite the field of explainable AI making strides in interpreting machine learning models, understanding latent variables in generative models remains challenging. This paper introduces LatentExplainer, a framework for automatically generating semantically meaningful explanations of latent variables in deep generative models. LatentExplainer tackles three main challenges: inferring the meaning of latent variables, aligning explanations with inductive biases, and handling varying degrees of explainability. Our approach perturbs latent variables, interprets changes in generated data, and uses multimodal large language models (MLLMs) to produce human-understandable explanations. We evaluate our proposed method on several real-world and synthetic datasets, and the results demonstrate superior performance in generating high-quality explanations for latent variables. The results highlight the effectiveness of incorporating inductive biases and uncertainty quantification, significantly enhancing model interpretability."
  },
  {
    "id": "http://arxiv.org/abs/2508.16201v2",
    "updated": "2025-08-28T06:44:28Z",
    "published": "2025-08-22T08:23:09Z",
    "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
    "authors": [
      "Yicheng Ji",
      "Jun Zhang",
      "Heming Xia",
      "Jinpeng Chen",
      "Lidan Shou",
      "Gang Chen",
      "Huan Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.16201v2.pdf",
    "comment": "Accepted at EMNLP 2025 Main",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens to enable efficient speculation without sacrificing accuracy. To achieve this, we performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for Qwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM."
  },
  {
    "id": "http://arxiv.org/abs/2508.20471v1",
    "updated": "2025-08-28T06:39:53Z",
    "published": "2025-08-28T06:39:53Z",
    "title": "Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation",
    "authors": [
      "Jiusi Li",
      "Jackson Jiang",
      "Jinyu Miao",
      "Miao Long",
      "Tuopu Wen",
      "Peijin Jia",
      "Shengxiang Liu",
      "Chunlei Yu",
      "Maolin Liu",
      "Yuzhan Cai",
      "Kun Jiang",
      "Mengmeng Yang",
      "Diange Yang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20471v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks."
  },
  {
    "id": "http://arxiv.org/abs/2508.20470v1",
    "updated": "2025-08-28T06:39:41Z",
    "published": "2025-08-28T06:39:41Z",
    "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
    "authors": [
      "Xiaochuan Li",
      "Guoguang Du",
      "Runze Zhang",
      "Liang Jin",
      "Qi Jia",
      "Lihua Lu",
      "Zhenhua Guo",
      "Yaqian Zhao",
      "Haiyang Liu",
      "Tianqi Wang",
      "Changsheng Li",
      "Xiaoli Gong",
      "Rengang Li",
      "Baoyu Fan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20470v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/."
  },
  {
    "id": "http://arxiv.org/abs/2508.20469v1",
    "updated": "2025-08-28T06:39:38Z",
    "published": "2025-08-28T06:39:38Z",
    "title": "Prediction of Distant Metastasis for Head and Neck Cancer Patients Using Multi-Modal Tumor and Peritumoral Feature Fusion Network",
    "authors": [
      "Zizhao Tang",
      "Changhao Liu",
      "Nuo Tong",
      "Shuiping Gou",
      "Mei Shi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20469v1.pdf",
    "comment": "19 pages, 4 figures, 5 tables. Zizhao Tang, Changhao Liu, and Nuo Tong contributed equally. Corresponding Authors: Mei Shi (mshi82@fmmu.edu.cn), Shuiping Gou (shpgou@mail.xidian.edu.cn)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Metastasis remains the major challenge in the clinical management of head and neck squamous cell carcinoma (HNSCC). Reliable pre-treatment prediction of metastatic risk is crucial for optimizing treatment strategies and prognosis. This study develops a deep learning-based multimodal framework to predict metastasis risk in HNSCC patients by integrating computed tomography (CT) images, radiomics, and clinical data. 1497 HNSCC patients were included. Tumor and organ masks were derived from pretreatment CT images. A 3D Swin Transformer extracted deep features from tumor regions. Meanwhile, 1562 radiomics features were obtained using PyRadiomics, followed by correlation filtering and random forest selection, leaving 36 features. Clinical variables including age, sex, smoking, and alcohol status were encoded and fused with imaging-derived features. Multimodal features were fed into a fully connected network to predict metastasis risk. Performance was evaluated using five-fold cross-validation with area under the curve (AUC), accuracy (ACC), sensitivity (SEN), and specificity (SPE). The proposed fusion model outperformed single-modality models. The 3D deep learning module alone achieved an AUC of 0.715, and when combined with radiomics and clinical features, predictive performance improved (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758). Stratified analysis showed generalizability across tumor subtypes. Ablation studies indicated complementary information from different modalities. Evaluation showed the 3D Swin Transformer provided more robust representation learning than conventional networks. This multimodal fusion model demonstrated high accuracy and robustness in predicting metastasis risk in HNSCC, offering a comprehensive representation of tumor biology. The interpretable model has potential as a clinical decision-support tool for personalized treatment planning."
  },
  {
    "id": "http://arxiv.org/abs/2504.08419v2",
    "updated": "2025-08-28T06:36:37Z",
    "published": "2025-04-11T10:23:55Z",
    "title": "GeoTexBuild: 3D Building Model Generation from Map Footprints",
    "authors": [
      "Ruizhe Wang",
      "Junyan Yang",
      "Qiao Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.08419v2.pdf",
    "comment": "13 pages, 14 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce GeoTexBuild, a modular generative framework for creating 3D building models from footprints derived from site planning or map designs. The system is designed for architects and city planners, offering a seamless solution that directly converts map features into 3D buildings. The proposed framework employs a three-stage process comprising height map generation, geometry reconstruction, and appearance stylization, culminating in building models with detailed geometry and appearance attributes. By integrating customized ControlNet, Neural style field (NSF), and Multi-view diffusion model, we explore effective methods for controlling both geometric and visual attributes during the generation process. Our approach eliminates the problem of structural variations in a single facade image in existing 3D generation techniques for buildings. Experimental results at each stage validate the capability of GeoTexBuild to generate detailed and accurate building models from footprints."
  },
  {
    "id": "http://arxiv.org/abs/2508.20466v1",
    "updated": "2025-08-28T06:36:10Z",
    "published": "2025-08-28T06:36:10Z",
    "title": "Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds",
    "authors": [
      "Pengpeng Yu",
      "Haoran Li",
      "Dingquan Li",
      "Runqing Jiang",
      "Jing Wang",
      "Liang Lin",
      "Yulan Guo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20466v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "LiDAR point clouds are fundamental to various applications, yet high-precision scans incur substantial storage and transmission overhead. Existing methods typically convert unordered points into hierarchical octree or voxel structures for dense-to-sparse predictive coding. However, the extreme sparsity of geometric details hinders efficient context modeling, thereby limiting their compression performance and speed. To address this challenge, we propose to generate compact features for efficient predictive coding. Our framework comprises two lightweight modules. First, the Geometry Re-Densification Module re-densifies encoded sparse geometry, extracts features at denser scale, and then re-sparsifies the features for predictive coding. This module avoids costly computation on highly sparse details while maintaining a lightweight prediction head. Second, the Cross-scale Feature Propagation Module leverages occupancy cues from multiple resolution levels to guide hierarchical feature propagation. This design facilitates information sharing across scales, thereby reducing redundant feature extraction and providing enriched features for the Geometry Re-Densification Module. By integrating these two modules, our method yields a compact feature representation that provides efficient context modeling and accelerates the coding process. Experiments on the KITTI dataset demonstrate state-of-the-art compression ratios and real-time performance, achieving 26 FPS for both encoding and decoding at 12-bit quantization. Code is available at https://github.com/pengpeng-yu/FastPCC."
  },
  {
    "id": "http://arxiv.org/abs/2504.20376v2",
    "updated": "2025-08-28T06:34:01Z",
    "published": "2025-04-29T02:40:36Z",
    "title": "When Memory Becomes a Vulnerability: Towards Multi-turn Jailbreak Attacks against Text-to-Image Generation Systems",
    "authors": [
      "Shiqian Zhao",
      "Jiayang Liu",
      "Yiming Li",
      "Runyi Hu",
      "Xiaojun Jia",
      "Wenshu Fan",
      "Xinfeng Li",
      "Jie Zhang",
      "Wei Dong",
      "Tianwei Zhang",
      "Luu Anh Tuan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.20376v2.pdf",
    "comment": "This work proposes a multi-turn jailbreak attack against real-world chat-based T2I generation systems that intergrate memory mechanism. It also constructed a simulation system, with considering three industrial-grade memory mechanisms, 7 kinds of safety filters (both input and output)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Modern text-to-image (T2I) generation systems (e.g., DALL$\\cdot$E 3) exploit the memory mechanism, which captures key information in multi-turn interactions for faithful generation. Despite its practicality, the security analyses of this mechanism have fallen far behind. In this paper, we reveal that it can exacerbate the risk of jailbreak attacks. Previous attacks fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or lead to the generation of non-unsafe images due to under- or over-detoxification. In contrast, we propose embedding the malice at the inception of the chat session in memory, addressing the above limitations. Specifically, we propose Inception, the first multi-turn jailbreak attack against real-world text-to-image generation systems that explicitly exploits their memory mechanisms. Inception is composed of two key modules: segmentation and recursion. We introduce Segmentation, a semantic-preserving method that generates multi-round prompts. By leveraging NLP analysis techniques, we design policies to decompose a prompt, together with its malicious intent, according to sentence structure, thereby evading safety filters. Recursion further addresses the challenge posed by unsafe sub-prompts that cannot be separated through simple segmentation. It firstly expands the sub-prompt, then invokes segmentation recursively. To facilitate multi-turn adversarial prompts crafting, we build VisionFlow, an emulation T2I system that integrates two-stage safety filters and industrial-grade memory mechanisms. The experiment results show that Inception successfully allures unsafe image generation, surpassing the SOTA by a 20.0\\% margin in attack success rate. We also conduct experiments on the real-world commercial T2I generation platforms, further validating the threats of Inception in practice."
  },
  {
    "id": "http://arxiv.org/abs/2505.10583v2",
    "updated": "2025-08-28T06:16:17Z",
    "published": "2025-05-14T09:41:38Z",
    "title": "Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models",
    "authors": [
      "Diogo Freitas",
      "Brigt Håvardstun",
      "Cèsar Ferri",
      "Darío Garigliotti",
      "Jan Arne Telle",
      "José Hernández-Orallo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.10583v2.pdf",
    "comment": "54 pages (42 pages of appendix). Accepted for publication at the ECAI 2025 conference",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Large language models have become multimodal, and many of them are said to integrate their modalities using common representations. If this were true, a drawing of a car as an image, for instance, should map to a similar area in the latent space as a textual description of the strokes that form the drawing. To explore this in a black-box access regime to these models, we propose the use of machine teaching, a theory that studies the minimal set of examples a teacher needs to choose so that the learner captures the concept. In this paper, we evaluate the complexity of teaching vision-language models a subset of objects in the Quick, Draw! dataset using two presentations: raw images as bitmaps and trace coordinates in TikZ format. The results indicate that image-based representations generally require fewer segments and achieve higher accuracy than coordinate-based representations. But, surprisingly, the teaching size usually ranks concepts similarly across both modalities, even when controlling for (a human proxy of) concept priors, suggesting that the simplicity of concepts may be an inherent property that transcends modality representations."
  },
  {
    "id": "http://arxiv.org/abs/2508.20461v1",
    "updated": "2025-08-28T06:15:06Z",
    "published": "2025-08-28T06:15:06Z",
    "title": "Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification",
    "authors": [
      "Ayaka Tsutsumi",
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Satoshi Kondo",
      "Miki Haseyama"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20461v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We propose a novel medical image classification method that integrates dual-model weight selection with self-knowledge distillation (SKD). In real-world medical settings, deploying large-scale models is often limited by computational resource constraints, which pose significant challenges for their practical implementation. Thus, developing lightweight models that achieve comparable performance to large-scale models while maintaining computational efficiency is crucial. To address this, we employ a dual-model weight selection strategy that initializes two lightweight models with weights derived from a large pretrained model, enabling effective knowledge transfer. Next, SKD is applied to these selected models, allowing the use of a broad range of initial weight configurations without imposing additional excessive computational cost, followed by fine-tuning for the target classification tasks. By combining dual-model weight selection with self-knowledge distillation, our method overcomes the limitations of conventional approaches, which often fail to retain critical information in compact models. Extensive experiments on publicly available datasets-chest X-ray images, lung computed tomography scans, and brain magnetic resonance imaging scans-demonstrate the superior performance and robustness of our approach compared to existing methods."
  },
  {
    "id": "http://arxiv.org/abs/2508.20449v1",
    "updated": "2025-08-28T05:55:28Z",
    "published": "2025-08-28T05:55:28Z",
    "title": "A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection",
    "authors": [
      "Libo Lv",
      "Tianyi Wang",
      "Mengxiao Huang",
      "Ruixia Liu",
      "Yinglong Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20449v1.pdf",
    "comment": "Accepted to PRCV 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "With the rapid advancement of real-time deepfake generation techniques, forged content is becoming increasingly realistic and widespread across applications like video conferencing and social media. Although state-of-the-art detectors achieve high accuracy on standard benchmarks, their heavy computational cost hinders real-time deployment in practical applications. To address this, we propose the Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture for real-time deepfake detection. We design a spatial-frequency hybrid aware module that jointly leverages spatial textures and frequency artifacts through a gated mechanism, enhancing sensitivity to subtle manipulations. A token-selective cross attention mechanism enables efficient multi-level feature interaction, while a residual-enhanced blur pooling structure helps retain key semantic cues during downsampling. Experiments on several benchmark datasets show that SFMFNet achieves a favorable balance between accuracy and efficiency, with strong generalization and practical value for real-time applications."
  },
  {
    "id": "http://arxiv.org/abs/2508.20447v1",
    "updated": "2025-08-28T05:52:58Z",
    "published": "2025-08-28T05:52:58Z",
    "title": "MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection",
    "authors": [
      "Taiga Yamane",
      "Satoshi Suzuki",
      "Ryo Masumura",
      "Shota Orihashi",
      "Tomohiro Tanaka",
      "Mana Ihori",
      "Naoki Makishima",
      "Naotaka Kawata"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20447v1.pdf",
    "comment": "Accepted by BMVC 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end trainable deep learning methods have progressed greatly. However, they often struggle to detect pedestrians with consistently small or large scales in views or with vastly different scales between views. This is because they do not exploit multi-scale image features to generate the BEV feature and detect pedestrians. To overcome this problem, we propose a novel MVPD method, called Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV features by projecting multi-scale image features extracted from individual views into the BEV space, scale-by-scale. Each of these BEV features inherits the properties of its corresponding scale image features from multiple views. Therefore, these BEV features help the precise detection of pedestrians with consistently small or large scales in views. Then, MSMVD combines information at different scales of multiple views by processing the multi-scale BEV features using a feature pyramid network. This improves the detection of pedestrians with vastly different scales between views. Extensive experiments demonstrate that exploiting multi-scale image features via multi-scale BEV features greatly improves the detection performance, and MSMVD outperforms the previous highest MODA by $4.5$ points on the GMVD dataset."
  },
  {
    "id": "http://arxiv.org/abs/2501.14316v3",
    "updated": "2025-08-28T05:41:16Z",
    "published": "2025-01-24T08:21:35Z",
    "title": "T-Stars-Poster: A Framework for Product-Centric Advertising Image Design",
    "authors": [
      "Hongyu Chen",
      "Min Zhou",
      "Jing Jiang",
      "Jiale Chen",
      "Yang Lu",
      "Zihang Lin",
      "Bo Xiao",
      "Tiezheng Ge",
      "Bo Zheng"
    ],
    "pdf_url": "https://arxiv.org/pdf/2501.14316v3.pdf",
    "comment": "Accepted by CIKM 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Creating advertising images is often a labor-intensive and time-consuming process. Can we automatically generate such images using basic product information like a product foreground image, taglines, and a target size? Existing methods mainly focus on parts of the problem and lack a comprehensive solution. To bridge this gap, we propose a novel product-centric framework for advertising image design called T-Stars-Poster. It consists of four sequential stages to highlight product foregrounds and taglines while achieving overall image aesthetics: prompt generation, layout generation, background image generation, and graphics rendering. Different expert models are designed and trained for the first three stages: First, a visual language model (VLM) generates background prompts that match the products. Next, a VLM-based layout generation model arranges the placement of product foregrounds, graphic elements (taglines and decorative underlays), and various nongraphic elements (objects from the background prompt). Following this, an SDXL-based model can simultaneously accept prompts, layouts, and foreground controls to generate images. To support T-Stars-Poster, we create two corresponding datasets with over 50,000 labeled images. Extensive experiments and online A/B tests demonstrate that T-Stars-Poster can produce more visually appealing advertising images."
  },
  {
    "id": "http://arxiv.org/abs/2411.00626v2",
    "updated": "2025-08-28T04:44:11Z",
    "published": "2024-11-01T14:34:33Z",
    "title": "ZIM: Zero-Shot Image Matting for Anything",
    "authors": [
      "Beomyoung Kim",
      "Chanyong Shin",
      "Joonhyun Jeong",
      "Hyungsik Jung",
      "Se-Yun Lee",
      "Sewhan Chun",
      "Dong-Hyun Hwang",
      "Joonsang Yu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.00626v2.pdf",
    "comment": "ICCV 2025 (Highlight)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D NeRF. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at https://github.com/naver-ai/ZIM."
  },
  {
    "id": "http://arxiv.org/abs/2412.10439v3",
    "updated": "2025-08-28T04:36:42Z",
    "published": "2024-12-11T09:50:35Z",
    "title": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs",
    "authors": [
      "Yihan Cao",
      "Jiazhao Zhang",
      "Zhinan Yu",
      "Shuzhen Liu",
      "Zheng Qin",
      "Qin Zou",
      "Bo Du",
      "Kai Xu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2412.10439v3.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Object goal navigation (ObjectNav) is a fundamental task in embodied AI, requiring an agent to locate a target object in previously unseen environments. This task is particularly challenging because it requires both perceptual and cognitive processes, including object recognition and decision-making. While substantial advancements in perception have been driven by the rapid development of visual foundation models, progress on the cognitive aspect remains constrained, primarily limited to either implicit learning through simulator rollouts or explicit reliance on predefined heuristic rules. Inspired by neuroscientific findings demonstrating that humans maintain and dynamically update fine-grained cognitive states during object search tasks in novel environments, we propose CogNav, a framework designed to mimic this cognitive process using large language models. Specifically, we model the cognitive process using a finite state machine comprising fine-grained cognitive states, ranging from exploration to identification. Transitions between states are determined by a large language model based on a dynamically constructed heterogeneous cognitive map, which contains spatial and semantic information about the scene being explored. Extensive evaluations on the HM3D, MP3D, and RoboTHOR benchmarks demonstrate that our cognitive process modeling significantly improves the success rate of ObjectNav at least by relative 14% over the state-of-the-arts."
  },
  {
    "id": "http://arxiv.org/abs/2508.20415v1",
    "updated": "2025-08-28T04:31:48Z",
    "published": "2025-08-28T04:31:48Z",
    "title": "Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection",
    "authors": [
      "Yuqi Xiong",
      "Wuzhen Shi",
      "Yang Wen",
      "Ruhan Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20415v1.pdf",
    "comment": "ICONIP 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "In view of the problems that existing salient object detection (SOD) methods are prone to losing details, blurring edges, and insufficient fusion of single-modal information in complex scenes, this paper proposes a dynamic uncertainty propagation and multimodal collaborative reasoning network (DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is designed to propagate uncertainty between layers through a sparse graph constructed based on spatial semantic distance, and combined with channel adaptive interaction, it effectively improves the detection accuracy of small structures and edge regions. Secondly, a multimodal collaborative fusion strategy (MCF) is proposed, which uses learnable modality gating weights to weightedly fuse the attention maps of RGB, depth, and edge features. It can dynamically adjust the importance of each modality according to different scenes, effectively suppress redundant or interfering information, and strengthen the semantic complementarity and consistency between cross-modalities, thereby improving the ability to identify salient regions under occlusion, weak texture or background interference. Finally, the detection performance at the pixel level and region level is optimized through multi-scale BCE and IoU loss, cross-scale consistency constraints, and uncertainty-guided supervision mechanisms. Extensive experiments show that DUP-MCRNet outperforms various SOD methods on most common benchmark datasets, especially in terms of edge clarity and robustness to complex backgrounds. Our code is publicly available at https://github.com/YukiBear426/DUP-MCRNet."
  },
  {
    "id": "http://arxiv.org/abs/2508.20414v1",
    "updated": "2025-08-28T04:31:41Z",
    "published": "2025-08-28T04:31:41Z",
    "title": "Federated Learning for Large Models in Medical Imaging: A Comprehensive Review",
    "authors": [
      "Mengyu Sun",
      "Ziyuan Yang",
      "Yongqiang Huang",
      "Hui Yu",
      "Yingyu Chen",
      "Shuren Qi",
      "Andrew Beng Jin Teoh",
      "Yi Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20414v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Artificial intelligence (AI) has demonstrated considerable potential in the realm of medical imaging. However, the development of high-performance AI models typically necessitates training on large-scale, centralized datasets. This approach is confronted with significant challenges due to strict patient privacy regulations and legal restrictions on data sharing and utilization. These limitations hinder the development of large-scale models in medical domains and impede continuous updates and training with new data. Federated Learning (FL), a privacy-preserving distributed training framework, offers a new solution by enabling collaborative model development across fragmented medical datasets. In this survey, we review FL's contributions at two stages of the full-stack medical analysis pipeline. First, in upstream tasks such as CT or MRI reconstruction, FL enables joint training of robust reconstruction networks on diverse, multi-institutional datasets, alleviating data scarcity while preserving confidentiality. Second, in downstream clinical tasks like tumor diagnosis and segmentation, FL supports continuous model updating by allowing local fine-tuning on new data without centralizing sensitive images. We comprehensively analyze FL implementations across the medical imaging pipeline, from physics-informed reconstruction networks to diagnostic AI systems, highlighting innovations that improve communication efficiency, align heterogeneous data, and ensure secure parameter aggregation. Meanwhile, this paper provides an outlook on future research directions, aiming to serve as a valuable reference for the field's development."
  },
  {
    "id": "http://arxiv.org/abs/2505.24389v2",
    "updated": "2025-08-28T04:31:31Z",
    "published": "2025-05-30T09:19:33Z",
    "title": "Leadership Assessment in Pediatric Intensive Care Unit Team Training",
    "authors": [
      "Liangyang Ouyang",
      "Yuki Sakai",
      "Ryosuke Furuta",
      "Hisataka Nozawa",
      "Hikoro Matsui",
      "Yoichi Sato"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.24389v2.pdf",
    "comment": "This paper is accepted by EgoVis Workshop at CVPR 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "This paper addresses the task of assessing PICU team's leadership skills by developing an automated analysis framework based on egocentric vision. We identify key behavioral cues, including fixation object, eye contact, and conversation patterns, as essential indicators of leadership assessment. In order to capture these multimodal signals, we employ Aria Glasses to record egocentric video, audio, gaze, and head movement data. We collect one-hour videos of four simulated sessions involving doctors with different roles and levels. To automate data processing, we propose a method leveraging REMoDNaV, SAM, YOLO, and ChatGPT for fixation object detection, eye contact detection, and conversation classification. In the experiments, significant correlations are observed between leadership skills and behavioral metrics, i.e., the output of our proposed methods, such as fixation time, transition patterns, and direct orders in speech. These results indicate that our proposed data collection and analysis framework can effectively solve skill assessment for training PICU teams."
  },
  {
    "id": "http://arxiv.org/abs/2406.04680v3",
    "updated": "2025-08-28T04:29:29Z",
    "published": "2024-06-07T06:51:09Z",
    "title": "MTS-Net: Dual-Enhanced Positional Multi-Head Self-Attention for 3D CT Diagnosis of May-Thurner Syndrome",
    "authors": [
      "Yixin Huang",
      "Yiqi Jin",
      "Ke Tao",
      "Kaijian Xia",
      "Jianfeng Gu",
      "Lei Yu",
      "Haojie Li",
      "Lan Du",
      "Cunjian Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.04680v3.pdf",
    "comment": "Accepted by Biomedical Signal Processing and Control",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "May-Thurner Syndrome (MTS) is a vascular condition that affects over 20\\% of the population and significantly increases the risk of iliofemoral deep venous thrombosis. Accurate and early diagnosis of MTS using computed tomography (CT) remains a clinical challenge due to the subtle anatomical compression and variability across patients. In this paper, we propose MTS-Net, an end-to-end 3D deep learning framework designed to capture spatial-temporal patterns from CT volumes for reliable MTS diagnosis. MTS-Net builds upon 3D ResNet-18 by embedding a novel dual-enhanced positional multi-head self-attention (DEP-MHSA) module into the Transformer encoder of the network's final stages. The proposed DEP-MHSA employs multi-scale convolution and integrates positional embeddings into both attention weights and residual paths, enhancing spatial context preservation, which is crucial for identifying venous compression. To validate our approach, we curate the first publicly available dataset for MTS, MTS-CT, containing over 747 gender-balanced subjects with standard and enhanced CT scans. Experimental results demonstrate that MTS-Net achieves average 0.79 accuracy, 0.84 AUC, and 0.78 F1-score, outperforming baseline models including 3D ResNet, DenseNet-BC, and BabyNet. Our work not only introduces a new diagnostic architecture for MTS but also provides a high-quality benchmark dataset to facilitate future research in automated vascular syndrome detection. We make our code and dataset publicly available at:https://github.com/Nutingnon/MTS_dep_mhsa."
  },
  {
    "id": "http://arxiv.org/abs/2506.23484v2",
    "updated": "2025-08-28T04:19:48Z",
    "published": "2025-06-30T03:14:07Z",
    "title": "TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity",
    "authors": [
      "Yuzhuo Chen",
      "Zehua Ma",
      "Han Fang",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2506.23484v2.pdf",
    "comment": "Camera-ready version for ICCV 2025. Adds GitHub link; acknowledgments; appendix. Abstract and Figure 1 updated for clarity",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. However, the widespread adoption and advancing capabilities of generative image editing tools have amplified malicious tampering risks, while simultaneously posing new challenges to passive tampering detection and watermark robustness. To address these challenges, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results demonstrate that TAG-WM achieves state-of-the-art performance in both tampering robustness and localization capability even under distortion, while preserving lossless generation quality and maintaining a watermark capacity of 256 bits. The code is available at: https://github.com/Suchenl/TAG-WM."
  },
  {
    "id": "http://arxiv.org/abs/2505.15576v2",
    "updated": "2025-08-28T04:15:35Z",
    "published": "2025-05-21T14:28:43Z",
    "title": "Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models",
    "authors": [
      "Xin Huang",
      "Ruibin Li",
      "Tong Jia",
      "Wei Zheng",
      "Ya Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.15576v2.pdf",
    "comment": "Accepted at the International Joint Conference on Artificial Intelligence (IJCAI 2025)",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. Moreover, negative samples are typically treated uniformly, without considering their difficulty levels, and the alignment of positive samples is insufficient, which leads to challenges in aligning difficult sample pairs. To address these issues, we propose Adaptive Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard negatives into the visual domain to generate semantically disturbed image-based negatives for training the model, thereby enhancing its overall performance. AHNPL also introduces a contrastive learning approach using a multimodal hard negative loss to improve the model's discrimination of hard negatives within each modality and a dynamic margin loss that adjusts the contrastive margin according to sample difficulty to enhance the distinction of challenging sample pairs. Experiments on three public datasets demonstrate that our method effectively boosts VLMs' performance on complex CR tasks. The source code is available at https://github.com/nynu-BDAI/AHNPL."
  },
  {
    "id": "http://arxiv.org/abs/2508.20392v1",
    "updated": "2025-08-28T03:32:45Z",
    "published": "2025-08-28T03:32:45Z",
    "title": "Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection",
    "authors": [
      "Chengjun Zhang",
      "Yuhao Zhang",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20392v1.pdf",
    "comment": "12 pages, 8 figures",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Spiking Neural Networks (SNNs), inspired by the brain, are characterized by minimal power consumption and swift inference capabilities on neuromorphic hardware, and have been widely applied to various visual perception tasks. Current ANN-SNN conversion methods have achieved excellent results in classification tasks with ultra-low time-steps, but their performance in visual detection tasks remains suboptimal. In this paper, we propose a delay-spike approach to mitigate the issue of residual membrane potential caused by heterogeneous spiking patterns. Furthermore, we propose a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This enables Integrate-and-fire (IF) neurons to dynamically adjust their accumulation and firing behaviors based on the temporal order of time-steps. Our method enables spikes to exhibit distinct temporal properties, rather than relying solely on frequency-based representations. Moreover, the tdIF neuron maintains energy consumption on par with traditional IF neuron. We demonstrate that our method achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. In this study, we conduct extensive evaluation of the tdIF method across two critical vision tasks: object detection and lane line detection. The results demonstrate that the proposed method surpasses current ANN-SNN conversion approaches, achieving state-of-the-art performance with ultra-low latency (within 5 time-steps)."
  },
  {
    "id": "http://arxiv.org/abs/2508.20381v1",
    "updated": "2025-08-28T03:07:57Z",
    "published": "2025-08-28T03:07:57Z",
    "title": "More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning",
    "authors": [
      "Luong Tran",
      "Thieu Vo",
      "Anh Nguyen",
      "Sang Dinh",
      "Van Nguyen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20381v1.pdf",
    "comment": "ICCV 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Multi-label learning is a challenging computer vision task that requires assigning multiple categories to each image. However, fully annotating large-scale datasets is often impractical due to high costs and effort, motivating the study of learning from partially annotated data. In the extreme case of Single Positive Multi-Label Learning (SPML), each image is provided with only one positive label, while all other labels remain unannotated. Traditional SPML methods that treat missing labels as unknown or negative tend to yield inaccuracies and false negatives, and integrating various pseudo-labeling strategies can introduce additional noise. To address these challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a novel loss function that effectively learns from diverse pseudo-labels while mitigating noise. Complementing this, we introduce a simple yet effective Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling (AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate that our framework significantly advances multi-label classification, achieving state-of-the-art results."
  },
  {
    "id": "http://arxiv.org/abs/2508.20379v1",
    "updated": "2025-08-28T03:00:30Z",
    "published": "2025-08-28T03:00:30Z",
    "title": "Audio-Guided Visual Editing with Complex Multi-Modal Prompts",
    "authors": [
      "Hyeonyu Kim",
      "Seokhoon Jeong",
      "Seonghee Han",
      "Chanhyuk Choi",
      "Taehwan Kim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20379v1.pdf",
    "comment": "Accepted to BMVC 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail."
  },
  {
    "id": "http://arxiv.org/abs/2507.04671v2",
    "updated": "2025-08-28T02:55:48Z",
    "published": "2025-07-07T05:22:55Z",
    "title": "DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and Continuous Adaptation",
    "authors": [
      "Maolin Wang",
      "Tianshuo Wei",
      "Sheng Zhang",
      "Ruocheng Guo",
      "Wanyu Wang",
      "Shanshan Ye",
      "Lixin Zou",
      "Xuetao Wei",
      "Xiangyu Zhao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.04671v2.pdf",
    "comment": "Accepted by IJCAI 2025",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Neural Architecture Search (NAS) has emerged as a powerful approach for automating neural network design. However, existing NAS methods face critical limitations in real-world deployments: architectures lack adaptability across scenarios, each deployment context requires costly separate searches, and performance consistency across diverse platforms remains challenging. We propose DANCE (Dynamic Architectures with Neural Continuous Evolution), which reformulates architecture search as a continuous evolution problem through learning distributions over architectural components. DANCE introduces three key innovations: a continuous architecture distribution enabling smooth adaptation, a unified architecture space with learned selection gates for efficient sampling, and a multi-stage training strategy for effective deployment optimization. Extensive experiments across five datasets demonstrate DANCE's effectiveness. Our method consistently outperforms state-of-the-art NAS approaches in terms of accuracy while significantly reducing search costs. Under varying computational constraints, DANCE maintains robust performance while smoothly adapting architectures to different hardware requirements. The code and appendix can be found at https://github.com/Applied-Machine-Learning-Lab/DANCE."
  },
  {
    "id": "http://arxiv.org/abs/2508.20376v1",
    "updated": "2025-08-28T02:50:19Z",
    "published": "2025-08-28T02:50:19Z",
    "title": "Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction",
    "authors": [
      "Mang Cao",
      "Sanping Zhou",
      "Yizhe Li",
      "Ye Deng",
      "Wenli Huang",
      "Le Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20376v1.pdf",
    "comment": "Codes are available online: \\url{https://github.com/mmm-cc/BIM\\_for\\_MTL}",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Sufficient cross-task interaction is crucial for success in multi-task dense prediction. However, sufficient interaction often results in high computational complexity, forcing existing methods to face the trade-off between interaction completeness and computational efficiency. To address this limitation, this work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel scanning mechanisms to adapt the Mamba modeling approach for multi-task dense prediction. On the one hand, we introduce a novel Bidirectional Interaction Scan (BI-Scan) mechanism, which constructs task-specific representations as bidirectional sequences during interaction. By integrating task-first and position-first scanning modes within a unified linear complexity architecture, BI-Scan efficiently preserves critical cross-task information. On the other hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve multi-granularity scene modeling. This design not only meets the diverse granularity requirements of various tasks but also enhances nuanced cross-task feature interactions. Extensive experiments on two challenging benchmarks, \\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its state-of-the-art competitors."
  },
  {
    "id": "http://arxiv.org/abs/2503.22677v2",
    "updated": "2025-08-28T01:47:51Z",
    "published": "2025-03-28T17:59:53Z",
    "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.22677v2.pdf",
    "comment": "Accepted at ICCV 2025 (Highlight). Project page: https://ruiningli.com/dso",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Most 3D object generators prioritize aesthetic quality, often neglecting the physical constraints necessary for practical applications. One such constraint is that a 3D object should be self-supporting, i.e., remain balanced under gravity. Previous approaches to generating stable 3D objects relied on differentiable physics simulators to optimize geometry at test time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models with external feedback, we propose Direct Simulation Optimization (DSO). This framework leverages feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator directly outputs stable 3D objects. We construct a dataset of 3D objects labeled with stability scores obtained from the physics simulator. This dataset enables fine-tuning of the 3D generator using the stability score as an alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO) - a novel objective we introduce to align diffusion models without requiring pairwise preferences. Our experiments demonstrate that the fine-tuned feed-forward generator, using either the DPO or DRO objective, is significantly faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework functions even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs."
  },
  {
    "id": "http://arxiv.org/abs/2508.20345v1",
    "updated": "2025-08-28T01:39:16Z",
    "published": "2025-08-28T01:39:16Z",
    "title": "MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models",
    "authors": [
      "Xiao Li",
      "Yanfan Zhu",
      "Ruining Deng",
      "Wei-Qi Wei",
      "Yu Wang",
      "Shilin Zhao",
      "Yaohong Wang",
      "Haichun Yang",
      "Yuankai Huo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20345v1.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Recent advances in medical vision-language models (VLMs) open up remarkable opportunities for clinical applications such as automated report generation, copilots for physicians, and uncertainty quantification. However, despite their promise, medical VLMs introduce serious security concerns, most notably risks of Protected Health Information (PHI) exposure, data leakage, and vulnerability to cyberthreats - which are especially critical in hospital environments. Even when adopted for research or non-clinical purposes, healthcare organizations must exercise caution and implement safeguards. To address these challenges, we present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1) enables physicians to manually select and use different models without programming expertise, (2) supports engineers in efficiently deploying medical VLMs in a plug-and-play fashion, with seamless integration of Hugging Face open-source models, and (3) ensures privacy-preserving inference through Docker-orchestrated, operating system agnostic deployment. MedFoundationHub requires only an offline local workstation equipped with a single NVIDIA A6000 GPU, making it both secure and accessible within the typical resources of academic research labs. To evaluate current capabilities, we engaged board-certified pathologists to deploy and assess five state-of-the-art VLMs (Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases, yielding 1015 clinician-model scoring events. These assessments revealed recurring limitations, including off-target answers, vague reasoning, and inconsistent pathology terminology."
  },
  {
    "id": "http://arxiv.org/abs/2408.04631v2",
    "updated": "2025-08-28T01:30:18Z",
    "published": "2024-08-08T17:59:38Z",
    "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2408.04631v2.pdf",
    "comment": "Accepted at ICCV 2025. Project page: https://vgg-puppetmaster.github.io/",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "We introduce Puppet-Master, an interactive video generator that captures the internal, part-level motion of objects, serving as a proxy for modeling object dynamics universally. Given an image of an object and a set of \"drags\" specifying the trajectory of a few points on the object, the model synthesizes a video where the object's parts move accordingly. To build Puppet-Master, we extend a pre-trained image-to-video generator to encode the input drags. We also propose all-to-first attention, an alternative to conventional spatial attention that mitigates artifacts caused by fine-tuning a video generator on out-of-domain data. The model is fine-tuned on Objaverse-Animation-HQ, a new dataset of curated part-level motion clips obtained by rendering synthetic 3D animations. Unlike real videos, these synthetic clips avoid confounding part-level motion with overall object and camera motion. We extensively filter sub-optimal animations and augment the synthetic renderings with meaningful drags that emphasize the internal dynamics of objects. We demonstrate that Puppet-Master learns to generate part-level motions, unlike other motion-conditioned video generators that primarily move the object as a whole. Moreover, Puppet-Master generalizes well to out-of-domain real images, outperforming existing methods on real-world benchmarks in a zero-shot manner."
  },
  {
    "id": "http://arxiv.org/abs/2508.19575v2",
    "updated": "2025-08-28T01:28:08Z",
    "published": "2025-08-27T05:15:16Z",
    "title": "Interact-Custom: Customized Human Object Interaction Image Generation",
    "authors": [
      "Zhu Xu",
      "Zhaowen Wang",
      "Yuxin Peng",
      "Yang Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19575v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "Compositional Customized Image Generation aims to customize multiple target concepts within generation content, which has gained attention for its wild application. Existing approaches mainly concentrate on the target entity's appearance preservation, while neglecting the fine-grained interaction control among target entities. To enable the model of such interaction control capability, we focus on human object interaction scenario and propose the task of Customized Human Object Interaction Image Generation(CHOI), which simultaneously requires identity preservation for target human object and the interaction semantic control between them. Two primary challenges exist for CHOI:(1)simultaneous identity preservation and interaction control demands require the model to decompose the human object into self-contained identity features and pose-oriented interaction features, while the current HOI image datasets fail to provide ideal samples for such feature-decomposed learning.(2)inappropriate spatial configuration between human and object may lead to the lack of desired interaction semantics. To tackle it, we first process a large-scale dataset, where each sample encompasses the same pair of human object involving different interactive poses. Then we design a two-stage model Interact-Custom, which firstly explicitly models the spatial configuration by generating a foreground mask depicting the interaction behavior, then under the guidance of this mask, we generate the target human object interacting while preserving their identities features. Furthermore, if the background image and the union location of where the target human object should appear are provided by users, Interact-Custom also provides the optional functionality to specify them, offering high content controllability. Extensive experiments on our tailored metrics for CHOI task demonstrate the effectiveness of our approach."
  },
  {
    "id": "http://arxiv.org/abs/2407.14209v2",
    "updated": "2025-08-28T01:17:16Z",
    "published": "2024-07-19T11:15:02Z",
    "title": "Unlearning Concepts from Text-to-Video Diffusion Models",
    "authors": [
      "Shiqi Liu",
      "Yihua Tan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2407.14209v2.pdf",
    "comment": null,
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "With the advancement of computer vision and natural language processing, text-to-video generation, enabled by text-to-video diffusion models, has become more prevalent. These models are trained using a large amount of data from the internet. However, the training data often contain copyrighted content, including cartoon character icons and artist styles, private portraits, and unsafe videos. Since filtering the data and retraining the model is challenging, methods for unlearning specific concepts from text-to-video diffusion models have been investigated. However, due to the high computational complexity and relative large optimization scale, there is little work on unlearning methods for text-to-video diffusion models. We propose a novel concept-unlearning method by transferring the unlearning capability of the text encoder of text-to-image diffusion models to text-to-video diffusion models. Specifically, the method optimizes the text encoder using few-shot unlearning, where several generated images are used. We then use the optimized text encoder in text-to-video diffusion models to generate videos. Our method costs low computation resources and has small optimization scale. We discuss the generated videos after unlearning a concept. The experiments demonstrates that our method can unlearn copyrighted cartoon characters, artist styles, objects and people's facial characteristics. Our method can unlearn a concept within about 100 seconds on an RTX 3070. Since there was no concept unlearning method for text-to-video diffusion models before, we make concept unlearning feasible and more accessible in the text-to-video domain."
  },
  {
    "id": "http://arxiv.org/abs/2508.20325v1",
    "updated": "2025-08-28T00:07:10Z",
    "published": "2025-08-28T00:07:10Z",
    "title": "GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs",
    "authors": [
      "Haibo Jin",
      "Ruoxi Chen",
      "Peiyan Zhang",
      "Andy Zhou",
      "Yang Zhang",
      "Haohan Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20325v1.pdf",
    "comment": "54 pages",
    "category": "Computer Vision and Pattern Recognition",
    "abstract": "As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance. To address this challenge, we introduce GUARD (\\textbf{G}uideline \\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and Jailbreak \\textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations. We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications."
  },
  {
    "id": "http://arxiv.org/abs/2405.14862v2",
    "updated": "2025-08-28T17:59:31Z",
    "published": "2024-05-23T17:59:22Z",
    "title": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs",
    "authors": [
      "Dawid J. Kopiczko",
      "Tijmen Blankevoort",
      "Yuki M. Asano"
    ],
    "pdf_url": "https://arxiv.org/pdf/2405.14862v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning."
  },
  {
    "id": "http://arxiv.org/abs/2508.21051v1",
    "updated": "2025-08-28T17:55:07Z",
    "published": "2025-08-28T17:55:07Z",
    "title": "Enabling Equitable Access to Trustworthy Financial Reasoning",
    "authors": [
      "William Jurayj",
      "Nils Holzenberger",
      "Benjamin Van Durme"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21051v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "According to the United States Internal Revenue Service, ''the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance."
  },
  {
    "id": "http://arxiv.org/abs/2508.10175v2",
    "updated": "2025-08-28T17:54:36Z",
    "published": "2025-08-13T20:22:58Z",
    "title": "Estimating Machine Translation Difficulty",
    "authors": [
      "Lorenzo Proietti",
      "Stefano Perrella",
      "Vilém Zouhar",
      "Roberto Navigli",
      "Tom Kocmi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.10175v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Machine translation quality has steadily improved over the years, achieving near-perfect translations in recent benchmarks. These high-quality outputs make it difficult to distinguish between state-of-the-art models and to identify areas for future improvement. In this context, automatically identifying texts where machine translation systems struggle holds promise for developing more discriminative evaluations and guiding future research. In this work, we address this gap by formalizing the task of translation difficulty estimation, defining a text's difficulty based on the expected quality of its translations. We introduce a new metric to evaluate difficulty estimators and use it to assess both baselines and novel approaches. Finally, we demonstrate the practical utility of difficulty estimators by using them to construct more challenging benchmarks for machine translation. Our results show that dedicated models outperform both heuristic-based methods and LLM-as-a-judge approaches, with Sentinel-src achieving the best performance. Thus, we release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which can be used to scan large collections of texts and select those most likely to challenge contemporary machine translation systems."
  },
  {
    "id": "http://arxiv.org/abs/2508.21049v1",
    "updated": "2025-08-28T17:54:35Z",
    "published": "2025-08-28T17:54:35Z",
    "title": "Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm",
    "authors": [
      "Ramazan Ali Bahrami",
      "Ramin Yahyapour"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21049v1.pdf",
    "comment": "Presented in 8th International Conference on Natural Language and Speech Processing (ICNLSP), 25-27 August 2025, SDU, Odense, Denmark",
    "category": "Computation and Language",
    "abstract": "Sentential relation extraction (RE) is an important task in natural language processing (NLP). In this paper we propose to do sentential RE with dynamic routing in capsules. We first show that the proposed approach outperform state of the art on common sentential relation extraction datasets Tacred, Tacredrev, Retacred, and Conll04. We then investigate potential reasons for its good performance on the mentioned datasets, and yet low performance on another similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise in Wikidata labels as one of the reasons that can hinder performance. Additionally, we show associativity of better performance with better re-representation, a term from neuroscience referred to change of representation in human brain to improve the match at comparison time. As example, in the given analogous terms King:Queen::Man:Woman, at comparison time, and as a result of re-representation, the similarity between related head terms (King,Man), and tail terms (Queen,Woman) increases. As such, our observation show that our proposed model can do re-representation better than the vanilla model compared with. To that end, beside noise in the labels of the distantly supervised RE datasets, we propose re-representation as a challenge in sentential RE."
  },
  {
    "id": "http://arxiv.org/abs/2508.21038v1",
    "updated": "2025-08-28T17:43:53Z",
    "published": "2025-08-28T17:43:53Z",
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "authors": [
      "Orion Weller",
      "Michael Boratko",
      "Iftekhar Naim",
      "Jinhyuk Lee"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21038v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation."
  },
  {
    "id": "http://arxiv.org/abs/2203.13722v3",
    "updated": "2025-08-28T17:30:05Z",
    "published": "2022-03-25T15:45:49Z",
    "title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in Values",
    "authors": [
      "Arnav Arora",
      "Lucie-Aimée Kaffee",
      "Isabelle Augenstein"
    ],
    "pdf_url": "https://arxiv.org/pdf/2203.13722v3.pdf",
    "comment": "Accepted to C3NLP, EACL 2023: https://aclanthology.org/2023.c3nlp-1.12/",
    "category": "Computation and Language",
    "abstract": "Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys."
  },
  {
    "id": "http://arxiv.org/abs/2508.19200v2",
    "updated": "2025-08-28T17:29:36Z",
    "published": "2025-08-26T17:03:43Z",
    "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
    "authors": [
      "Xinran Zhao",
      "Boyuan Zheng",
      "Chenglei Si",
      "Haofei Yu",
      "Ken Liu",
      "Runlong Zhou",
      "Ruochen Li",
      "Tong Chen",
      "Xiang Li",
      "Yiming Zhang",
      "Tongshuang Wu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19200v2.pdf",
    "comment": "21 pages, 3 figures",
    "category": "Computation and Language",
    "abstract": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI."
  },
  {
    "id": "http://arxiv.org/abs/2508.21024v1",
    "updated": "2025-08-28T17:27:09Z",
    "published": "2025-08-28T17:27:09Z",
    "title": "An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs",
    "authors": [
      "Mathieu Bourdin",
      "Anas Neumann",
      "Thomas Paviot",
      "Robert Pellerin",
      "Samir Lamouri"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21024v1.pdf",
    "comment": "20 pages, 3 figures",
    "category": "Computation and Language",
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to mitigate the limitations of Large Language Models (LLMs), such as hallucinations and outdated knowledge. However, deploying RAG-based tools in Small and Medium Enterprises (SMEs) remains a challenge due to their limited resources and lack of expertise in natural language processing (NLP). This paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a structured, agile method designed to facilitate the deployment of RAG systems in industrial SME contexts. EASI-RAG is based on method engineering principles and comprises well-defined roles, activities, and techniques. The method was validated through a real-world case study in an environmental testing laboratory, where a RAG tool was implemented to answer operators queries using data extracted from operational procedures. The system was deployed in under a month by a team with no prior RAG experience and was later iteratively improved based on user feedback. Results demonstrate that EASI-RAG supports fast implementation, high user adoption, delivers accurate answers, and enhances the reliability of underlying data. This work highlights the potential of RAG deployment in industrial SMEs. Future works include the need for generalization across diverse use cases and further integration with fine-tuned models."
  },
  {
    "id": "http://arxiv.org/abs/2301.06375v2",
    "updated": "2025-08-28T17:12:29Z",
    "published": "2023-01-16T11:40:50Z",
    "title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset",
    "authors": [
      "Jeongkyun Park",
      "Jung-Wook Hwang",
      "Kwanghee Choi",
      "Seung-Hyun Lee",
      "Jun Hwan Ahn",
      "Rae-Hong Park",
      "Hyung-Min Park"
    ],
    "pdf_url": "https://arxiv.org/pdf/2301.06375v2.pdf",
    "comment": "Accepted to ICASSP 2024",
    "category": "Computation and Language",
    "abstract": "Inspired by humans comprehending speech in a multi-modal manner, various audio-visual datasets have been constructed. However, most existing datasets focus on English, induce dependencies with various prediction models during dataset preparation, and have only a small number of multi-view videos. To mitigate the limitations, we recently developed the Open Large-scale Korean Audio-Visual Speech (OLKAVS) dataset, which is the largest among publicly available audio-visual speech datasets. The dataset contains 1,150 hours of transcribed audio from 1,107 Korean speakers in a studio setup with nine different viewpoints and various noise situations. We also provide the pre-trained baseline models for two tasks, audio-visual speech recognition and lip reading. We conducted experiments based on the models to verify the effectiveness of multi-modal and multi-view training over uni-modal and frontal-view-only training. We expect the OLKAVS dataset to facilitate multi-modal research in broader areas such as Korean speech recognition, speaker recognition, pronunciation level classification, and mouth motion analysis."
  },
  {
    "id": "http://arxiv.org/abs/2508.21010v1",
    "updated": "2025-08-28T17:10:53Z",
    "published": "2025-08-28T17:10:53Z",
    "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Basura Fernando"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21010v1.pdf",
    "comment": "Project page: https://paritoshparmar.github.io/chainreaction/",
    "category": "Computation and Language",
    "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/"
  },
  {
    "id": "http://arxiv.org/abs/2508.21004v1",
    "updated": "2025-08-28T17:05:18Z",
    "published": "2025-08-28T17:05:18Z",
    "title": "Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution",
    "authors": [
      "Chen Chen",
      "Yuchen Sun",
      "Jiaxin Gao",
      "Xueluan Gong",
      "Qian Wang",
      "Ziyao Wang",
      "Yongsen Zheng",
      "Kwok-Yan Lam"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.21004v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) have seen significant advancements, achieving superior performance in various Natural Language Processing (NLP) tasks. However, they remain vulnerable to backdoor attacks, where models behave normally for standard queries but generate harmful responses or unintended output when specific triggers are activated. Existing backdoor defenses either lack comprehensiveness, focusing on narrow trigger settings, detection-only mechanisms, and limited domains, or fail to withstand advanced scenarios like model-editing-based, multi-trigger, and triggerless attacks. In this paper, we present LETHE, a novel method to eliminate backdoor behaviors from LLMs through knowledge dilution using both internal and external mechanisms. Internally, LETHE leverages a lightweight dataset to train a clean model, which is then merged with the backdoored model to neutralize malicious behaviors by diluting the backdoor impact within the model's parametric memory. Externally, LETHE incorporates benign and semantically relevant evidence into the prompt to distract LLM's attention from backdoor features. Experimental results on classification and generation domains across 5 widely used LLMs demonstrate that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor attacks. LETHE reduces the attack success rate of advanced backdoor attacks by up to 98% while maintaining model utility. Furthermore, LETHE has proven to be cost-efficient and robust against adaptive backdoor attacks."
  },
  {
    "id": "http://arxiv.org/abs/2405.07764v4",
    "updated": "2025-08-28T16:44:35Z",
    "published": "2024-05-13T14:07:15Z",
    "title": "LGDE: Local Graph-based Dictionary Expansion",
    "authors": [
      "Juni Schindler",
      "Sneha Jha",
      "Xixuan Zhang",
      "Kilian Buehling",
      "Annett Heft",
      "Mauricio Barahona"
    ],
    "pdf_url": "https://arxiv.org/pdf/2405.07764v4.pdf",
    "comment": "Python code available at: https://github.com/barahona-research-group/LGDE",
    "category": "Computation and Language",
    "abstract": "We present Local Graph-based Dictionary Expansion (LGDE), a method for data-driven discovery of the semantic neighbourhood of words using tools from manifold learning and network science. At the heart of LGDE lies the creation of a word similarity graph from the geometry of word embeddings followed by local community detection based on graph diffusion. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings to capture word similarities based on paths of semantic association, over and above direct pairwise similarities. Exploiting such semantic neighbourhoods enables the expansion of dictionaries of pre-selected keywords, an important step for tasks in information retrieval, such as database queries and online data collection. We validate LGDE on two user-generated English-language corpora and show that LGDE enriches the list of keywords with improved performance relative to methods based on direct word similarities or co-occurrences. We further demonstrate our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on the expansion of a conspiracy-related dictionary from online data collected and analysed by domain experts. Our empirical results and expert user assessment indicate that LGDE expands the seed dictionary with more useful keywords due to the manifold-learning-based similarity network."
  },
  {
    "id": "http://arxiv.org/abs/2507.22931v2",
    "updated": "2025-08-28T16:42:39Z",
    "published": "2025-07-24T13:46:51Z",
    "title": "Dynamic Context Compression for Efficient RAG",
    "authors": [
      "Shuyu Guo",
      "Zhaochun Ren"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.22931v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy."
  },
  {
    "id": "http://arxiv.org/abs/2508.20973v1",
    "updated": "2025-08-28T16:26:44Z",
    "published": "2025-08-28T16:26:44Z",
    "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents",
    "authors": [
      "Tianjian Liu",
      "Fanqi Wan",
      "Jiajian Guo",
      "Xiaojun Quan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20973v1.pdf",
    "comment": "21 pages, 6 Figures",
    "category": "Computation and Language",
    "abstract": "Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development."
  },
  {
    "id": "http://arxiv.org/abs/2504.07612v2",
    "updated": "2025-08-28T16:22:13Z",
    "published": "2025-04-10T10:03:29Z",
    "title": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline Dataset",
    "authors": [
      "Mihnea-Alexandru Vîrlan",
      "Răzvan-Alexandru Smădu",
      "Dumitru-Clementin Cercel",
      "Florin Pop",
      "Mihaela-Claudia Cercel"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.07612v2.pdf",
    "comment": "13 pages, 2 figures",
    "category": "Computation and Language",
    "abstract": "The primary goal of a news headline is to summarize an event in as few words as possible. Depending on the media outlet, a headline can serve as a means to objectively deliver a summary or improve its visibility. For the latter, specific publications may employ stylistic approaches that incorporate the use of sarcasm, irony, and exaggeration, key elements of a satirical approach. As such, even the headline must reflect the tone of the satirical main content. Current approaches for the Romanian language tend to detect the non-conventional tone (i.e., satire and clickbait) of the news content by combining both the main article and the headline. Because we consider a headline to be merely a brief summary of the main article, we investigate in this paper the presence of satirical tone in headlines alone, testing multiple baselines ranging from standard machine learning algorithms to deep learning models. Our experiments show that Bidirectional Transformer models outperform both standard machine-learning approaches and Large Language Models (LLMs), particularly when the meta-learning Reptile approach is employed."
  },
  {
    "id": "http://arxiv.org/abs/2508.20944v1",
    "updated": "2025-08-28T16:04:39Z",
    "published": "2025-08-28T16:04:39Z",
    "title": "STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment",
    "authors": [
      "Jiaqian Li",
      "Qisheng Hu",
      "Jing Li",
      "Wenya Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20944v1.pdf",
    "comment": "EMNLP 2025 Main",
    "category": "Computation and Language",
    "abstract": "In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to perform a wide range of tasks without task-specific fine-tuning. However, the effectiveness of ICL heavily depends on the quality of exemplar selection. In particular, for structured prediction tasks such as semantic parsing, existing ICL selection strategies often overlook structural alignment, leading to suboptimal performance and poor generalization. To address this issue, we propose a novel two-stage exemplar selection strategy that achieves a strong balance between efficiency, generalizability, and performance. First, we fine-tune a BERT-based retriever using structure-aware supervision, guiding it to select exemplars that are both semantically relevant and structurally aligned. Then, we enhance the retriever with a plug-in module, which amplifies syntactically meaningful information in the hidden representations. This plug-in is model-agnostic, requires minimal overhead, and can be seamlessly integrated into existing pipelines. Experiments on four benchmarks spanning three semantic parsing tasks demonstrate that our method consistently outperforms existing baselines with multiple recent LLMs as inference-time models."
  },
  {
    "id": "http://arxiv.org/abs/2508.20931v1",
    "updated": "2025-08-28T15:57:33Z",
    "published": "2025-08-28T15:57:33Z",
    "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench",
    "authors": [
      "Venkatesh Mishra",
      "Amir Saeidi",
      "Satyam Raj",
      "Mutsumi Nakamura",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Ali Payani",
      "Chitta Baral"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20931v1.pdf",
    "comment": "Accepted to EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like $\\tau$-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments."
  },
  {
    "id": "http://arxiv.org/abs/2508.11017v2",
    "updated": "2025-08-28T15:51:55Z",
    "published": "2025-08-14T18:44:13Z",
    "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
    "authors": [
      "Carter Blum",
      "Katja Filippova",
      "Ann Yuan",
      "Asma Ghandeharioun",
      "Julian Zimmert",
      "Fred Zhang",
      "Jessica Hoffmann",
      "Tal Linzen",
      "Martin Wattenberg",
      "Lucas Dixon",
      "Mor Geva"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.11017v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs."
  },
  {
    "id": "http://arxiv.org/abs/2508.20916v1",
    "updated": "2025-08-28T15:47:37Z",
    "published": "2025-08-28T15:47:37Z",
    "title": "SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement",
    "authors": [
      "Yuan Ge",
      "Junxiang Zhang",
      "Xiaoqian Liu",
      "Bei Li",
      "Xiangnan Ma",
      "Chenglong Wang",
      "Kaiyang Ye",
      "Yangfan Du",
      "Linfeng Zhang",
      "Yuxin Huang",
      "Tong Xiao",
      "Zhengtao Yu",
      "JingBo Zhu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20916v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling end-to-end spoken dialogue systems. However, evaluating these models remains a fundamental challenge. We propose \\texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches that disregard acoustic features, SageLM jointly assesses both semantic and acoustic dimensions. Second, it leverages rationale-based supervision to enhance explainability and guide model learning, achieving superior alignment with evaluation outcomes compared to rule-based reinforcement learning methods. Third, we introduce \\textit{SpeechFeedback}, a synthetic preference dataset, and employ a two-stage training paradigm to mitigate the scarcity of speech preference data. Trained on both semantic and acoustic dimensions, SageLM achieves an 82.79\\% agreement rate with human evaluators, outperforming cascaded and SLM-based baselines by at least 7.42\\% and 26.20\\%, respectively."
  },
  {
    "id": "http://arxiv.org/abs/2508.20893v1",
    "updated": "2025-08-28T15:22:31Z",
    "published": "2025-08-28T15:22:31Z",
    "title": "The Uneven Impact of Post-Training Quantization in Machine Translation",
    "authors": [
      "Benjamin Marie",
      "Atsushi Fujita"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20893v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Quantization is essential for deploying large language models (LLMs) on resource-constrained hardware, but its implications for multilingual tasks remain underexplored. We conduct the first large-scale evaluation of post-training quantization (PTQ) on machine translation across 55 languages using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that while 4-bit quantization often preserves translation quality for high-resource languages and large models, significant degradation occurs for low-resource and typologically diverse languages, particularly in 2-bit settings. We compare four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing that algorithm choice and model size jointly determine robustness. GGUF variants provide the most consistent performance, even at 2-bit precision. Additionally, we quantify the interactions between quantization, decoding hyperparameters, and calibration languages, finding that language-matched calibration offers benefits primarily in low-bit scenarios. Our findings offer actionable insights for deploying multilingual LLMs for machine translation under quantization constraints, especially in low-resource settings."
  },
  {
    "id": "http://arxiv.org/abs/2507.17232v2",
    "updated": "2025-08-28T15:15:18Z",
    "published": "2025-07-23T05:56:20Z",
    "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task",
    "authors": [
      "Mashiro Toyooka",
      "Kiyoharu Aizawa",
      "Yoko Yamakata"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.17232v2.pdf",
    "comment": "Accepted to ACM Multimedia 2025. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1",
    "category": "Computation and Language",
    "abstract": "Large Language Models (LLMs) are trained on a vast amount of procedural texts, but they do not directly observe real-world phenomena. In the context of cooking recipes, this poses a challenge, as intermediate states of ingredients are often omitted, making it difficult for models to track ingredient states and understand recipes accurately. In this paper, we apply state probing, a method for evaluating a language model's understanding of the world, to the domain of cooking. We propose a new task and dataset for evaluating how well LLMs can recognize intermediate ingredient states during cooking procedures. We first construct a new Japanese recipe dataset with clear and accurate annotations of ingredient state changes, collected from well-structured and controlled recipe texts. Using this dataset, we design three novel tasks to evaluate whether LLMs can track ingredient state transitions and identify ingredients present at intermediate steps. Our experiments with widely used LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state knowledge improves their understanding of cooking processes, achieving performance comparable to commercial LLMs. The dataset are publicly available at: https://huggingface.co/datasets/mashi6n/nhkrecipe-100-anno-1"
  },
  {
    "id": "http://arxiv.org/abs/2409.15912v3",
    "updated": "2025-08-28T15:08:16Z",
    "published": "2024-09-24T09:28:24Z",
    "title": "Explaining word embeddings with perfect fidelity: Case study in research impact prediction",
    "authors": [
      "Lucie Dvorackova",
      "Marcin P. Joachimiak",
      "Michal Cerny",
      "Adriana Kubecova",
      "Vilem Sklenak",
      "Tomas Kliegr"
    ],
    "pdf_url": "https://arxiv.org/pdf/2409.15912v3.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "The best-performing approaches for scholarly document quality prediction are based on embedding models. In addition to their performance when used in classifiers, embedding models can also provide predictions even for words that were not contained in the labelled training data for the classification model, which is important in the context of the ever-evolving research terminology. Although model-agnostic explanation methods, such as Local interpretable model-agnostic explanations, can be applied to explain machine learning classifiers trained on embedding models, these produce results with questionable correspondence to the model. We introduce a new feature importance method, Self-model Rated Entities (SMER), for logistic regression-based classification models trained on word embeddings. We show that SMER has theoretically perfect fidelity with the explained model, as the average of logits of SMER scores for individual words (SMER explanation) exactly corresponds to the logit of the prediction of the explained model. Quantitative and qualitative evaluation is performed through five diverse experiments conducted on 50,000 research articles (papers) from the CORD-19 corpus. Through an AOPC curve analysis, we experimentally demonstrate that SMER produces better explanations than LIME, SHAP and global tree surrogates."
  },
  {
    "id": "http://arxiv.org/abs/2508.20869v1",
    "updated": "2025-08-28T15:00:51Z",
    "published": "2025-08-28T15:00:51Z",
    "title": "OLMoASR: Open Models and Data for Training Robust Speech Recognition Models",
    "authors": [
      "Huong Ngo",
      "Matt Deitke",
      "Martijn Bartelds",
      "Sarah Pratt",
      "Josh Gardner",
      "Matt Jordan",
      "Ludwig Schmidt"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20869v1.pdf",
    "comment": "17 pages, 7 figures",
    "category": "Computation and Language",
    "abstract": "Improvements in training data scale and quality have led to significant advances, yet its influence in speech recognition remains underexplored. In this paper, we present a large-scale dataset, OLMoASR-Pool, and series of models, OLMoASR, to study and develop robust zero-shot speech recognition models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio and 17M transcripts, we design text heuristic filters to remove low-quality or mistranscribed data. Our curation pipeline produces a new dataset containing 1M hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M (tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR achieves comparable average performance to OpenAI's Whisper on short and long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a 12.8\\% and 11.0\\% word error rate (WER) that is on par with Whisper's largest English-only model Whisper-medium.en's 12.4\\% and 10.5\\% WER for short and long-form recognition respectively (at equivalent parameter count). OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will be made publicly available to further research on robust speech processing."
  },
  {
    "id": "http://arxiv.org/abs/2508.20867v1",
    "updated": "2025-08-28T14:59:55Z",
    "published": "2025-08-28T14:59:55Z",
    "title": "MSRS: Evaluating Multi-Source Retrieval-Augmented Generation",
    "authors": [
      "Rohan Phanse",
      "Yijie Zhou",
      "Kejian Shi",
      "Wencai Zhang",
      "Yixin Liu",
      "Yilun Zhao",
      "Arman Cohan"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20867v1.pdf",
    "comment": "COLM 2025; this article supersedes the preprint: arXiv:2309.08960",
    "category": "Computation and Language",
    "abstract": "Retrieval-augmented systems are typically evaluated in settings where information required to answer the query can be found within a single source or the answer is short-form or factoid-based. However, many real-world applications demand the ability to integrate and summarize information scattered across multiple sources, where no single source is sufficient to respond to the user's question. In such settings, the retrieval component of a RAG pipeline must recognize a variety of relevance signals, and the generation component must connect and synthesize information across multiple sources. We present a scalable framework for constructing evaluation benchmarks that challenge RAG systems to integrate information across distinct sources and generate long-form responses. Using our framework, we build two new benchmarks on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing narrative synthesis and summarization tasks, respectively, that require retrieval from large collections. Our extensive experiments with various RAG pipelines -- including sparse and dense retrievers combined with frontier LLMs -- reveal that generation quality is highly dependent on retrieval effectiveness, which varies greatly by task. While multi-source synthesis proves challenging even in an oracle retrieval setting, we find that reasoning models significantly outperform standard LLMs at this distinct step."
  },
  {
    "id": "http://arxiv.org/abs/2503.11519v3",
    "updated": "2025-08-28T14:55:38Z",
    "published": "2025-03-14T15:42:42Z",
    "title": "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models",
    "authors": [
      "Hao Cheng",
      "Erjia Xiao",
      "Yichi Wang",
      "Lingfeng Zhang",
      "Qiang Zhang",
      "Jiahang Cao",
      "Kaidi Xu",
      "Mengshu Sun",
      "Xiaoshuai Hao",
      "Jindong Gu",
      "Renjing Xu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.11519v3.pdf",
    "comment": "This paper is accepted by IJCAI2025 Workshop on Deepfake Detection, Localization, and Interpretability",
    "category": "Computation and Language",
    "abstract": "Current Cross-Modality Generation Models (GMs) demonstrate remarkable capabilities in various generative tasks. Given the ubiquity and information richness of vision modality inputs in real-world scenarios, Cross-Vision tasks, encompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), have attracted significant attention. Large Vision Language Models (LVLMs) and I2I Generation Models (GMs) are employed to handle VLP and I2I tasks, respectively. Previous research indicates that printing typographic words into input images significantly induces LVLMs and I2I GMs to produce disruptive outputs that are semantically aligned with those words. Additionally, visual prompts, as a more sophisticated form of typography, are also revealed to pose security risks to various applications of cross-vision tasks. However, the specific characteristics of the threats posed by visual prompts remain underexplored. In this paper, to comprehensively investigate the performance impact induced by Typographic Visual Prompt Injection (TVPI) in various LVLMs and I2I GMs, we propose the Typographic Visual Prompts Injection Dataset and thoroughly evaluate the TVPI security risks on various open-source and closed-source LVLMs and I2I GMs under visual prompts with different target semantics, deepening the understanding of TVPI threats."
  },
  {
    "id": "http://arxiv.org/abs/2504.12140v2",
    "updated": "2025-08-28T14:32:15Z",
    "published": "2025-04-16T14:52:22Z",
    "title": "Multilingual Contextualization of Large Language Models for Document-Level Machine Translation",
    "authors": [
      "Miguel Moura Ramos",
      "Patrick Fernandes",
      "Sweta Agrawal",
      "André F. T. Martins"
    ],
    "pdf_url": "https://arxiv.org/pdf/2504.12140v2.pdf",
    "comment": "COLM 2025",
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods."
  },
  {
    "id": "http://arxiv.org/abs/2508.20828v1",
    "updated": "2025-08-28T14:23:39Z",
    "published": "2025-08-28T14:23:39Z",
    "title": "GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction",
    "authors": [
      "Jie Zhao",
      "Wanting Ning",
      "Yuxiao Fei",
      "Yubo Feng",
      "Lishuang Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20828v1.pdf",
    "comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP Findings)",
    "category": "Computation and Language",
    "abstract": "In Natural Language Processing(NLP), Event Temporal Relation Extraction (ETRE) is to recognize the temporal relations of two events. Prior studies have noted the importance of language models for ETRE. However, the restricted pre-trained knowledge of Small Language Models(SLMs) limits their capability to handle minority class relations in imbalanced classification datasets. For Large Language Models(LLMs), researchers adopt manually designed prompts or instructions, which may introduce extra noise, leading to interference with the model's judgment of the long-distance dependencies between events. To address these issues, we propose GDLLM, a Global Distance-aware modeling approach based on LLMs. We first present a distance-aware graph structure utilizing Graph Attention Network(GAT) to assist the LLMs in capturing long-distance dependency features. Additionally, we design a temporal feature learning paradigm based on soft inference to augment the identification of relations with a short-distance proximity band, which supplements the probabilistic information generated by LLMs into the multi-head attention mechanism. Since the global feature can be captured effectively, our framework substantially enhances the performance of minority relation classes and improves the overall learning ability. Experiments on two publicly available datasets, TB-Dense and MATRES, demonstrate that our approach achieves state-of-the-art (SOTA) performance."
  },
  {
    "id": "http://arxiv.org/abs/2508.20810v1",
    "updated": "2025-08-28T14:10:59Z",
    "published": "2025-08-28T14:10:59Z",
    "title": "A Graph-Based Test-Harness for LLM Evaluation",
    "authors": [
      "Jessica Lundin",
      "Guillaume Chabot-Couture"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20810v1.pdf",
    "comment": "4 pages, 2 figures, dataset",
    "category": "Computation and Language",
    "abstract": "We present a first known prototype of a dynamic, systematic benchmark of medical guidelines for 400+ questions, with 3.3+ trillion possible combinations, covering 100\\% of guideline relationships. We transformed the WHO IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms, treatments, follow-ups, severities) and 300+ edges, then used graph traversal to generate questions that incorporated age-specific scenarios and contextual distractors to ensure clinical relevance. Our graph-based approach enables systematic evaluation across clinical tasks (45-67\\% accuracy), and we find models excel at symptom recognition but struggle with triaging severity, treatment protocols and follow-up care, demonstrating how customized benchmarks can identify specific capability gaps that general-domain evaluations miss. Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training (supervised finetuning, GRPO, DPO), where correct answers provide high-reward samples without expensive human annotation. The graph-based approach successfully addresses the coverage limitations of manually curated benchmarks. This methodology is a step toward scalable, contamination-resistant solution for creating comprehensive benchmarks that can be dynamically generated, including when the guidelines are updated. Code and datasets are available at https://github.com/jessicalundin/graph_testing_harness"
  },
  {
    "id": "http://arxiv.org/abs/2508.08846v2",
    "updated": "2025-08-28T14:07:41Z",
    "published": "2025-08-12T11:09:03Z",
    "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs",
    "authors": [
      "Afrozah Nadeem",
      "Mark Dras",
      "Usman Naseem"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.08846v2.pdf",
    "comment": "Accepted at CASE@RANLP2025",
    "category": "Computation and Language",
    "abstract": "Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases along political and economic dimensions. In this paper, we employ a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), this method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions."
  },
  {
    "id": "http://arxiv.org/abs/2508.20805v1",
    "updated": "2025-08-28T14:07:07Z",
    "published": "2025-08-28T14:07:07Z",
    "title": "Exploring Machine Learning and Language Models for Multimodal Depression Detection",
    "authors": [
      "Javier Si Zhao Hong",
      "Timothy Zoe Delaya",
      "Sherwyn Chan Yin Kit",
      "Pai Chet Ng",
      "Xiaoxiao Miao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20805v1.pdf",
    "comment": "This paper has been accepted by APCIPA ASC 2025",
    "category": "Computation and Language",
    "abstract": "This paper presents our approach to the first Multimodal Personality-Aware Depression Detection Challenge, focusing on multimodal depression detection using machine learning and deep learning models. We explore and compare the performance of XGBoost, transformer-based architectures, and large language models (LLMs) on audio, video, and text features. Our results highlight the strengths and limitations of each type of model in capturing depression-related signals across modalities, offering insights into effective multimodal representation strategies for mental health prediction."
  },
  {
    "id": "http://arxiv.org/abs/2508.20771v1",
    "updated": "2025-08-28T13:28:07Z",
    "published": "2025-08-28T13:28:07Z",
    "title": "Signs of Struggle: Spotting Cognitive Distortions across Language and Register",
    "authors": [
      "Abhishek Kuber",
      "Enrico Liscio",
      "Ruixuan Zhang",
      "Caroline Figueroa",
      "Pradeep K. Murukannaiah"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20771v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Rising mental health issues among youth have increased interest in automated approaches for detecting early signs of psychological distress in digital text. One key focus is the identification of cognitive distortions, irrational thought patterns that have a role in aggravating mental distress. Early detection of these distortions may enable timely, low-cost interventions. While prior work has focused on English clinical data, we present the first in-depth study of cross-lingual and cross-register generalization of cognitive distortion detection, analyzing forum posts written by Dutch adolescents. Our findings show that while changes in language and writing style can significantly affect model performance, domain adaptation methods show the most promise."
  },
  {
    "id": "http://arxiv.org/abs/2508.20766v1",
    "updated": "2025-08-28T13:22:33Z",
    "published": "2025-08-28T13:22:33Z",
    "title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection",
    "authors": [
      "Harethah Abu Shairah",
      "Hasan Abed Al Kader Hammoud",
      "George Turkiyyah",
      "Bernard Ghanem"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20766v1.pdf",
    "comment": "Under Review",
    "category": "Computation and Language",
    "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms."
  },
  {
    "id": "http://arxiv.org/abs/2508.20764v1",
    "updated": "2025-08-28T13:19:31Z",
    "published": "2025-08-28T13:19:31Z",
    "title": "Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions",
    "authors": [
      "Xiaoyi Wang",
      "Jiwei Zhang",
      "Guangtao Zhang",
      "Honglei Guo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20764v1.pdf",
    "comment": "Accepted at EMNLP 2025,14 page,3 figures",
    "category": "Computation and Language",
    "abstract": "Synthetic therapy dialogues generated by large language models (LLMs) are increasingly used in mental health NLP to simulate counseling scenarios, train models, and supplement limited real-world data. However, it remains unclear whether these synthetic conversations capture the nuanced emotional dynamics of real therapy. In this work, we conduct the first comparative analysis of emotional arcs between real and LLM-generated Cognitive Behavioral Therapy dialogues. We adapt the Utterance Emotion Dynamics framework to analyze fine-grained affective trajectories across valence, arousal, and dominance dimensions. Our analysis spans both full dialogues and individual speaker roles (counselor and client), using real sessions transcribed from public videos and synthetic dialogues from the CACTUS dataset. We find that while synthetic dialogues are fluent and structurally coherent, they diverge from real conversations in key emotional properties: real sessions exhibit greater emotional variability,more emotion-laden language, and more authentic patterns of reactivity and regulation. Moreover, emotional arc similarity between real and synthetic speakers is low, especially for clients. These findings underscore the limitations of current LLM-generated therapy data and highlight the importance of emotional fidelity in mental health applications. We introduce RealCBT, a curated dataset of real CBT sessions, to support future research in this space."
  },
  {
    "id": "http://arxiv.org/abs/2508.20757v1",
    "updated": "2025-08-28T13:14:20Z",
    "published": "2025-08-28T13:14:20Z",
    "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation",
    "authors": [
      "Yuanhao Ding",
      "Esteban Garces Arias",
      "Meimingwei Li",
      "Julian Rodemann",
      "Matthias Aßenmacher",
      "Danlu Chen",
      "Gaojuan Fan",
      "Christian Heumann",
      "Chongsheng Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20757v1.pdf",
    "comment": "Accepted at Findings of the Association for Computational Linguistics: EMNLP (Findings) 2025",
    "category": "Computation and Language",
    "abstract": "Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. While contrastive search-based decoding strategies have emerged to address this trade-off, their practical utility is often limited by hyperparameter dependence and high computational costs. We introduce GUARD, a self-adaptive decoding method that effectively balances these competing objectives through a novel \"Glocal\" uncertainty-driven framework. GUARD combines global entropy estimates with local entropy deviations to integrate both long-term and short-term uncertainty signals. We demonstrate that our proposed global entropy formulation effectively mitigates abrupt variations in uncertainty, such as sudden overconfidence or high entropy spikes, and provides theoretical guarantees of unbiasedness and consistency. To reduce computational overhead, we incorporate a simple yet effective token-count-based penalty into GUARD. Experimental results demonstrate that GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. In a more nuanced comparison study across different dimensions of text quality, both human and LLM evaluators validated its remarkable performance. Our code is available at https://github.com/YecanLee/GUARD."
  },
  {
    "id": "http://arxiv.org/abs/2411.19770v2",
    "updated": "2025-08-28T13:12:13Z",
    "published": "2024-11-29T15:18:01Z",
    "title": "Noro: Noise-Robust One-shot Voice Conversion with Hidden Speaker Representation Learning",
    "authors": [
      "Haorui He",
      "Yuchen Song",
      "Yuancheng Wang",
      "Haoyang Li",
      "Xueyao Zhang",
      "Li Wang",
      "Gongping Huang",
      "Eng Siong Chng",
      "Zhizheng Wu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.19770v2.pdf",
    "comment": "Accepted by APSIPA ASC 2025",
    "category": "Computation and Language",
    "abstract": "The effectiveness of one-shot voice conversion (VC) decreases in real-world scenarios where reference speeches, which are often sourced from the internet, contain various disturbances like background noise. To address this issue, we introduce Noro, a noise-robust one-shot VC system. Noro features innovative components tailored for VC using noisy reference speeches, including a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss. Experimental results demonstrate that Noro outperforms our baseline system in both clean and noisy scenarios, highlighting its efficacy for real-world applications. Additionally, we investigate the hidden speaker representation capabilities of our baseline system by repurposing its reference encoder as a speaker encoder. The results show that it is competitive with several advanced self-supervised learning models for speaker representation under the SUPERB settings, highlighting the potential for advancing speaker representation learning through one-shot VC tasks."
  },
  {
    "id": "http://arxiv.org/abs/2508.20750v1",
    "updated": "2025-08-28T13:08:57Z",
    "published": "2025-08-28T13:08:57Z",
    "title": "Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets",
    "authors": [
      "Vassiliy Cheremetiev",
      "Quang Long Ho Ngo",
      "Chau Ying Kot",
      "Alina Elena Baia",
      "Andrea Cavallaro"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20750v1.pdf",
    "comment": "Paper accepted at the DHOW Workshop at ACM Multimedia 2025. Code available at https://github.com/idiap/implicit-hsd",
    "category": "Computation and Language",
    "abstract": "Implicit hate speech (IHS) is indirect language that conveys prejudice or hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to detect as it does not include explicit derogatory or inflammatory words. To address this challenge, task-specific pipelines can be complemented with external knowledge or additional information such as context, emotions and sentiment data. In this paper, we show that, by solely fine-tuning recent general-purpose embedding models based on large language models (LLMs), such as Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance. Experiments on multiple IHS datasets show up to 1.10 percentage points improvements for in-dataset, and up to 20.35 percentage points improvements in cross-dataset evaluation, in terms of F1-macro score."
  },
  {
    "id": "http://arxiv.org/abs/2508.20736v1",
    "updated": "2025-08-28T12:59:01Z",
    "published": "2025-08-28T12:59:01Z",
    "title": "Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees",
    "authors": [
      "Stephen Meisenbacher",
      "Maulik Chevli",
      "Florian Matthes"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20736v1.pdf",
    "comment": "17 pages, 2 figures, 11 tables. Accepted to EMNLP 2025 (Main)",
    "category": "Computation and Language",
    "abstract": "Many works at the intersection of Differential Privacy (DP) in Natural Language Processing aim to protect privacy by transforming texts under DP guarantees. This can be performed in a variety of ways, from word perturbations to full document rewriting, and most often under local DP. Here, an input text must be made indistinguishable from any other potential text, within some bound governed by the privacy parameter $\\varepsilon$. Such a guarantee is quite demanding, and recent works show that privatizing texts under local DP can only be done reasonably under very high $\\varepsilon$ values. Addressing this challenge, we introduce DP-ST, which leverages semantic triples for neighborhood-aware private document generation under local DP guarantees. Through the evaluation of our method, we demonstrate the effectiveness of the divide-and-conquer paradigm, particularly when limiting the DP notion (and privacy guarantees) to that of a privatization neighborhood. When combined with LLM post-processing, our method allows for coherent text generation even at lower $\\varepsilon$ values, while still balancing privacy and utility. These findings highlight the importance of coherence in achieving balanced privatization outputs at reasonable $\\varepsilon$ levels."
  },
  {
    "id": "http://arxiv.org/abs/2508.20722v1",
    "updated": "2025-08-28T12:45:25Z",
    "published": "2025-08-28T12:45:25Z",
    "title": "rStar2-Agent: Agentic Reasoning Technical Report",
    "authors": [
      "Ning Shang",
      "Yifei Liu",
      "Yi Zhu",
      "Li Lyna Zhang",
      "Weijiang Xu",
      "Xinyu Guan",
      "Buze Zhang",
      "Bingcheng Dong",
      "Xudong Zhou",
      "Bowen Zhang",
      "Ying Xin",
      "Ziming Miao",
      "Scarlett Li",
      "Fan Yang",
      "Mao Yang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20722v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar."
  },
  {
    "id": "http://arxiv.org/abs/2508.20718v1",
    "updated": "2025-08-28T12:43:21Z",
    "published": "2025-08-28T12:43:21Z",
    "title": "Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models",
    "authors": [
      "Ruiyi Yan",
      "Yugo Murawaki"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20718v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models have significantly enhanced the capacities and efficiency of text generation. On the one hand, they have improved the quality of text-based steganography. On the other hand, they have also underscored the importance of watermarking as a safeguard against malicious misuse. In this study, we focus on tokenization inconsistency (TI) between Alice and Bob in steganography and watermarking, where TI can undermine robustness. Our investigation reveals that the problematic tokens responsible for TI exhibit two key characteristics: infrequency and temporariness. Based on these findings, we propose two tailored solutions for TI elimination: a stepwise verification method for steganography and a post-hoc rollback method for watermarking. Experiments show that (1) compared to traditional disambiguation methods in steganography, directly addressing TI leads to improvements in fluency, imperceptibility, and anti-steganalysis capacity; (2) for watermarking, addressing TI enhances detectability and robustness against attacks."
  },
  {
    "id": "http://arxiv.org/abs/2508.20712v1",
    "updated": "2025-08-28T12:30:32Z",
    "published": "2025-08-28T12:30:32Z",
    "title": "Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning",
    "authors": [
      "Nelson Filipe Costa",
      "Leila Kosseim"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20712v1.pdf",
    "comment": "Published at SIGDIAL 2025. Best paper award",
    "category": "Computation and Language",
    "abstract": "This paper introduces the first multi-lingual and multi-label classification model for implicit discourse relation recognition (IDRR). Our model, HArch, is evaluated on the recently released DiscoGeM 2.0 corpus and leverages hierarchical dependencies between discourse senses to predict probability distributions across all three sense levels in the PDTB 3.0 framework. We compare several pre-trained encoder backbones and find that RoBERTa-HArch achieves the best performance in English, while XLM-RoBERTa-HArch performs best in the multi-lingual setting. In addition, we compare our fine-tuned models against GPT-4o and Llama-4-Maverick using few-shot prompting across all language configurations. Our results show that our fine-tuned models consistently outperform these LLMs, highlighting the advantages of task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our hierarchical approach."
  },
  {
    "id": "http://arxiv.org/abs/2212.07126v2",
    "updated": "2025-08-28T12:21:45Z",
    "published": "2022-12-14T09:25:49Z",
    "title": "Explainability of Text Processing and Retrieval Methods: A Survey",
    "authors": [
      "Sourav Saha",
      "Debapriyo Majumdar",
      "Mandar Mitra"
    ],
    "pdf_url": "https://arxiv.org/pdf/2212.07126v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Deep Learning and Machine Learning based models have become extremely popular in text processing and information retrieval. However, the non-linear structures present inside the networks make these models largely inscrutable. A significant body of research has focused on increasing the transparency of these models. This article provides a broad overview of research on the explainability and interpretability of natural language processing and information retrieval methods. More specifically, we survey approaches that have been applied to explain word embeddings, sequence modeling, attention modules, transformers, BERT, and document ranking. The concluding section suggests some possible directions for future research on this topic."
  },
  {
    "id": "http://arxiv.org/abs/2508.20701v1",
    "updated": "2025-08-28T12:19:34Z",
    "published": "2025-08-28T12:19:34Z",
    "title": "Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings",
    "authors": [
      "Ares Fabregat-Hernández",
      "Javier Palanca",
      "Vicent Botti"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20701v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "The paper introduces a novel framework based on category theory to enhance the explainability of artificial intelligence systems, particularly focusing on word embeddings. Key topics include the construction of categories $\\mathcal{L}_T$ and $\\mathcal{P}_T$, providing schematic representations of the semantics of a text $ T $, and reframing the selection of the element with maximum probability as a categorical notion. Additionally, the monoidal category $\\mathcal{P}_T$ is constructed to visualize various methods of extracting semantic information from $T$, offering a dimension-agnostic definition of semantic spaces reliant solely on information within the text. Furthermore, the paper defines the categories of configurations Conf and word embeddings $\\mathcal{Emb}$, accompanied by the concept of divergence as a decoration on $\\mathcal{Emb}$. It establishes a mathematically precise method for comparing word embeddings, demonstrating the equivalence between the GloVe and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural network algorithms (black box) to a transparent framework. Finally, the paper presents a mathematical approach to computing biases before embedding and offers insights on mitigating biases at the semantic space level, advancing the field of explainable artificial intelligence."
  },
  {
    "id": "http://arxiv.org/abs/2508.20700v1",
    "updated": "2025-08-28T12:18:35Z",
    "published": "2025-08-28T12:18:35Z",
    "title": "Generative Annotation for ASR Named Entity Correction",
    "authors": [
      "Yuanchang Luo",
      "Daimeng Wei",
      "Shaojun Li",
      "Hengchao Shang",
      "Jiaxin Guo",
      "Zongyao Li",
      "Zhanglin Wu",
      "Xiaoyu Chen",
      "Zhiqiang Rao",
      "Jinlong Yang",
      "Hao Yang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20700v1.pdf",
    "comment": "12 pages, 7 figures, 7 tables, EMNLP 2025",
    "category": "Computation and Language",
    "abstract": "End-to-end automatic speech recognition systems often fail to transcribe domain-specific named entities, causing catastrophic failures in downstream tasks. Numerous fast and lightweight named entity correction (NEC) models have been proposed in recent years. These models, mainly leveraging phonetic-level edit distance algorithms, have shown impressive performances. However, when the forms of the wrongly-transcribed words(s) and the ground-truth entity are significantly different, these methods often fail to locate the wrongly transcribed words in hypothesis, thus limiting their usage. We propose a novel NEC method that utilizes speech sound features to retrieve candidate entities. With speech sound features and candidate entities, we inovatively design a generative method to annotate entity errors in ASR transcripts and replace the text with correct entities. This method is effective in scenarios of word form difference. We test our method using open-source and self-constructed test sets. The results demonstrate that our NEC method can bring significant improvement to entity accuracy. We will open source our self-constructed test set and training data."
  },
  {
    "id": "http://arxiv.org/abs/2508.18321v2",
    "updated": "2025-08-28T12:18:04Z",
    "published": "2025-08-24T09:58:10Z",
    "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions",
    "authors": [
      "Maojia Song",
      "Tej Deep Pala",
      "Weisheng Jin",
      "Amir Zadeh",
      "Chuan Li",
      "Dorien Herremans",
      "Soujanya Poria"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.18321v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: https://github.com/declare-lab/KAIROS."
  },
  {
    "id": "http://arxiv.org/abs/2508.20697v1",
    "updated": "2025-08-28T12:07:11Z",
    "published": "2025-08-28T12:07:11Z",
    "title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning",
    "authors": [
      "Weitao Feng",
      "Lixu Wang",
      "Tianyi Wei",
      "Jie Zhang",
      "Chongyang Gao",
      "Sinong Zhan",
      "Peizhuo Lv",
      "Wei Dong"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20697v1.pdf",
    "comment": "Project Hompage: https://tokenbuncher.github.io/",
    "category": "Computation and Language",
    "abstract": "As large language models (LLMs) continue to grow in capability, so do the risks of harmful misuse through fine-tuning. While most prior studies assume that attackers rely on supervised fine-tuning (SFT) for such misuse, we systematically demonstrate that reinforcement learning (RL) enables adversaries to more effectively break safety alignment and facilitate advanced harmful task assistance, under matched computational budgets. To counter this emerging threat, we propose TokenBuncher, the first effective defense specifically targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation on which RL relies: model response uncertainty. By constraining uncertainty, RL-based fine-tuning can no longer exploit distinct reward signals to drive the model toward harmful behaviors. We realize this defense through entropy-as-reward RL and a Token Noiser mechanism designed to prevent the escalation of expert-domain harmful capabilities. Extensive experiments across multiple models and RL algorithms show that TokenBuncher robustly mitigates harmful RL fine-tuning while preserving benign task utility and finetunability. Our results highlight that RL-based harmful fine-tuning poses a greater systemic risk than SFT, and that TokenBuncher provides an effective and general defense."
  },
  {
    "id": "http://arxiv.org/abs/2508.19724v2",
    "updated": "2025-08-28T12:05:33Z",
    "published": "2025-08-27T09:34:28Z",
    "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks",
    "authors": [
      "Aritra Dutta",
      "Swapnanil Mukherjee",
      "Deepanway Ghosal",
      "Somak Aditya"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19724v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Commonsense visual-question answering often hinges on knowledge that is missing from the image or the question. Small vision-language models (sVLMs) such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative counterparts. To study the effect of careful commonsense knowledge integration on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural language facts, (ii) prompts an LLM to craft natural language explanations, and (iii) feeds both signals to sVLMs respectively across two commonsense VQA datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts retrieved using a fine-tuned ColBERTv2 and an object information-enriched prompt yield explanations that largely cut down hallucinations, while lifting the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional finetuning using noise-robust losses (such as symmetric cross entropy and generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our findings expose when LLM-based commonsense knowledge beats retrieval from commonsense knowledge bases, how noise-aware training stabilises small models in the context of external knowledge augmentation, and why parameter-efficient commonsense reasoning is now within reach for 250M models."
  },
  {
    "id": "http://arxiv.org/abs/2508.20693v1",
    "updated": "2025-08-28T11:53:45Z",
    "published": "2025-08-28T11:53:45Z",
    "title": "Leveraging Large Language Models for Generating Research Topic Ontologies: A Multi-Disciplinary Study",
    "authors": [
      "Tanay Aggarwal",
      "Angelo Salatino",
      "Francesco Osborne",
      "Enrico Motta"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20693v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Ontologies and taxonomies of research fields are critical for managing and organising scientific knowledge, as they facilitate efficient classification, dissemination and retrieval of information. However, the creation and maintenance of such ontologies are expensive and time-consuming tasks, usually requiring the coordinated effort of multiple domain experts. Consequently, ontologies in this space often exhibit uneven coverage across different disciplines, limited inter-domain connectivity, and infrequent updating cycles. In this study, we investigate the capability of several large language models to identify semantic relationships among research topics within three academic domains: biomedicine, physics, and engineering. The models were evaluated under three distinct conditions: zero-shot prompting, chain-of-thought prompting, and fine-tuning on existing ontologies. Additionally, we assessed the cross-domain transferability of fine-tuned models by measuring their performance when trained in one domain and subsequently applied to a different one. To support this analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over 8,000 relationships extracted from the most widely adopted taxonomies in the three disciplines considered in this study: MeSH, PhySH, and IEEE. Our experiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent performance across all disciplines."
  },
  {
    "id": "http://arxiv.org/abs/2508.16599v2",
    "updated": "2025-08-28T11:53:23Z",
    "published": "2025-08-09T16:29:10Z",
    "title": "Humans Perceive Wrong Narratives from AI Reasoning Texts",
    "authors": [
      "Mosh Levy",
      "Zohar Elyoseph",
      "Yoav Goldberg"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.16599v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "A new generation of AI models generates step-by-step reasoning text before producing an answer. This text appears to offer a human-readable window into their computation process, and is increasingly relied upon for transparency and interpretability. However, it is unclear whether human understanding of this text matches the model's actual computational process. In this paper, we investigate a necessary condition for correspondence: the ability of humans to identify which steps in a reasoning text causally influence later steps. We evaluated humans on this ability by composing questions based on counterfactual measurements and found a significant discrepancy: participant accuracy was only 29%, barely above chance (25%), and remained low (42%) even when evaluating the majority vote on questions with high agreement. Our results reveal a fundamental gap between how humans interpret reasoning texts and how models use it, challenging its utility as a simple interpretability tool. We argue that reasoning texts should be treated as an artifact to be investigated, not taken at face value, and that understanding the non-human ways these models use language is a critical research direction."
  },
  {
    "id": "http://arxiv.org/abs/2508.20691v1",
    "updated": "2025-08-28T11:50:22Z",
    "published": "2025-08-28T11:50:22Z",
    "title": "MobileCLIP2: Improving Multi-Modal Reinforced Training",
    "authors": [
      "Fartash Faghri",
      "Pavan Kumar Anasosalu Vasu",
      "Cem Koc",
      "Vaishaal Shankar",
      "Alexander Toshev",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20691v1.pdf",
    "comment": "TMLR August 2025",
    "category": "Computation and Language",
    "abstract": "Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\\times$ smaller and improves on DFN ViT-L/14 at 2.5$\\times$ lower latency. We release our pretrained models (https://github.com/apple/ml-mobileclip) and the data generation code (https://github.com/apple/ml-mobileclip-dr). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing."
  },
  {
    "id": "http://arxiv.org/abs/2406.16464v6",
    "updated": "2025-08-28T11:35:48Z",
    "published": "2024-06-24T09:13:42Z",
    "title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection",
    "authors": [
      "Junjie Chen",
      "Hang Yu",
      "Subin Huang",
      "Sanmin Liu",
      "Linfeng Zhang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.16464v6.pdf",
    "comment": "ACM TOMM (Under Review); Code and data are available at https://github.com/CoderChen01/InterCLIP-MEP",
    "category": "Computation and Language",
    "abstract": "Sarcasm in social media, often expressed through text-image combinations, poses challenges for sentiment analysis and intention mining. Current multi-modal sarcasm detection methods have been demonstrated to overly rely on spurious cues within the textual modality, revealing a limited ability to genuinely identify sarcasm through nuanced text-image interactions. To solve this problem, we propose InterCLIP-MEP, which introduces Interactive CLIP (InterCLIP) with an efficient training strategy to extract enriched text-image representations by embedding cross-modal information directly into each encoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a dynamic dual-channel memory that stores valuable test sample knowledge during inference, acting as a non-parametric classifier for robust sarcasm recognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP achieves state-of-the-art performance, with significant accuracy and F1 score improvements on MMSD and MMSD2.0. Our code is available at https://github.com/CoderChen01/InterCLIP-MEP."
  },
  {
    "id": "http://arxiv.org/abs/2502.19074v2",
    "updated": "2025-08-28T11:20:32Z",
    "published": "2025-02-26T11:56:43Z",
    "title": "Improving the quality of Web-mined Parallel Corpora of Low-Resource Languages using Debiasing Heuristics",
    "authors": [
      "Aloka Fernando",
      "Nisansa de Silva",
      "Menan Velyuthan",
      "Charitha Rathnayake",
      "Surangika Ranathunga"
    ],
    "pdf_url": "https://arxiv.org/pdf/2502.19074v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Parallel Data Curation (PDC) techniques aim to filter out noisy parallel sentences from the web-mined corpora. Prior research has demonstrated that ranking sentence pairs using similarity scores on sentence embeddings derived from Pre-trained Multilingual Language Models (multiPLMs) and training the NMT systems with the top-ranked samples, produces superior NMT performance than when trained using the full dataset. However, previous research has shown that the choice of multiPLM significantly impacts the ranking quality. This paper investigates the reasons behind this disparity across multiPLMs. Using the web-mined corpora CCMatrix and CCAligned for En$\\rightarrow$Si, En$\\rightarrow$Ta and Si$\\rightarrow$Ta, we show that different multiPLMs (LASER3, XLM-R, and LaBSE) are biased towards certain types of sentences, which allows noisy sentences to creep into the top-ranked samples. We show that by employing a series of heuristics, this noise can be removed to a certain extent. This results in improving the results of NMT systems trained with web-mined corpora and reduces the disparity across multiPLMs."
  },
  {
    "id": "http://arxiv.org/abs/2508.20655v1",
    "updated": "2025-08-28T11:01:33Z",
    "published": "2025-08-28T11:01:33Z",
    "title": "Improving Alignment in LVLMs with Debiased Self-Judgment",
    "authors": [
      "Sihan Yang",
      "Chenhang Cui",
      "Zihao Zhao",
      "Yiyang Zhou",
      "Weilong Yan",
      "Ying Wei",
      "Huaxiu Yao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20655v1.pdf",
    "comment": "EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinations--where generated outputs are not grounded in the visual input--and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs."
  },
  {
    "id": "http://arxiv.org/abs/2508.20637v1",
    "updated": "2025-08-28T10:35:44Z",
    "published": "2025-08-28T10:35:44Z",
    "title": "GDS Agent: A Graph Algorithmic Reasoning Agent",
    "authors": [
      "Borun Shi",
      "Ioannis Panagiotas"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20637v1.pdf",
    "comment": "Technical report",
    "category": "Computation and Language",
    "abstract": "Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We also introduce a new benchmark that evaluates intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap."
  },
  {
    "id": "http://arxiv.org/abs/2508.19720v2",
    "updated": "2025-08-28T10:00:55Z",
    "published": "2025-08-27T09:30:24Z",
    "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models",
    "authors": [
      "Yilin Wang",
      "Heng Wang",
      "Yuyang Bai",
      "Minnan Luo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19720v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "In Large Language Models (LLMs) generation, there exist knowledge conflicts and scenarios where parametric knowledge contradicts knowledge provided in the context. Previous works studied tuning, decoding algorithms, or locating and editing context-aware neurons to adapt LLMs to be faithful to new contextual knowledge. However, they are usually inefficient or ineffective for large models, not workable for black-box models, or unable to continuously adjust LLMs' sensitivity to the knowledge provided in the context. To mitigate these problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a simple framework that can steer LLMs' sensitivity to contextual knowledge continuously at a lightweight cost. Specifically, we tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM without modifying the LLM weights. In the evaluation process, we not only design synthetic data and fine-grained metrics to measure models' sensitivity to contextual knowledge but also use a real conflict dataset to validate CSKS's practical efficacy. Extensive experiments demonstrate that our framework achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity and reduced sensitivity, thereby allowing LLMs to prioritize either contextual or parametric knowledge as needed flexibly. Our data and code are available at https://github.com/OliveJuiceLin/CSKS."
  },
  {
    "id": "http://arxiv.org/abs/2506.11752v2",
    "updated": "2025-08-28T09:45:44Z",
    "published": "2025-06-13T13:05:41Z",
    "title": "DART: Distilling Autoregressive Reasoning to Silent Thought",
    "authors": [
      "Nan Jiang",
      "Ziming Wu",
      "De-Chuan Zhan",
      "Fuming Lai",
      "Shaobing Lian"
    ],
    "pdf_url": "https://arxiv.org/pdf/2506.11752v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose \\textbf{DART} (\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent \\textbf{T}hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART offers significant performance gains compared with existing non-autoregressive baselines without extra inference latency, serving as a feasible alternative for efficient reasoning."
  },
  {
    "id": "http://arxiv.org/abs/2506.06294v2",
    "updated": "2025-08-28T09:38:11Z",
    "published": "2025-05-17T14:45:13Z",
    "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning",
    "authors": [
      "Yunqing Liu",
      "Wenqi Fan",
      "Xiaoyong Wei",
      "Qing Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2506.06294v2.pdf",
    "comment": "Accepted to EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \\textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on."
  },
  {
    "id": "http://arxiv.org/abs/2508.07999v2",
    "updated": "2025-08-28T09:31:57Z",
    "published": "2025-08-11T14:03:09Z",
    "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
    "authors": [
      "Ryan Wong",
      "Jiawei Wang",
      "Junjie Zhao",
      "Li Chen",
      "Yan Gao",
      "Long Zhang",
      "Xuan Zhou",
      "Zuo Wang",
      "Kai Xiang",
      "Ge Zhang",
      "Wenhao Huang",
      "Yang Wang",
      "Ke Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.07999v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such \"wide-context\" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\\%, with the best performer reaching just 5\\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/"
  },
  {
    "id": "http://arxiv.org/abs/2508.20583v1",
    "updated": "2025-08-28T09:20:47Z",
    "published": "2025-08-28T09:20:47Z",
    "title": "A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models",
    "authors": [
      "Soham Petkar",
      "Hari Aakash K",
      "Anirudh Vempati",
      "Akshit Sinha",
      "Ponnurangam Kumarauguru",
      "Chirag Agarwal"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20583v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Developments in Graph-Language Models (GLMs) aim to integrate the structural reasoning capabilities of Graph Neural Networks (GNNs) with the semantic understanding of Large Language Models (LLMs). However, we demonstrate that current evaluation benchmarks for GLMs, which are primarily repurposed node-level classification datasets, are insufficient to assess multimodal reasoning. Our analysis reveals that strong performance on these benchmarks is achievable using unimodal information alone, suggesting that they do not necessitate graph-language integration. To address this evaluation gap, we introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed to evaluate multimodal reasoning at various complexity levels. Our benchmark employs a synthetic graph generation pipeline paired with questions that require joint reasoning over structure and textual semantics. We perform a thorough evaluation of representative GLM architectures and find that soft-prompted LLM baselines perform on par with GLMs that incorporate a full GNN backbone. This result calls into question the architectural necessity of incorporating graph structure into LLMs. We further show that GLMs exhibit significant performance degradation in tasks that require structural reasoning. These findings highlight limitations in the graph reasoning capabilities of current GLMs and provide a foundation for advancing the community toward explicit multimodal reasoning involving graph structure and language."
  },
  {
    "id": "http://arxiv.org/abs/2503.11302v4",
    "updated": "2025-08-28T09:20:39Z",
    "published": "2025-03-14T11:11:03Z",
    "title": "Are formal and functional linguistic mechanisms dissociated in language models?",
    "authors": [
      "Michael Hanna",
      "Yonatan Belinkov",
      "Sandro Pezzelle"
    ],
    "pdf_url": "https://arxiv.org/pdf/2503.11302v4.pdf",
    "comment": "To appear in Computational Linguistics. Pre-MIT Press publication version. 40 pages, 14 figures, 3 tables. Code available at https://github.com/hannamw/formal-functional-dissociation",
    "category": "Computation and Language",
    "abstract": "Although large language models (LLMs) are increasingly capable, these capabilities are unevenly distributed: they excel at formal linguistic tasks, such as producing fluent, grammatical text, but struggle more with functional linguistic tasks like reasoning and consistent fact retrieval. Inspired by neuroscience, recent work suggests that to succeed on both formal and functional linguistic tasks, LLMs should use different mechanisms for each; such localization could either be built-in or emerge spontaneously through training. In this paper, we ask: do current models, with fast-improving functional linguistic abilities, exhibit distinct localization of formal and functional linguistic mechanisms? We answer this by finding and comparing the \"circuits\", or minimal computational subgraphs, responsible for various formal and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that while there is indeed little overlap between circuits for formal and functional tasks, there is also little overlap between formal linguistic tasks, as exists in the human brain. Thus, a single formal linguistic network, unified and distinct from functional task circuits, remains elusive. However, in terms of cross-task faithfulness - the ability of one circuit to solve another's task - we observe a separation between formal and functional mechanisms, suggesting that shared mechanisms between formal tasks may exist."
  },
  {
    "id": "http://arxiv.org/abs/2508.20577v1",
    "updated": "2025-08-28T09:14:23Z",
    "published": "2025-08-28T09:14:23Z",
    "title": "MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training",
    "authors": [
      "Yang Luo",
      "Zangwei Zheng",
      "Ziheng Qin",
      "Zirui Zhu",
      "Yong Liu",
      "Yang You"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20577v1.pdf",
    "comment": "ICML 2025",
    "category": "Computation and Language",
    "abstract": "Large-batch training has become a cornerstone in accelerating the training of deep neural networks, yet it poses challenges in optimization and generalization. Existing optimizers like AdamW present performance degradation during language models' large-batch training, due to the information bottleneck in attention layers caused by the sharp increase of max attention logit. While the LAMB optimizer partially addresses this issue, some attention layers still face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are less effective in directly influencing the max value of query/key weights. Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks relationships of weight values within rows or columns. Building on these observations, we propose a novel optimizer, MERIT, which leverages the max-norm to calculate the trust ratio to constrain the max attention logit more effectively. Moreover, we further construct element-wise trust ratios to provide more robust update scaling by focusing on local weight structures. Extensive experiments of large-batch training across various sizes of GPT-2 models demonstrate the superior performance of MERIT. Notably, during the training of GPT-2 Medium, MERIT enables a 6k batch size without any performance degradation compared to the standard batch size (480) with 48B training tokens. This work highlights the importance of considering the max attention logit and finer-granularity trust ratio in large-batch training. It successfully improves the training stability and paves the way for larger batch usage, enabling faster development and iteration of large language models. Code is available at https://github.com/NUS-HPC-AI-Lab/MERIT."
  },
  {
    "id": "http://arxiv.org/abs/2407.08952v6",
    "updated": "2025-08-28T09:09:46Z",
    "published": "2024-07-12T03:15:01Z",
    "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework for Few-shot Fake News Detection",
    "authors": [
      "Ye Liu",
      "Jiajun Zhu",
      "Xukai Liu",
      "Haoyu Tang",
      "Yanghai Zhang",
      "Kai Zhang",
      "Xiaofang Zhou",
      "Enhong Chen"
    ],
    "pdf_url": "https://arxiv.org/pdf/2407.08952v6.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news from real ones in extremely low-resource scenarios. This task has garnered increased attention due to the widespread dissemination and harmful impact of fake news on social media. Large Language Models (LLMs) have demonstrated competitive performance with the help of their rich prior knowledge and excellent in-context learning abilities. However, existing methods face significant limitations, such as the Understanding Ambiguity and Information Scarcity, which significantly undermine the potential of LLMs. To address these shortcomings, we propose a Dual-perspective Knowledge-guided Fake News Detection (DKFND) model, designed to enhance LLMs from both inside and outside perspectives. Specifically, DKFND first identifies the knowledge concepts of each news article through a Detection Module. Subsequently, DKFND creatively designs an Investigation Module to retrieve inside and outside valuable information concerning to the current news, followed by another Judge Module to evaluate the relevance and confidence of them. Finally, a Determination Module further derives two respective predictions and obtain the final result. Extensive experiments on two public datasets show the efficacy of our proposed method, particularly in low-resource settings."
  },
  {
    "id": "http://arxiv.org/abs/2508.20567v1",
    "updated": "2025-08-28T09:06:38Z",
    "published": "2025-08-28T09:06:38Z",
    "title": "KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling",
    "authors": [
      "Yangfan Wang",
      "Jie Liu",
      "Chen Tang",
      "Lian Yan",
      "Jingchi Jiang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20567v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Multi-hop question answering faces substantial challenges due to data sparsity, which increases the likelihood of language models learning spurious patterns. To address this issue, prior research has focused on diversifying question generation through content planning and varied expression. However, these approaches often emphasize generating simple questions and neglect the integration of essential knowledge, such as relevant sentences within documents. This paper introduces the Knowledge Composition Sampling (KCS), an innovative framework designed to expand the diversity of generated multi-hop questions by sampling varied knowledge compositions within a given context. KCS models the knowledge composition selection as a sentence-level conditional prediction task and utilizes a probabilistic contrastive loss to predict the next most relevant piece of knowledge. During inference, we employ a stochastic decoding strategy to effectively balance accuracy and diversity. Compared to competitive baselines, our KCS improves the overall accuracy of knowledge composition selection by 3.9%, and its application for data augmentation yields improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available at: https://github.com/yangfanww/kcs."
  },
  {
    "id": "http://arxiv.org/abs/2508.18642v2",
    "updated": "2025-08-28T08:59:07Z",
    "published": "2025-08-26T03:40:06Z",
    "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing",
    "authors": [
      "Jianxing Liao",
      "Tian Zhang",
      "Xiao Feng",
      "Yusong Zhang",
      "Rui Yang",
      "Haorui Wang",
      "Bosi Wen",
      "Ziying Wang",
      "Runzhi Shi"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.18642v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large language models are extensively utilized in creative writing applications. Creative writing requires a balance between subjective writing quality (e.g., literariness and emotional expression) and objective constraint following (e.g., format requirements and word limits). Existing methods find it difficult to balance these two aspects: single reward strategies fail to improve both abilities simultaneously, while fixed-weight mixed-reward methods lack the ability to adapt to different writing scenarios. To address this problem, we propose Reinforcement Learning with Mixed Rewards (RLMR), utilizing a dynamically mixed reward system from a writing reward model evaluating subjective writing quality and a constraint verification model assessing objective constraint following. The constraint following reward weight is adjusted dynamically according to the writing quality within sampled groups, ensuring that samples violating constraints get negative advantage in GRPO and thus penalized during training, which is the key innovation of this proposed method. We conduct automated and manual evaluations across diverse model families from 8B to 72B parameters. Additionally, we construct a real-world writing benchmark named WriteEval for comprehensive evaluation. Results illustrate that our method achieves consistent improvements in both instruction following (IFEval from 83.36% to 86.65%) and writing quality (72.75% win rate in manual expert pairwise evaluations on WriteEval). To the best of our knowledge, RLMR is the first work to combine subjective preferences with objective verification in online RL training, providing an effective solution for multi-dimensional creative writing optimization."
  },
  {
    "id": "http://arxiv.org/abs/2508.20559v1",
    "updated": "2025-08-28T08:51:51Z",
    "published": "2025-08-28T08:51:51Z",
    "title": "Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search",
    "authors": [
      "Zeyu Xiong",
      "Yixuan Nan",
      "Li Gao",
      "Hengzhu Tang",
      "Shuaiqiang Wang",
      "Junfeng Wang",
      "Dawei Yin"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20559v1.pdf",
    "comment": "CIKM'25",
    "category": "Computation and Language",
    "abstract": "In the dynamic landscape of large-scale web search, Query-Driven Text Summarization (QDTS) aims to generate concise and informative summaries from textual documents based on a given query, which is essential for improving user engagement and facilitating rapid decision-making. Traditional extractive summarization models, based primarily on ranking candidate summary segments, have been the dominant approach in industrial applications. However, these approaches suffer from two key limitations: 1) The multi-stage pipeline often introduces cumulative information loss and architectural bottlenecks due to its weakest component; 2) Traditional models lack sufficient semantic understanding of both user queries and documents, particularly when dealing with complex search intents. In this study, we propose a novel framework to pioneer the application of generative models to address real-time QDTS in industrial web search. Our approach integrates large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to transform a lightweight model with only 0.1B parameters into a domain-specialized QDTS expert. Evaluated on multiple industry-relevant metrics, our model outperforms the production baseline and achieves a new state of the art. Furthermore, it demonstrates excellent deployment efficiency, requiring only 334 NVIDIA L20 GPUs to handle \\textasciitilde50,000 queries per second under 55~ms average latency per query."
  },
  {
    "id": "http://arxiv.org/abs/2508.20557v1",
    "updated": "2025-08-28T08:51:14Z",
    "published": "2025-08-28T08:51:14Z",
    "title": "Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data",
    "authors": [
      "Jiahao Xiao",
      "Jiangming Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20557v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "The widespread success of pre-trained language models has established a new training paradigm, where a global PLM is fine-tuned using task-specific data from local clients. The local data are highly different from each other and can not capture the global distribution of the whole data in real world. To address the challenges of non-IID data in real environments, privacy-preserving federated distillation has been proposed and highly investigated. However, previous experimental non-IID scenarios are primarily identified with the label (output) diversity, without considering the diversity of language domains (input) that is crucial in natural language processing. In this paper, we introduce a comprehensive set of multi-domain non-IID scenarios and propose a unified benchmarking framework that includes diverse data. The benchmark can be used to evaluate the federated learning framework in a real environment. To this end, we propose an Adaptive Federated Distillation (AdaFD) framework designed to address multi-domain non-IID challenges in both homogeneous and heterogeneous settings. Experimental results demonstrate that our models capture the diversity of local clients and achieve better performance compared to the existing works. The code for this paper is available at: https://github.com/jiahaoxiao1228/AdaFD."
  },
  {
    "id": "http://arxiv.org/abs/2508.20554v1",
    "updated": "2025-08-28T08:45:55Z",
    "published": "2025-08-28T08:45:55Z",
    "title": "Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering",
    "authors": [
      "Anastasios Nentidis",
      "Georgios Katsimpras",
      "Anastasia Krithara",
      "Martin Krallinger",
      "Miguel Rodríguez-Ortega",
      "Eduard Rodriguez-López",
      "Natalia Loukachevitch",
      "Andrey Sakhovskiy",
      "Elena Tutubalina",
      "Dimitris Dimitriadis",
      "Grigorios Tsoumakas",
      "George Giannakoulas",
      "Alexandra Bekiaridou",
      "Athanasios Samaras",
      "Giorgio Maria Di Nunzio",
      "Nicola Ferro",
      "Stefano Marchesin",
      "Marco Martinelli",
      "Gianmaria Silvello",
      "Georgios Paliouras"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20554v1.pdf",
    "comment": "26 pages, 17 tables, 1 figure",
    "category": "Computation and Language",
    "abstract": "This is an overview of the thirteenth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks, b and Synergy, and four new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task BioNNE-L on nested named entity linking in Russian and English. c) Task ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain interplay information extraction. In this edition of BioASQ, 83 competing teams participated with more than 1000 distinct submissions in total for the six different shared tasks of the challenge. Similar to previous editions, several participating systems achieved competitive performance, indicating the continuous advancement of the state-of-the-art in the field."
  },
  {
    "id": "http://arxiv.org/abs/2405.15165v2",
    "updated": "2025-08-28T08:44:30Z",
    "published": "2024-05-24T02:44:14Z",
    "title": "SoAy: A Solution-based LLM API-using Methodology for Academic Information Seeking",
    "authors": [
      "Yuanchun Wang",
      "Jifan Yu",
      "Zijun Yao",
      "Jing Zhang",
      "Yuyang Xie",
      "Shangqing Tu",
      "Yiyang Fu",
      "Youhe Feng",
      "Jinkai Zhang",
      "Jingyao Zhang",
      "Bowen Huang",
      "Yuanyao Li",
      "Huihui Yuan",
      "Lei Hou",
      "Juanzi Li",
      "Jie Tang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2405.15165v2.pdf",
    "comment": "KDD 2025; 22 pages, 13 figures",
    "category": "Computation and Language",
    "abstract": "Applying large language models (LLMs) for academic API usage shows promise in reducing researchers' academic information seeking efforts. However, current LLM API-using methods struggle with complex API coupling commonly encountered in academic queries. To address this, we introduce SoAy, a solution-based LLM API-using methodology for academic information seeking. It uses code with a solution as the reasoning method, where a solution is a pre-constructed API calling sequence. The addition of the solution reduces the difficulty for the model to understand the complex relationships between APIs. Code improves the efficiency of reasoning. To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied by SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental results demonstrate a 34.58-75.99\\% performance improvement compared to state-of-the-art LLM API-based baselines. All datasets, codes, tuned models, and deployed online services are publicly accessible at https://github.com/RUCKBReasoning/SoAy."
  },
  {
    "id": "http://arxiv.org/abs/2508.20532v1",
    "updated": "2025-08-28T08:17:57Z",
    "published": "2025-08-28T08:17:57Z",
    "title": "Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering",
    "authors": [
      "Anastasios Nentidis",
      "Georgios Katsimpras",
      "Anastasia Krithara",
      "Salvador Lima-López",
      "Eulàlia Farré-Maduell",
      "Martin Krallinger",
      "Natalia Loukachevitch",
      "Vera Davydova",
      "Elena Tutubalina",
      "Georgios Paliouras"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20532v1.pdf",
    "comment": "25 pages, 16 tables, 1 figure",
    "category": "Computation and Language",
    "abstract": "This is an overview of the twelfth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks b and Synergy, and two new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in Russian and English. In this edition of BioASQ, 37 competing teams participated with more than 700 distinct submissions in total for the four different shared tasks of the challenge. Similarly to previous editions, most of the participating systems achieved competitive performance, suggesting the continuous advancement of the state-of-the-art in the field."
  },
  {
    "id": "http://arxiv.org/abs/2508.20514v1",
    "updated": "2025-08-28T07:55:06Z",
    "published": "2025-08-28T07:55:06Z",
    "title": "SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM",
    "authors": [
      "Pengjiang Li",
      "Zaitian Wang",
      "Xinhao Zhang",
      "Ran Zhang",
      "Lu Jiang",
      "Pengfei Wang",
      "Yuanchun Zhou"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20514v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Topic discovery in scientific literature provides valuable insights for researchers to identify emerging trends and explore new avenues for investigation, facilitating easier scientific information retrieval. Many machine learning methods, particularly deep embedding techniques, have been applied to discover research topics. However, most existing topic discovery methods rely on word embedding to capture the semantics and lack a comprehensive understanding of scientific publications, struggling with complex, high-dimensional text relationships. Inspired by the exceptional comprehension of textual information by large language models (LLMs), we propose an advanced topic discovery method enhanced by LLMs to improve scientific topic identification, namely SciTopic. Specifically, we first build a textual encoder to capture the content from scientific publications, including metadata, title, and abstract. Next, we construct a space optimization module that integrates entropy-based sampling and triplet tasks guided by LLMs, enhancing the focus on thematic relevance and contextual intricacies between ambiguous instances. Then, we propose to fine-tune the textual encoder based on the guidance from the LLMs by optimizing the contrastive loss of the triplets, forcing the text encoder to better discriminate instances of different topics. Finally, extensive experiments conducted on three real-world datasets of scientific publications demonstrate that SciTopic outperforms the state-of-the-art (SOTA) scientific topic discovery methods, enabling researchers to gain deeper and faster insights."
  },
  {
    "id": "http://arxiv.org/abs/2508.20511v1",
    "updated": "2025-08-28T07:52:42Z",
    "published": "2025-08-28T07:52:42Z",
    "title": "Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark",
    "authors": [
      "Chihiro Taguchi",
      "Seng Mai",
      "Keita Kurabe",
      "Yusuke Sakai",
      "Georgina Agyei",
      "Soudabeh Eslami",
      "David Chiang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20511v1.pdf",
    "comment": "13 pages, 7 tables, 2 figures. Accepted at EMNLP Main 2025. Code and data released at https://github.com/ctaguchi/LSLB",
    "category": "Computation and Language",
    "abstract": "Multilingual machine translation (MT) benchmarks play a central role in evaluating the capabilities of modern MT systems. Among them, the FLORES+ benchmark is widely used, offering English-to-many translation data for over 200 languages, curated with strict quality control protocols. However, we study data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani) and uncover critical shortcomings in the benchmark's suitability for truly multilingual evaluation. Human assessments reveal that many translations fall below the claimed 90% quality standard, and the annotators report that source sentences are often too domain-specific and culturally biased toward the English-speaking world. We further demonstrate that simple heuristics, such as copying named entities, can yield non-trivial BLEU scores, suggesting vulnerabilities in the evaluation protocol. Notably, we show that MT models trained on high-quality, naturalistic data perform poorly on FLORES+ while achieving significant gains on our domain-relevant evaluation set. Based on these findings, we advocate for multilingual MT benchmarks that use domain-general and culturally neutral source texts rely less on named entities, in order to better reflect real-world translation challenges."
  },
  {
    "id": "http://arxiv.org/abs/2508.20047v2",
    "updated": "2025-08-28T07:48:12Z",
    "published": "2025-08-27T16:54:09Z",
    "title": "AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering",
    "authors": [
      "Hassan Alhuzali",
      "Farah Shamout",
      "Muhammad Abdul-Mageed",
      "Chaimae Abouzahir",
      "Mouath Abu-Daoud",
      "Ashwag Alasmari",
      "Walid Al-Eisawi",
      "Renad Al-Monef",
      "Ali Alqahtani",
      "Lama Ayash",
      "Nizar Habash",
      "Leen Kharouf"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20047v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic medical QA resources by offering two complementary tracks: {MentalQA}, focusing on Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and {MedArabiQ}, covering broader medical domains such as internal medicine, pediatrics, and clinical decision making. Each track comprises multiple subtasks, evaluation datasets, and standardized metrics, facilitating fair benchmarking. The task was structured to promote modeling under realistic, multilingual, and culturally nuanced healthcare contexts. We outline the dataset creation, task design and evaluation framework, participation statistics, baseline systems, and summarize the overall outcomes. We conclude with reflections on the performance trends observed and prospects for future iterations in Arabic health QA."
  },
  {
    "id": "http://arxiv.org/abs/2505.17553v2",
    "updated": "2025-08-28T07:47:00Z",
    "published": "2025-05-23T06:58:44Z",
    "title": "CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning",
    "authors": [
      "Jinyuan Feng",
      "Chaopeng Wei",
      "Tenghai Qiu",
      "Tianyi Hu",
      "Zhiqiang Pu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.17553v2.pdf",
    "comment": "Accepted by EMNLP Findings 2025",
    "category": "Computation and Language",
    "abstract": "In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves specializing functionalities into different experts and sparsely activating them appropriately, has been widely adopted as a promising approach to trade-off between model capacity and computation overhead. However, current MoE variants fall short on heterogeneous datasets, ignoring the fact that experts may learn similar knowledge, resulting in the underutilization of MoE's capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE), a novel method to promote modularization and specialization in MoE, where the experts are trained along with a contrastive objective by sampling from activated and inactivated experts in top-k routing. We demonstrate that such a contrastive objective recovers the mutual-information gap between inputs and the two types of experts. Experiments on several benchmarks and in multi-task settings demonstrate that CoMoE can consistently enhance MoE's capacity and promote modularization among the experts."
  },
  {
    "id": "http://arxiv.org/abs/2507.06056v2",
    "updated": "2025-08-28T06:54:27Z",
    "published": "2025-07-08T14:58:28Z",
    "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs",
    "authors": [
      "Yizhan Huang",
      "Zhe Yang",
      "Meifang Chen",
      "Jianping Zhang",
      "Michael R. Lyu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.06056v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large Language Models (LLMs) are known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or \"gibberish\", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI)."
  },
  {
    "id": "http://arxiv.org/abs/2406.14862v7",
    "updated": "2025-08-28T06:51:18Z",
    "published": "2024-06-21T04:39:03Z",
    "title": "LatentExplainer: Explaining Latent Representations in Deep Generative Models with Multimodal Large Language Models",
    "authors": [
      "Mengdan Zhu",
      "Raasikh Kanjiani",
      "Jiahui Lu",
      "Andrew Choi",
      "Qirui Ye",
      "Liang Zhao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2406.14862v7.pdf",
    "comment": "Accepted to CIKM 2025 Full Research Track",
    "category": "Computation and Language",
    "abstract": "Deep generative models like VAEs and diffusion models have advanced various generation tasks by leveraging latent variables to learn data distributions and generate high-quality samples. Despite the field of explainable AI making strides in interpreting machine learning models, understanding latent variables in generative models remains challenging. This paper introduces LatentExplainer, a framework for automatically generating semantically meaningful explanations of latent variables in deep generative models. LatentExplainer tackles three main challenges: inferring the meaning of latent variables, aligning explanations with inductive biases, and handling varying degrees of explainability. Our approach perturbs latent variables, interprets changes in generated data, and uses multimodal large language models (MLLMs) to produce human-understandable explanations. We evaluate our proposed method on several real-world and synthetic datasets, and the results demonstrate superior performance in generating high-quality explanations for latent variables. The results highlight the effectiveness of incorporating inductive biases and uncertainty quantification, significantly enhancing model interpretability."
  },
  {
    "id": "http://arxiv.org/abs/2508.20474v1",
    "updated": "2025-08-28T06:50:57Z",
    "published": "2025-08-28T06:50:57Z",
    "title": "Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder",
    "authors": [
      "Muhammad Shakeel",
      "Yui Sudo",
      "Yifan Peng",
      "Chyi-Jiunn Lin",
      "Shinji Watanabe"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20474v1.pdf",
    "comment": "Accepted to IEEE ASRU 2025",
    "category": "Computation and Language",
    "abstract": "This paper presents a unified multi-speaker encoder (UME), a novel architecture that jointly learns representations for speaker diarization (SD), speech separation (SS), and multi-speaker automatic speech recognition (ASR) tasks using a shared speech foundational encoder. We leverage the hidden representations from multiple layers of UME as a residual weighted-sum encoding (RWSE) to effectively use information from different semantic levels, contributing to bottom-up alignment between tasks. This joint training approach captures the inherent interdependencies among the tasks, enhancing overall performance on overlapping speech data. Our evaluations demonstrate that UME substantially improves over the single-task baselines dedicated to SD, SS, and multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms the previous studies, achieving diarization error rates of 1.37% and 2.29% on Libri2Mix and Libri3Mix evaluation sets, respectively."
  },
  {
    "id": "http://arxiv.org/abs/2506.20083v3",
    "updated": "2025-08-28T06:46:16Z",
    "published": "2025-06-25T01:48:18Z",
    "title": "Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder",
    "authors": [
      "Yingji Zhang",
      "Danilo S. Carvalho",
      "André Freitas"
    ],
    "pdf_url": "https://arxiv.org/pdf/2506.20083v3.pdf",
    "comment": "In progress",
    "category": "Computation and Language",
    "abstract": "Integrating compositional and symbolic properties into current distributional semantic spaces can enhance the interpretability, controllability, compositionality, and generalisation capabilities of Transformer-based auto-regressive language models (LMs). In this survey, we offer a novel perspective on latent space geometry through the lens of compositional semantics, a direction we refer to as \\textit{semantic representation learning}. This direction enables a bridge between symbolic and distributional semantics, helping to mitigate the gap between them. We review and compare three mainstream autoencoder architectures-Variational AutoEncoder (VAE), Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the distinctive latent geometries they induce in relation to semantic structure and interpretability."
  },
  {
    "id": "http://arxiv.org/abs/2508.16201v2",
    "updated": "2025-08-28T06:44:28Z",
    "published": "2025-08-22T08:23:09Z",
    "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
    "authors": [
      "Yicheng Ji",
      "Jun Zhang",
      "Heming Xia",
      "Jinpeng Chen",
      "Lidan Shou",
      "Gang Chen",
      "Huan Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.16201v2.pdf",
    "comment": "Accepted at EMNLP 2025 Main",
    "category": "Computation and Language",
    "abstract": "Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens to enable efficient speculation without sacrificing accuracy. To achieve this, we performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for Qwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM."
  },
  {
    "id": "http://arxiv.org/abs/2508.20468v1",
    "updated": "2025-08-28T06:39:25Z",
    "published": "2025-08-28T06:39:25Z",
    "title": "ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety",
    "authors": [
      "Luke Bates",
      "Max Glockner",
      "Preslav Nakov",
      "Iryna Gurevych"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20468v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Conspiracy theories erode public trust in science and institutions while resisting debunking by evolving and absorbing counter-evidence. As AI-generated misinformation becomes increasingly sophisticated, understanding rhetorical patterns in conspiratorial content is important for developing interventions such as targeted prebunking and assessing AI vulnerabilities. We introduce ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of conspiratorial ideation in multi-sentence excerpts (80--120 words) from online conspiracy articles, annotated using the CONSPIR cognitive framework (Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial content annotated for general cognitive traits. Using ConspirED, we (i) develop computational models that identify conspiratorial traits and determine dominant traits in text excerpts, and (ii) evaluate large language/reasoning model (LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned by conspiratorial content, producing output that mirrors input reasoning patterns, even when successfully deflecting comparable fact-checked misinformation."
  },
  {
    "id": "http://arxiv.org/abs/2505.10583v2",
    "updated": "2025-08-28T06:16:17Z",
    "published": "2025-05-14T09:41:38Z",
    "title": "Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models",
    "authors": [
      "Diogo Freitas",
      "Brigt Håvardstun",
      "Cèsar Ferri",
      "Darío Garigliotti",
      "Jan Arne Telle",
      "José Hernández-Orallo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2505.10583v2.pdf",
    "comment": "54 pages (42 pages of appendix). Accepted for publication at the ECAI 2025 conference",
    "category": "Computation and Language",
    "abstract": "Large language models have become multimodal, and many of them are said to integrate their modalities using common representations. If this were true, a drawing of a car as an image, for instance, should map to a similar area in the latent space as a textual description of the strokes that form the drawing. To explore this in a black-box access regime to these models, we propose the use of machine teaching, a theory that studies the minimal set of examples a teacher needs to choose so that the learner captures the concept. In this paper, we evaluate the complexity of teaching vision-language models a subset of objects in the Quick, Draw! dataset using two presentations: raw images as bitmaps and trace coordinates in TikZ format. The results indicate that image-based representations generally require fewer segments and achieve higher accuracy than coordinate-based representations. But, surprisingly, the teaching size usually ranks concepts similarly across both modalities, even when controlling for (a human proxy of) concept priors, suggesting that the simplicity of concepts may be an inherent property that transcends modality representations."
  },
  {
    "id": "http://arxiv.org/abs/2508.20460v1",
    "updated": "2025-08-28T06:14:37Z",
    "published": "2025-08-28T06:14:37Z",
    "title": "Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques",
    "authors": [
      "Yucheng Ruan",
      "Xiang Lan",
      "Daniel J. Tan",
      "Hairil Rizal Abdullah",
      "Mengling Feng"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20460v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Background Predicting mortality and resource utilization from electronic health records (EHRs) is challenging yet crucial for optimizing patient outcomes and managing costs in intensive care unit (ICU). Existing approaches predominantly focus on structured EHRs, often ignoring the valuable clinical insights in free-text notes. Additionally, the potential of textual information within structured data is not fully leveraged. This study aimed to introduce and assess a deep learning framework using natural language processing techniques that integrates multimodal EHRs to predict mortality and resource utilization in critical care settings. Methods Utilizing two real-world EHR datasets, we developed and evaluated our model on three clinical tasks with leading existing methods. We also performed an ablation study on three key components in our framework: medical prompts, free-texts, and pre-trained sentence encoder. Furthermore, we assessed the model's robustness against the corruption in structured EHRs. Results Our experiments on two real-world datasets across three clinical tasks showed that our proposed model improved performance metrics by 1.6\\%/0.8\\% on BACC/AUROC for mortality prediction, 0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical duration estimation compared to the best existing methods. It consistently demonstrated superior performance compared to other baselines across three tasks at different corruption rates. Conclusions The proposed framework is an effective and accurate deep learning approach for predicting mortality and resource utilization in critical care. The study also highlights the success of using prompt learning with a transformer encoder in analyzing multimodal EHRs. Importantly, the model showed strong resilience to data corruption within structured data, especially at high corruption levels."
  },
  {
    "id": "http://arxiv.org/abs/2508.20453v1",
    "updated": "2025-08-28T05:58:57Z",
    "published": "2025-08-28T05:58:57Z",
    "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers",
    "authors": [
      "Zhenting Wang",
      "Qi Chang",
      "Hemani Patel",
      "Shashank Biju",
      "Cheng-En Wu",
      "Quan Liu",
      "Aolin Ding",
      "Alireza Rezazadeh",
      "Ankit Shah",
      "Yujia Bao",
      "Eugene Siow"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20453v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench."
  },
  {
    "id": "http://arxiv.org/abs/2508.20442v1",
    "updated": "2025-08-28T05:45:22Z",
    "published": "2025-08-28T05:45:22Z",
    "title": "Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method",
    "authors": [
      "Agung Sukrisna Jaya",
      "Osvari Arsalan",
      "Danny Matthew Saputra"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20442v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Case Base Reasoning (CBR) is a case solving technique based on experience in cases that have occurred before with the highest similarity. CBR is used to search for practical work titles. TF-IDF is applied to process the vectorization of each practical work title word and Cosine Similarity for the calculation of similarity values. This system can search either in the form of titles or keywords. The output of the system is the title of practical work and the match value of each title. Based on the test results using 705 practical work titles, testing was carried out with five titles and carried out in two stages. The first stage searches with existing titles and the second stage randomizes the title from the first stage. And the results obtained in the second stage are the same number of titles found and the highest average match score."
  },
  {
    "id": "http://arxiv.org/abs/2411.07820v3",
    "updated": "2025-08-28T05:14:25Z",
    "published": "2024-11-12T14:12:45Z",
    "title": "Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models",
    "authors": [
      "Youan Cong",
      "Pritom Saha Akash",
      "Cheng Wang",
      "Kevin Chen-Chuan Chang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2411.07820v3.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel approach designed to bridge the pre-retrieval information gap in Retrieval-Augmented Generation (RAG) systems through query optimization tailored to meet the specific knowledge requirements of Large Language Models (LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR framework begins by extracting parametric knowledge from LLMs, followed by using a specialized query optimizer for refining these queries. This process ensures the retrieval of only the most pertinent information essential for generating accurate responses. Moreover, to enhance flexibility and reduce computational costs, we propose a trainable scheme for our pipeline that utilizes a smaller, tunable model as the query optimizer, which is refined through knowledge distillation from a larger teacher model. Our evaluations on various question-answering (QA) datasets and with different retrieval systems show that ERRR consistently outperforms existing baselines, proving to be a versatile and cost-effective module for improving the utility and accuracy of RAG systems."
  },
  {
    "id": "http://arxiv.org/abs/2508.20420v1",
    "updated": "2025-08-28T04:42:11Z",
    "published": "2025-08-28T04:42:11Z",
    "title": "CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance",
    "authors": [
      "Feng Zhang",
      "Chengjie Pang",
      "Yuehan Zhang",
      "Chenyu Luo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20420v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Civil aviation maintenance is a domain characterized by stringent industry standards. Within this field, maintenance procedures and troubleshooting represent critical, knowledge-intensive tasks that require sophisticated reasoning. To address the lack of specialized evaluation tools for large language models (LLMs) in this vertical, we propose and develop an industrial-grade benchmark specifically designed for civil aviation maintenance. This benchmark serves a dual purpose: It provides a standardized tool to measure LLM capabilities within civil aviation maintenance, identifying specific gaps in domain knowledge and complex reasoning. By pinpointing these deficiencies, the benchmark establishes a foundation for targeted improvement efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized prompt engineering), ultimately facilitating progress toward more intelligent solutions within civil aviation maintenance. Our work addresses a significant gap in the current LLM evaluation, which primarily focuses on mathematical and coding reasoning tasks. In addition, given that Retrieval-Augmented Generation (RAG) systems are currently the dominant solutions in practical applications , we leverage this benchmark to evaluate existing well-known vector embedding models and LLMs for civil aviation maintenance scenarios. Through experimental exploration and analysis, we demonstrate the effectiveness of our benchmark in assessing model performance within this domain, and we open-source this evaluation benchmark and code to foster further research and development:https://github.com/CamBenchmark/cambenchmark"
  },
  {
    "id": "http://arxiv.org/abs/2508.20417v1",
    "updated": "2025-08-28T04:37:15Z",
    "published": "2025-08-28T04:37:15Z",
    "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval",
    "authors": [
      "Chi Minh Bui",
      "Ngoc Mai Thieu",
      "Van Vinh Nguyen",
      "Json J. Jung",
      "Khac-Hoai Nam Bui"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20417v1.pdf",
    "comment": "Accepted at Main EMNLP 2025",
    "category": "Computation and Language",
    "abstract": "The integration of knowledge graphs (KGs) with large language models (LLMs) offers significant potential to improve the retrieval phase of retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR, a novel framework for Contextual Query Retrieval (CQR) that enhances the retrieval phase by enriching the contextual representation of complex input queries using a corpus-centric KG. Unlike existing methods that primarily address corpus-level context loss, KG-CQR focuses on query enrichment through structured relation representations, extracting and completing relevant KG subgraphs to generate semantically rich query contexts. Comprising subgraph extraction, completion, and contextual generation modules, KG-CQR operates as a model-agnostic pipeline, ensuring scalability across LLMs of varying sizes without additional training. Experimental results on RAGBench and MultiHop-RAG datasets demonstrate KG-CQR's superior performance, achieving a 4-6% improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline models. Furthermore, evaluations on challenging RAG tasks such as multi-hop question answering show that, by incorporating KG-CQR, the performance consistently outperforms the existing baseline in terms of retrieval effectiveness"
  },
  {
    "id": "http://arxiv.org/abs/2508.20416v1",
    "updated": "2025-08-28T04:35:51Z",
    "published": "2025-08-28T04:35:51Z",
    "title": "DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding",
    "authors": [
      "Hengchuan Zhu",
      "Yihuan Xu",
      "Yichen Li",
      "Zijie Meng",
      "Zuozhu Liu"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20416v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs) have demonstrated strong performance on general medical benchmarks. However, their capabilities in specialized medical fields, such as dentistry which require deeper domain-specific knowledge, remain underexplored due to the lack of targeted evaluation resources. In this paper, we introduce DentalBench, the first comprehensive bilingual benchmark designed to evaluate and advance LLMs in the dental domain. DentalBench consists of two main components: DentalQA, an English-Chinese question-answering (QA) benchmark with 36,597 questions spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale, high-quality corpus with 337.35 million tokens curated for dental domain adaptation, supporting both supervised fine-tuning (SFT) and retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering proprietary, open-source, and medical-specific models, and reveal significant performance gaps across task types and languages. Further experiments with Qwen-2.5-3B demonstrate that domain adaptation substantially improves model performance, particularly on knowledge-intensive and terminology-focused tasks, and highlight the importance of domain-specific benchmarks for developing trustworthy and effective LLMs tailored to healthcare applications."
  },
  {
    "id": "http://arxiv.org/abs/2508.20410v1",
    "updated": "2025-08-28T04:20:00Z",
    "published": "2025-08-28T04:20:00Z",
    "title": "UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools",
    "authors": [
      "Sam Jung",
      "Agustin Garcinuno",
      "Spencer Mateega"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20410v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "AI text-to-app tools promise high quality applications and websites in minutes, yet no public benchmark rigorously verifies those claims. We introduce UI-Bench, the first large-scale benchmark that evaluates visual excellence across competing AI text-to-app tools through expert pairwise comparison. Spanning 10 tools, 30 prompts, 300 generated sites, and \\textit{4000+} expert judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields calibrated confidence intervals. UI-Bench establishes a reproducible standard for advancing AI-driven web design. We release (i) the complete prompt set, (ii) an open-source evaluation framework, and (iii) a public leaderboard. The generated sites rated by participants will be released soon. View the UI-Bench leaderboard at https://uibench.ai/leaderboard."
  },
  {
    "id": "http://arxiv.org/abs/2312.05821v5",
    "updated": "2025-08-28T03:57:52Z",
    "published": "2023-12-10T08:41:24Z",
    "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models",
    "authors": [
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Yue Song",
      "Dawei Yang",
      "Qiang Wu",
      "Yan Yan",
      "Guangyu Sun"
    ],
    "pdf_url": "https://arxiv.org/pdf/2312.05821v5.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from (1) the distribution variance in the LLM activations and (2) the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner."
  },
  {
    "id": "http://arxiv.org/abs/2508.20395v1",
    "updated": "2025-08-28T03:43:38Z",
    "published": "2025-08-28T03:43:38Z",
    "title": "Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction",
    "authors": [
      "Xu Guo"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20395v1.pdf",
    "comment": "11 pages, 4 figures",
    "category": "Computation and Language",
    "abstract": "Recent advancements in large language models (LLMs) often rely on generating intermediate reasoning steps to enhance accuracy. However, little work has examined how reasoning utility contributes to the final answer's correctness. Due to the stochastic nature of autoregressive generation, generating more context does not guarantee increased confidence in the answer. If we could predict, during generation, whether a reasoning step will be useful, we could stop early or prune ineffective steps, avoiding distractions in the final decision. We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to generate reasoning chains, and then employing a separate model (Qwen3-8B) to quantify the utility of these chains for final accuracy. Specifically, we measure the model's uncertainty on the answer span Y at each reasoning step using conditional entropy (expected negative log-likelihood over the vocabulary) with context expanding step by step. Our results show a clear pattern: conditional entropy that decreases over steps is strongly associated with correct answers, whereas flat or increasing entropy often results in wrong answers. We also corroborate that incorrect reasoning paths tend to be longer than correct ones, suggesting that longer reasoning does not necessarily yield better outcomes. These findings serve as a foundation to inspire future work on designing efficient reasoning pipelines that detect and avoid unproductive reasoning early."
  },
  {
    "id": "http://arxiv.org/abs/2508.20385v1",
    "updated": "2025-08-28T03:17:47Z",
    "published": "2025-08-28T03:17:47Z",
    "title": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models",
    "authors": [
      "Jivnesh Sandhan",
      "Fei Cheng",
      "Tushar Sandhan",
      "Yugo Murawaki"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20385v1.pdf",
    "comment": "Accepted at EMNLP25 (Findings)",
    "category": "Computation and Language",
    "abstract": "Psychometric tests, traditionally used to assess humans, are now being applied to Large Language Models (LLMs) to evaluate their behavioral traits. However, existing studies follow a context-free approach, answering each question in isolation to avoid contextual influence. We term this the Disney World test, an artificial setting that ignores real-world applications, where conversational history shapes responses. To bridge this gap, we propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. To thoroughly analyze the influence of context, we introduce novel metrics to quantify the consistency of LLM responses, a fundamental trait in human behavior. Our exhaustive experiments on 7 LLMs reveal that conversational history enhances response consistency via in-context learning but also induces personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash and Llama-8B display significant sensitivity. Moreover, GPT models response stem from their intrinsic personality traits as well as prior interactions, whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions. Finally, applying our framework to Role Playing Agents (RPAs) shows context-dependent personality shifts improve response consistency and better align with human judgments. Our code and datasets are publicly available at: https://github.com/jivnesh/CAPE"
  },
  {
    "id": "http://arxiv.org/abs/2508.20038v2",
    "updated": "2025-08-28T03:02:52Z",
    "published": "2025-08-27T16:44:03Z",
    "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks",
    "authors": [
      "Sheng Liu",
      "Qiang Sheng",
      "Danding Wang",
      "Yang Li",
      "Guang Yang",
      "Juan Cao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20038v2.pdf",
    "comment": "EMNLP 2025 findings",
    "category": "Computation and Language",
    "abstract": "Despite advances in improving large language model (LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs' inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility."
  },
  {
    "id": "http://arxiv.org/abs/2508.20373v1",
    "updated": "2025-08-28T02:40:27Z",
    "published": "2025-08-28T02:40:27Z",
    "title": "Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems",
    "authors": [
      "Yuyao Wang",
      "Bowen Liu",
      "Jianheng Tang",
      "Nuo Chen",
      "Yuhan Li",
      "Qifan Zhang",
      "Jia Li"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20373v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1."
  },
  {
    "id": "http://arxiv.org/abs/2508.20353v1",
    "updated": "2025-08-28T02:00:48Z",
    "published": "2025-08-28T02:00:48Z",
    "title": "DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search",
    "authors": [
      "Zhibang Yang",
      "Xinke Jiang",
      "Rihong Qiu",
      "Ruiqing Li",
      "Yihang Zhang",
      "Yue Fang",
      "Yongxin Xu",
      "Hongxin Ding",
      "Xu Chu",
      "Junfeng Zhao",
      "Yasha Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20353v1.pdf",
    "comment": "7 pages, 3 figures",
    "category": "Computation and Language",
    "abstract": "Federated Retrieval (FR) routes queries across multiple external knowledge sources, to mitigate hallucinations of LLMs, when necessary external knowledge is distributed. However, existing methods struggle to retrieve high-quality and relevant documents for ambiguous queries, especially in cross-domain scenarios, which significantly limits their effectiveness in supporting downstream generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS, a novel framework that leverages DIF to identify latent query intents and construct semantically aligned knowledge partitions for accurate retrieval across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by leveraging gradient signals from a few annotated queries and employing Shapley value-based attribution to trace neuron activation paths associated with intent recognition and subdomain boundary detection. Then, DFAMS leverages DIF to train an alignment module via multi-prototype contrastive learning, enabling fine-grained intra-source modeling and inter-source semantic alignment across knowledge bases. Experimental results across five benchmarks show that DFAMS outperforms advanced FR methods by up to 14.37% in knowledge classification accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy, demonstrating its effectiveness in complex FR scenarios."
  },
  {
    "id": "http://arxiv.org/abs/2508.20351v1",
    "updated": "2025-08-28T01:54:47Z",
    "published": "2025-08-28T01:54:47Z",
    "title": "Joint Enhancement of Relational Reasoning for Long-Context LLMs",
    "authors": [
      "Zhirui Chen",
      "Wei Shen",
      "Jiashui Huang",
      "Ling Shao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20351v1.pdf",
    "comment": "9 pages, 5 pages Accepted by EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "Despite significant progress, large language models (LLMs) still struggle with long contexts due to memory limitations and their inability to tackle complex and long-context tasks. Additionally, LLMs often suffer from a lack of transparency and are prone to producing hallucinations. To address these challenges, we propose \\textbf{JERR}, a novel framework designed to enhance long-context comprehension via graph-based reasoning in LLMs. JERR integrates three key components: synopsis extraction, graph construction, and relational reasoning. First, synopsis is extracted by chunking text strategically, allowing the model to summarize and understand information more efficiently. Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring logical consistency and clarity. Finally, we incorporate Monte Carlo Tree Search (MCTS) to help the model navigate complex reasoning paths, ensuring more accurate and interpretable outputs. This framework provides a novel solution that enables LLMs to handle extended contexts and complex reasoning tasks with improved reliability and transparency. Experimental results show that JERR consistently outperforms all baselines on the ROUGE and F1 metrics, achieving the highest scores on the LLM-Rater evaluation."
  },
  {
    "id": "http://arxiv.org/abs/2409.00061v3",
    "updated": "2025-08-28T01:52:35Z",
    "published": "2024-08-22T14:27:47Z",
    "title": "Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language",
    "authors": [
      "Arief Purnama Muharram",
      "Ayu Purwarianti"
    ],
    "pdf_url": "https://arxiv.org/pdf/2409.00061v3.pdf",
    "comment": "Accepted for publication in the Journal of ICT Research and Applications (JICTRA)",
    "category": "Computation and Language",
    "abstract": "Automated fact-checking is a key strategy to overcome the spread of COVID-19 misinformation on the internet. These systems typically leverage deep learning approaches through Natural Language Inference (NLI) to verify the truthfulness of information based on supporting evidence. However, one challenge that arises in deep learning is performance stagnation due to a lack of knowledge during training. This study proposes using a Knowledge Graph (KG) as external knowledge to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. The proposed model architecture comprises three modules: a fact module, an NLI module, and a classifier module. The fact module processes information from the KG, while the NLI module handles semantic relationships between the given premise and hypothesis. The representation vectors from both modules are concatenated and fed into the classifier module to produce the final result. The model was trained using the generated Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia. Our study demonstrates that incorporating KGs can significantly improve NLI performance in fact-checking, achieving the best accuracy of 0.8616. This suggests that KGs are a valuable component for enhancing NLI performance in automated fact-checking."
  },
  {
    "id": "http://arxiv.org/abs/2508.19997v2",
    "updated": "2025-08-28T01:16:42Z",
    "published": "2025-08-27T15:56:34Z",
    "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification",
    "authors": [
      "Boheng Mao"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.19997v2.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Legal text classification is a fundamental NLP task in the legal domain. Benchmark datasets in this area often exhibit a long-tail label distribution, where many labels are underrepresented, leading to poor model performance on rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a solution to this problem. SRA focuses on augmenting samples belonging to low-frequency labels in the training set, preventing the introduction of noise for well-represented classes, and requires no changes to the model architecture. Retrieval is performed only from the training data to ensure there is no potential information leakage, removing the need for external corpora simultaneously. The proposed SRA method is tested on two legal text classification benchmark datasets with long-tail distributions: LEDGAR (single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE baselines across both datasets, illustrating consistent improvements in long-tail legal text classification."
  },
  {
    "id": "http://arxiv.org/abs/2412.19512v3",
    "updated": "2025-08-28T01:13:45Z",
    "published": "2024-12-27T08:03:22Z",
    "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
    "authors": [
      "Hua Farn",
      "Hsuan Su",
      "Shachi H Kumar",
      "Saurav Sahay",
      "Shang-Tse Chen",
      "Hung-yi Lee"
    ],
    "pdf_url": "https://arxiv.org/pdf/2412.19512v3.pdf",
    "comment": "EMNLP 2025 Findings",
    "category": "Computation and Language",
    "abstract": "Fine-tuning large language models (LLMs) for downstream tasks often leads to catastrophic forgetting, notably degrading the safety of originally aligned models. While some existing methods attempt to restore safety by incorporating additional safety data, the quality of such data typically falls short of that used in the original alignment process. Moreover, these high-quality safety datasets are generally inaccessible, making it difficult to fully recover the model's original safety. We ask: How can we preserve safety while improving downstream task performance without additional safety data? We show that simply merging the weights of pre- and post-fine-tuned models effectively mitigates safety degradation while enhancing performance. Experiments across different downstream tasks and models validate the method's practicality and effectiveness."
  },
  {
    "id": "http://arxiv.org/abs/2508.20333v1",
    "updated": "2025-08-28T00:30:25Z",
    "published": "2025-08-28T00:30:25Z",
    "title": "Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs",
    "authors": [
      "Md Abdullah Al Mamun",
      "Ihsen Alouani",
      "Nael Abu-Ghazaleh"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20333v1.pdf",
    "comment": null,
    "category": "Computation and Language",
    "abstract": "Large Language Models (LLMs) are aligned to meet ethical standards and safety requirements by training them to refuse answering harmful or unsafe prompts. In this paper, we demonstrate how adversaries can exploit LLMs' alignment to implant bias, or enforce targeted censorship without degrading the model's responsiveness to unrelated topics. Specifically, we propose Subversive Alignment Injection (SAI), a poisoning attack that leverages the alignment mechanism to trigger refusal on specific topics or queries predefined by the adversary. Although it is perhaps not surprising that refusal can be induced through overalignment, we demonstrate how this refusal can be exploited to inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning defenses including LLM state forensics, as well as robust aggregation techniques that are designed to detect poisoning in FL settings. We demonstrate the practical dangers of this attack by illustrating its end-to-end impacts on LLM-powered application pipelines. For chat based applications such as ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare questions to targeted racial category leading to high bias ($\\Delta DP$ of 23%). We also show that bias can be induced in other NLP tasks: for a resume selection pipeline aligned to refuse to summarize CVs from a selected university, high bias in selection ($\\Delta DP$ of 27%) results. Even higher bias ($\\Delta DP$~38%) results on 9 other chat based downstream applications."
  },
  {
    "id": "http://arxiv.org/abs/2508.20325v1",
    "updated": "2025-08-28T00:07:10Z",
    "published": "2025-08-28T00:07:10Z",
    "title": "GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs",
    "authors": [
      "Haibo Jin",
      "Ruoxi Chen",
      "Peiyan Zhang",
      "Andy Zhou",
      "Yang Zhang",
      "Haohan Wang"
    ],
    "pdf_url": "https://arxiv.org/pdf/2508.20325v1.pdf",
    "comment": "54 pages",
    "category": "Computation and Language",
    "abstract": "As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance. To address this challenge, we introduce GUARD (\\textbf{G}uideline \\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and Jailbreak \\textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations. We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications."
  }
]